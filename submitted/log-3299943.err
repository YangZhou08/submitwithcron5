Already on 'addinggriffin'
error: cannot lock ref 'refs/remotes/origin/addinggriffin': is at c553cb6465123ce743c7996d24f32f131d451c8b but expected a3ab2518c4acc407e96b46808ddb3c5e9fdf6628
From github.com:YangZhou08/CommonSenseReasoning
 ! a3ab251..c553cb6  addinggriffin -> origin/addinggriffin  (unable to update local ref)
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:06<01:06, 66.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:08<01:08, 68.00s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:08<01:08, 68.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:08<01:08, 68.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 39.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 43.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.57s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.57s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.62s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
  0%|          | 0/32 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:23<12:02, 23.30s/it]  3%|▎         | 1/32 [00:24<12:26, 24.07s/it]  3%|▎         | 1/32 [00:24<12:30, 24.22s/it]  3%|▎         | 1/32 [00:25<12:56, 25.04s/it]  3%|▎         | 1/32 [00:25<13:05, 25.34s/it]  3%|▎         | 1/32 [00:25<13:07, 25.41s/it]  3%|▎         | 1/32 [00:42<21:44, 42.07s/it]  3%|▎         | 1/32 [00:42<22:04, 42.73s/it]  6%|▋         | 2/32 [00:46<11:28, 22.95s/it]  6%|▋         | 2/32 [00:47<11:57, 23.91s/it]  6%|▋         | 2/32 [00:48<12:15, 24.52s/it]  6%|▋         | 2/32 [00:48<12:03, 24.12s/it]  6%|▋         | 2/32 [00:50<12:40, 25.35s/it]  6%|▋         | 2/32 [00:51<12:56, 25.87s/it]  9%|▉         | 3/32 [01:10<11:16, 23.33s/it]  9%|▉         | 3/32 [01:12<11:40, 24.17s/it]  9%|▉         | 3/32 [01:13<11:52, 24.56s/it]  9%|▉         | 3/32 [01:13<11:52, 24.57s/it]  9%|▉         | 3/32 [01:14<11:49, 24.47s/it]  9%|▉         | 3/32 [01:15<12:10, 25.18s/it]  6%|▋         | 2/32 [01:26<21:36, 43.22s/it]  6%|▋         | 2/32 [01:31<23:12, 46.43s/it] 12%|█▎        | 4/32 [01:34<11:05, 23.77s/it] 12%|█▎        | 4/32 [01:35<11:04, 23.75s/it] 12%|█▎        | 4/32 [01:37<11:22, 24.39s/it] 12%|█▎        | 4/32 [01:38<11:33, 24.78s/it] 12%|█▎        | 4/32 [01:40<11:43, 25.12s/it] 12%|█▎        | 4/32 [01:41<11:50, 25.38s/it] 16%|█▌        | 5/32 [01:58<10:41, 23.75s/it] 16%|█▌        | 5/32 [02:00<10:53, 24.19s/it] 16%|█▌        | 5/32 [02:02<11:05, 24.66s/it] 16%|█▌        | 5/32 [02:04<11:01, 24.50s/it] 16%|█▌        | 5/32 [02:04<11:14, 24.97s/it] 16%|█▌        | 5/32 [02:05<11:30, 25.57s/it]  9%|▉         | 3/32 [02:10<21:05, 43.62s/it] 19%|█▉        | 6/32 [02:20<10:05, 23.30s/it]  9%|▉         | 3/32 [02:21<23:10, 47.95s/it] 19%|█▉        | 6/32 [02:24<10:29, 24.23s/it] 19%|█▉        | 6/32 [02:27<10:46, 24.87s/it] 19%|█▉        | 6/32 [02:29<10:41, 24.66s/it] 19%|█▉        | 6/32 [02:29<10:46, 24.88s/it] 19%|█▉        | 6/32 [02:30<10:59, 25.37s/it] 22%|██▏       | 7/32 [02:46<10:03, 24.14s/it] 12%|█▎        | 4/32 [02:48<19:19, 41.41s/it] 22%|██▏       | 7/32 [02:48<10:01, 24.06s/it] 22%|██▏       | 7/32 [02:52<10:19, 24.77s/it] 22%|██▏       | 7/32 [02:52<10:05, 24.20s/it] 22%|██▏       | 7/32 [02:54<10:19, 24.78s/it] 22%|██▏       | 7/32 [03:01<11:14, 26.97s/it] 12%|█▎        | 4/32 [03:04<21:25, 45.92s/it] 25%|██▌       | 8/32 [03:11<09:41, 24.23s/it] 25%|██▌       | 8/32 [03:15<09:59, 24.97s/it] 25%|██▌       | 8/32 [03:17<09:50, 24.60s/it] 25%|██▌       | 8/32 [03:18<10:02, 25.11s/it] 25%|██▌       | 8/32 [03:18<09:51, 24.64s/it] 16%|█▌        | 5/32 [03:28<18:27, 41.01s/it] 25%|██▌       | 8/32 [03:35<11:47, 29.48s/it] 28%|██▊       | 9/32 [03:36<09:23, 24.50s/it] 28%|██▊       | 9/32 [03:39<09:25, 24.57s/it] 16%|█▌        | 5/32 [03:40<19:08, 42.53s/it] 28%|██▊       | 9/32 [03:41<09:17, 24.24s/it] 28%|██▊       | 9/32 [03:41<09:15, 24.17s/it] 28%|██▊       | 9/32 [03:43<09:39, 25.18s/it] 31%|███▏      | 10/32 [04:02<09:11, 25.06s/it] 31%|███▏      | 10/32 [04:03<08:58, 24.48s/it] 31%|███▏      | 10/32 [04:05<08:48, 24.01s/it] 31%|███▏      | 10/32 [04:07<09:05, 24.79s/it] 31%|███▏      | 10/32 [04:07<09:10, 25.01s/it] 28%|██▊       | 9/32 [04:08<11:37, 30.31s/it] 19%|█▉        | 6/32 [04:09<17:44, 40.94s/it] 19%|█▉        | 6/32 [04:21<18:06, 41.77s/it] 34%|███▍      | 11/32 [04:25<08:29, 24.28s/it] 34%|███▍      | 11/32 [04:27<08:13, 23.50s/it] 34%|███▍      | 11/32 [04:28<08:37, 24.66s/it] 34%|███▍      | 11/32 [04:32<08:39, 24.72s/it] 31%|███▏      | 10/32 [04:32<10:23, 28.35s/it] 34%|███▍      | 11/32 [04:34<08:56, 25.57s/it] 38%|███▊      | 12/32 [04:48<07:59, 23.96s/it] 38%|███▊      | 12/32 [04:52<07:55, 23.76s/it] 22%|██▏       | 7/32 [04:52<17:21, 41.64s/it] 38%|███▊      | 12/32 [04:53<08:15, 24.77s/it] 38%|███▊      | 12/32 [04:57<08:14, 24.71s/it] 38%|███▊      | 12/32 [04:59<08:33, 25.65s/it] 34%|███▍      | 11/32 [05:02<10:06, 28.89s/it] 22%|██▏       | 7/32 [05:06<17:52, 42.89s/it] 41%|████      | 13/32 [05:16<07:57, 25.11s/it] 41%|████      | 13/32 [05:17<07:44, 24.45s/it] 41%|████      | 13/32 [05:18<07:45, 24.51s/it] 41%|████      | 13/32 [05:21<07:46, 24.55s/it] 41%|████      | 13/32 [05:23<07:57, 25.12s/it] 38%|███▊      | 12/32 [05:29<09:27, 28.40s/it] 25%|██▌       | 8/32 [05:33<16:33, 41.41s/it] 44%|████▍     | 14/32 [05:41<07:32, 25.11s/it] 44%|████▍     | 14/32 [05:42<07:17, 24.29s/it] 44%|████▍     | 14/32 [05:42<07:23, 24.64s/it] 44%|████▍     | 14/32 [05:45<07:19, 24.44s/it] 44%|████▍     | 14/32 [05:47<07:22, 24.59s/it] 25%|██▌       | 8/32 [05:49<17:11, 42.96s/it] 41%|████      | 13/32 [05:59<09:12, 29.07s/it] 47%|████▋     | 15/32 [06:04<06:56, 24.50s/it] 47%|████▋     | 15/32 [06:08<07:05, 25.05s/it] 47%|████▋     | 15/32 [06:09<07:10, 25.32s/it] 47%|████▋     | 15/32 [06:11<06:57, 24.58s/it] 47%|████▋     | 15/32 [06:12<07:07, 25.14s/it] 28%|██▊       | 9/32 [06:15<15:56, 41.58s/it] 50%|█████     | 16/32 [06:28<06:31, 24.49s/it] 28%|██▊       | 9/32 [06:31<16:21, 42.68s/it] 44%|████▍     | 14/32 [06:32<09:01, 30.07s/it] 50%|█████     | 16/32 [06:32<06:34, 24.65s/it] 50%|█████     | 16/32 [06:33<06:40, 25.05s/it] 50%|█████     | 16/32 [06:36<06:37, 24.85s/it] 50%|█████     | 16/32 [06:37<06:39, 24.99s/it] 53%|█████▎    | 17/32 [06:52<06:03, 24.22s/it] 31%|███▏      | 10/32 [06:55<15:07, 41.27s/it] 53%|█████▎    | 17/32 [06:56<06:05, 24.35s/it] 53%|█████▎    | 17/32 [06:58<06:15, 25.02s/it] 53%|█████▎    | 17/32 [07:00<06:06, 24.46s/it] 53%|█████▎    | 17/32 [07:01<06:12, 24.81s/it] 47%|████▋     | 15/32 [07:05<08:46, 30.96s/it] 31%|███▏      | 10/32 [07:13<15:33, 42.41s/it] 56%|█████▋    | 18/32 [07:16<05:39, 24.24s/it] 56%|█████▋    | 18/32 [07:19<05:37, 24.12s/it] 56%|█████▋    | 18/32 [07:22<05:29, 23.56s/it] 56%|█████▋    | 18/32 [07:24<05:53, 25.22s/it] 56%|█████▋    | 18/32 [07:24<05:39, 24.22s/it] 34%|███▍      | 11/32 [07:36<14:25, 41.20s/it] 50%|█████     | 16/32 [07:37<08:19, 31.19s/it] 59%|█████▉    | 19/32 [07:41<05:05, 23.52s/it] 59%|█████▉    | 19/32 [07:41<05:18, 24.53s/it] 59%|█████▉    | 19/32 [07:46<05:07, 23.69s/it] 59%|█████▉    | 19/32 [07:47<05:19, 24.58s/it] 59%|█████▉    | 19/32 [07:47<05:09, 23.79s/it] 34%|███▍      | 11/32 [07:59<15:17, 43.67s/it] 62%|██████▎   | 20/32 [08:03<04:36, 23.06s/it] 53%|█████▎    | 17/32 [08:05<07:36, 30.47s/it] 62%|██████▎   | 20/32 [08:06<04:53, 24.50s/it] 62%|██████▎   | 20/32 [08:12<04:51, 24.32s/it] 62%|██████▎   | 20/32 [08:14<05:03, 25.28s/it] 62%|██████▎   | 20/32 [08:14<04:58, 24.88s/it] 38%|███▊      | 12/32 [08:18<13:49, 41.47s/it] 66%|██████▌   | 21/32 [08:26<04:12, 22.94s/it] 66%|██████▌   | 21/32 [08:30<04:29, 24.49s/it] 56%|█████▋    | 18/32 [08:36<07:07, 30.55s/it] 66%|██████▌   | 21/32 [08:38<04:29, 24.47s/it] 66%|██████▌   | 21/32 [08:39<04:37, 25.23s/it] 66%|██████▌   | 21/32 [08:40<04:41, 25.60s/it] 38%|███▊      | 12/32 [08:41<14:18, 42.91s/it] 69%|██████▉   | 22/32 [08:49<03:50, 23.07s/it] 69%|██████▉   | 22/32 [08:56<04:07, 24.70s/it] 41%|████      | 13/32 [09:00<13:05, 41.36s/it] 69%|██████▉   | 22/32 [09:02<04:04, 24.42s/it] 69%|██████▉   | 22/32 [09:02<04:05, 24.55s/it] 69%|██████▉   | 22/32 [09:03<04:10, 25.02s/it] 59%|█████▉    | 19/32 [09:09<06:46, 31.29s/it] 72%|███████▏  | 23/32 [09:13<03:29, 23.33s/it] 41%|████      | 13/32 [09:23<13:31, 42.69s/it] 72%|███████▏  | 23/32 [09:23<03:50, 25.60s/it] 72%|███████▏  | 23/32 [09:25<03:36, 24.03s/it] 72%|███████▏  | 23/32 [09:27<03:43, 24.78s/it] 72%|███████▏  | 23/32 [09:28<03:43, 24.89s/it] 78%|███████▊  | 25/32 [09:40<02:10, 18.71s/it] 44%|████▍     | 14/32 [09:41<12:23, 41.29s/it] 62%|██████▎   | 20/32 [09:41<06:19, 31.60s/it] 75%|███████▌  | 24/32 [09:48<03:23, 25.50s/it] 75%|███████▌  | 24/32 [09:50<03:12, 24.02s/it] 75%|███████▌  | 24/32 [09:50<03:14, 24.27s/it] 75%|███████▌  | 24/32 [09:52<03:17, 24.64s/it] 44%|████▍     | 14/32 [10:03<12:33, 41.84s/it] 81%|████████▏ | 26/32 [10:06<02:03, 20.52s/it] 78%|███████▊  | 25/32 [10:11<02:52, 24.61s/it] 78%|███████▊  | 25/32 [10:14<02:48, 24.03s/it] 66%|██████▌   | 21/32 [10:15<05:52, 32.05s/it] 78%|███████▊  | 25/32 [10:16<02:52, 24.64s/it] 78%|███████▊  | 25/32 [10:17<02:53, 24.79s/it] 47%|████▋     | 15/32 [10:24<11:50, 41.82s/it] 84%|████████▍ | 27/32 [10:30<01:47, 21.42s/it] 81%|████████▏ | 26/32 [10:36<02:28, 24.82s/it] 81%|████████▏ | 26/32 [10:40<02:26, 24.50s/it] 81%|████████▏ | 26/32 [10:41<02:28, 24.79s/it] 81%|████████▏ | 26/32 [10:43<02:31, 25.26s/it] 69%|██████▉   | 22/32 [10:45<05:14, 31.45s/it] 47%|████▋     | 15/32 [10:49<12:15, 43.26s/it] 88%|████████▊ | 28/32 [10:56<01:30, 22.65s/it] 84%|████████▍ | 27/32 [11:00<02:02, 24.51s/it] 84%|████████▍ | 27/32 [11:04<02:01, 24.33s/it] 50%|█████     | 16/32 [11:05<11:05, 41.61s/it] 84%|████████▍ | 27/32 [11:05<02:01, 24.39s/it] 84%|████████▍ | 27/32 [11:06<02:04, 24.97s/it]