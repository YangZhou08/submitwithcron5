Already on 'addinggriffin'
error: cannot lock ref 'refs/remotes/origin/addinggriffin': is at c553cb6465123ce743c7996d24f32f131d451c8b but expected a3ab2518c4acc407e96b46808ddb3c5e9fdf6628
From github.com:YangZhou08/CommonSenseReasoning
 ! a3ab251..c553cb6  addinggriffin -> origin/addinggriffin  (unable to update local ref)
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:06<01:06, 66.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:08<01:08, 68.00s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:08<01:08, 68.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:08<01:08, 68.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 39.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 43.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.57s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.57s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.62s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
  0%|          | 0/32 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:23<12:02, 23.30s/it]  3%|▎         | 1/32 [00:24<12:26, 24.07s/it]  3%|▎         | 1/32 [00:24<12:30, 24.22s/it]  3%|▎         | 1/32 [00:25<12:56, 25.04s/it]  3%|▎         | 1/32 [00:25<13:05, 25.34s/it]  3%|▎         | 1/32 [00:25<13:07, 25.41s/it]  3%|▎         | 1/32 [00:42<21:44, 42.07s/it]  3%|▎         | 1/32 [00:42<22:04, 42.73s/it]  6%|▋         | 2/32 [00:46<11:28, 22.95s/it]  6%|▋         | 2/32 [00:47<11:57, 23.91s/it]  6%|▋         | 2/32 [00:48<12:15, 24.52s/it]  6%|▋         | 2/32 [00:48<12:03, 24.12s/it]  6%|▋         | 2/32 [00:50<12:40, 25.35s/it]  6%|▋         | 2/32 [00:51<12:56, 25.87s/it]  9%|▉         | 3/32 [01:10<11:16, 23.33s/it]  9%|▉         | 3/32 [01:12<11:40, 24.17s/it]  9%|▉         | 3/32 [01:13<11:52, 24.56s/it]  9%|▉         | 3/32 [01:13<11:52, 24.57s/it]  9%|▉         | 3/32 [01:14<11:49, 24.47s/it]  9%|▉         | 3/32 [01:15<12:10, 25.18s/it]  6%|▋         | 2/32 [01:26<21:36, 43.22s/it]  6%|▋         | 2/32 [01:31<23:12, 46.43s/it] 12%|█▎        | 4/32 [01:34<11:05, 23.77s/it] 12%|█▎        | 4/32 [01:35<11:04, 23.75s/it] 12%|█▎        | 4/32 [01:37<11:22, 24.39s/it] 12%|█▎        | 4/32 [01:38<11:33, 24.78s/it] 12%|█▎        | 4/32 [01:40<11:43, 25.12s/it] 12%|█▎        | 4/32 [01:41<11:50, 25.38s/it] 16%|█▌        | 5/32 [01:58<10:41, 23.75s/it] 16%|█▌        | 5/32 [02:00<10:53, 24.19s/it] 16%|█▌        | 5/32 [02:02<11:05, 24.66s/it] 16%|█▌        | 5/32 [02:04<11:01, 24.50s/it] 16%|█▌        | 5/32 [02:04<11:14, 24.97s/it] 16%|█▌        | 5/32 [02:05<11:30, 25.57s/it]  9%|▉         | 3/32 [02:10<21:05, 43.62s/it] 19%|█▉        | 6/32 [02:20<10:05, 23.30s/it]  9%|▉         | 3/32 [02:21<23:10, 47.95s/it] 19%|█▉        | 6/32 [02:24<10:29, 24.23s/it] 19%|█▉        | 6/32 [02:27<10:46, 24.87s/it] 19%|█▉        | 6/32 [02:29<10:41, 24.66s/it] 19%|█▉        | 6/32 [02:29<10:46, 24.88s/it] 19%|█▉        | 6/32 [02:30<10:59, 25.37s/it] 22%|██▏       | 7/32 [02:46<10:03, 24.14s/it] 12%|█▎        | 4/32 [02:48<19:19, 41.41s/it] 22%|██▏       | 7/32 [02:48<10:01, 24.06s/it] 22%|██▏       | 7/32 [02:52<10:19, 24.77s/it] 22%|██▏       | 7/32 [02:52<10:05, 24.20s/it] 22%|██▏       | 7/32 [02:54<10:19, 24.78s/it] 22%|██▏       | 7/32 [03:01<11:14, 26.97s/it] 12%|█▎        | 4/32 [03:04<21:25, 45.92s/it] 25%|██▌       | 8/32 [03:11<09:41, 24.23s/it] 25%|██▌       | 8/32 [03:15<09:59, 24.97s/it] 25%|██▌       | 8/32 [03:17<09:50, 24.60s/it] 25%|██▌       | 8/32 [03:18<10:02, 25.11s/it] 25%|██▌       | 8/32 [03:18<09:51, 24.64s/it] 16%|█▌        | 5/32 [03:28<18:27, 41.01s/it] 25%|██▌       | 8/32 [03:35<11:47, 29.48s/it] 28%|██▊       | 9/32 [03:36<09:23, 24.50s/it] 28%|██▊       | 9/32 [03:39<09:25, 24.57s/it] 16%|█▌        | 5/32 [03:40<19:08, 42.53s/it] 28%|██▊       | 9/32 [03:41<09:17, 24.24s/it] 28%|██▊       | 9/32 [03:41<09:15, 24.17s/it] 28%|██▊       | 9/32 [03:43<09:39, 25.18s/it] 31%|███▏      | 10/32 [04:02<09:11, 25.06s/it] 31%|███▏      | 10/32 [04:03<08:58, 24.48s/it] 31%|███▏      | 10/32 [04:05<08:48, 24.01s/it] 31%|███▏      | 10/32 [04:07<09:05, 24.79s/it] 31%|███▏      | 10/32 [04:07<09:10, 25.01s/it] 28%|██▊       | 9/32 [04:08<11:37, 30.31s/it] 19%|█▉        | 6/32 [04:09<17:44, 40.94s/it] 19%|█▉        | 6/32 [04:21<18:06, 41.77s/it] 34%|███▍      | 11/32 [04:25<08:29, 24.28s/it] 34%|███▍      | 11/32 [04:27<08:13, 23.50s/it] 34%|███▍      | 11/32 [04:28<08:37, 24.66s/it] 34%|███▍      | 11/32 [04:32<08:39, 24.72s/it] 31%|███▏      | 10/32 [04:32<10:23, 28.35s/it] 34%|███▍      | 11/32 [04:34<08:56, 25.57s/it] 38%|███▊      | 12/32 [04:48<07:59, 23.96s/it] 38%|███▊      | 12/32 [04:52<07:55, 23.76s/it] 22%|██▏       | 7/32 [04:52<17:21, 41.64s/it] 38%|███▊      | 12/32 [04:53<08:15, 24.77s/it] 38%|███▊      | 12/32 [04:57<08:14, 24.71s/it] 38%|███▊      | 12/32 [04:59<08:33, 25.65s/it] 34%|███▍      | 11/32 [05:02<10:06, 28.89s/it] 22%|██▏       | 7/32 [05:06<17:52, 42.89s/it] 41%|████      | 13/32 [05:16<07:57, 25.11s/it] 41%|████      | 13/32 [05:17<07:44, 24.45s/it] 41%|████      | 13/32 [05:18<07:45, 24.51s/it] 41%|████      | 13/32 [05:21<07:46, 24.55s/it] 41%|████      | 13/32 [05:23<07:57, 25.12s/it] 38%|███▊      | 12/32 [05:29<09:27, 28.40s/it] 25%|██▌       | 8/32 [05:33<16:33, 41.41s/it] 44%|████▍     | 14/32 [05:41<07:32, 25.11s/it] 44%|████▍     | 14/32 [05:42<07:17, 24.29s/it] 44%|████▍     | 14/32 [05:42<07:23, 24.64s/it] 44%|████▍     | 14/32 [05:45<07:19, 24.44s/it] 44%|████▍     | 14/32 [05:47<07:22, 24.59s/it] 25%|██▌       | 8/32 [05:49<17:11, 42.96s/it] 41%|████      | 13/32 [05:59<09:12, 29.07s/it] 47%|████▋     | 15/32 [06:04<06:56, 24.50s/it] 47%|████▋     | 15/32 [06:08<07:05, 25.05s/it] 47%|████▋     | 15/32 [06:09<07:10, 25.32s/it] 47%|████▋     | 15/32 [06:11<06:57, 24.58s/it] 47%|████▋     | 15/32 [06:12<07:07, 25.14s/it] 28%|██▊       | 9/32 [06:15<15:56, 41.58s/it] 50%|█████     | 16/32 [06:28<06:31, 24.49s/it] 28%|██▊       | 9/32 [06:31<16:21, 42.68s/it] 44%|████▍     | 14/32 [06:32<09:01, 30.07s/it] 50%|█████     | 16/32 [06:32<06:34, 24.65s/it] 50%|█████     | 16/32 [06:33<06:40, 25.05s/it] 50%|█████     | 16/32 [06:36<06:37, 24.85s/it] 50%|█████     | 16/32 [06:37<06:39, 24.99s/it] 53%|█████▎    | 17/32 [06:52<06:03, 24.22s/it] 31%|███▏      | 10/32 [06:55<15:07, 41.27s/it] 53%|█████▎    | 17/32 [06:56<06:05, 24.35s/it] 53%|█████▎    | 17/32 [06:58<06:15, 25.02s/it] 53%|█████▎    | 17/32 [07:00<06:06, 24.46s/it] 53%|█████▎    | 17/32 [07:01<06:12, 24.81s/it] 47%|████▋     | 15/32 [07:05<08:46, 30.96s/it] 31%|███▏      | 10/32 [07:13<15:33, 42.41s/it] 56%|█████▋    | 18/32 [07:16<05:39, 24.24s/it] 56%|█████▋    | 18/32 [07:19<05:37, 24.12s/it] 56%|█████▋    | 18/32 [07:22<05:29, 23.56s/it] 56%|█████▋    | 18/32 [07:24<05:53, 25.22s/it] 56%|█████▋    | 18/32 [07:24<05:39, 24.22s/it] 34%|███▍      | 11/32 [07:36<14:25, 41.20s/it] 50%|█████     | 16/32 [07:37<08:19, 31.19s/it] 59%|█████▉    | 19/32 [07:41<05:05, 23.52s/it] 59%|█████▉    | 19/32 [07:41<05:18, 24.53s/it] 59%|█████▉    | 19/32 [07:46<05:07, 23.69s/it] 59%|█████▉    | 19/32 [07:47<05:19, 24.58s/it] 59%|█████▉    | 19/32 [07:47<05:09, 23.79s/it] 34%|███▍      | 11/32 [07:59<15:17, 43.67s/it] 62%|██████▎   | 20/32 [08:03<04:36, 23.06s/it] 53%|█████▎    | 17/32 [08:05<07:36, 30.47s/it] 62%|██████▎   | 20/32 [08:06<04:53, 24.50s/it] 62%|██████▎   | 20/32 [08:12<04:51, 24.32s/it] 62%|██████▎   | 20/32 [08:14<05:03, 25.28s/it] 62%|██████▎   | 20/32 [08:14<04:58, 24.88s/it] 38%|███▊      | 12/32 [08:18<13:49, 41.47s/it] 66%|██████▌   | 21/32 [08:26<04:12, 22.94s/it] 66%|██████▌   | 21/32 [08:30<04:29, 24.49s/it] 56%|█████▋    | 18/32 [08:36<07:07, 30.55s/it] 66%|██████▌   | 21/32 [08:38<04:29, 24.47s/it] 66%|██████▌   | 21/32 [08:39<04:37, 25.23s/it] 66%|██████▌   | 21/32 [08:40<04:41, 25.60s/it] 38%|███▊      | 12/32 [08:41<14:18, 42.91s/it] 69%|██████▉   | 22/32 [08:49<03:50, 23.07s/it] 69%|██████▉   | 22/32 [08:56<04:07, 24.70s/it] 41%|████      | 13/32 [09:00<13:05, 41.36s/it] 69%|██████▉   | 22/32 [09:02<04:04, 24.42s/it] 69%|██████▉   | 22/32 [09:02<04:05, 24.55s/it] 69%|██████▉   | 22/32 [09:03<04:10, 25.02s/it] 59%|█████▉    | 19/32 [09:09<06:46, 31.29s/it] 72%|███████▏  | 23/32 [09:13<03:29, 23.33s/it] 41%|████      | 13/32 [09:23<13:31, 42.69s/it] 72%|███████▏  | 23/32 [09:23<03:50, 25.60s/it] 72%|███████▏  | 23/32 [09:25<03:36, 24.03s/it] 72%|███████▏  | 23/32 [09:27<03:43, 24.78s/it] 72%|███████▏  | 23/32 [09:28<03:43, 24.89s/it] 78%|███████▊  | 25/32 [09:40<02:10, 18.71s/it] 44%|████▍     | 14/32 [09:41<12:23, 41.29s/it] 62%|██████▎   | 20/32 [09:41<06:19, 31.60s/it] 75%|███████▌  | 24/32 [09:48<03:23, 25.50s/it] 75%|███████▌  | 24/32 [09:50<03:12, 24.02s/it] 75%|███████▌  | 24/32 [09:50<03:14, 24.27s/it] 75%|███████▌  | 24/32 [09:52<03:17, 24.64s/it] 44%|████▍     | 14/32 [10:03<12:33, 41.84s/it] 81%|████████▏ | 26/32 [10:06<02:03, 20.52s/it] 78%|███████▊  | 25/32 [10:11<02:52, 24.61s/it] 78%|███████▊  | 25/32 [10:14<02:48, 24.03s/it] 66%|██████▌   | 21/32 [10:15<05:52, 32.05s/it] 78%|███████▊  | 25/32 [10:16<02:52, 24.64s/it] 78%|███████▊  | 25/32 [10:17<02:53, 24.79s/it] 47%|████▋     | 15/32 [10:24<11:50, 41.82s/it] 84%|████████▍ | 27/32 [10:30<01:47, 21.42s/it] 81%|████████▏ | 26/32 [10:36<02:28, 24.82s/it] 81%|████████▏ | 26/32 [10:40<02:26, 24.50s/it] 81%|████████▏ | 26/32 [10:41<02:28, 24.79s/it] 81%|████████▏ | 26/32 [10:43<02:31, 25.26s/it] 69%|██████▉   | 22/32 [10:45<05:14, 31.45s/it] 47%|████▋     | 15/32 [10:49<12:15, 43.26s/it] 88%|████████▊ | 28/32 [10:56<01:30, 22.65s/it] 84%|████████▍ | 27/32 [11:00<02:02, 24.51s/it] 84%|████████▍ | 27/32 [11:04<02:01, 24.33s/it] 50%|█████     | 16/32 [11:05<11:05, 41.61s/it] 84%|████████▍ | 27/32 [11:05<02:01, 24.39s/it] 84%|████████▍ | 27/32 [11:06<02:04, 24.97s/it] 72%|███████▏  | 23/32 [11:09<04:24, 29.35s/it] 91%|█████████ | 29/32 [11:23<01:11, 23.76s/it] 88%|████████▊ | 28/32 [11:27<01:35, 23.83s/it] 88%|████████▊ | 28/32 [11:27<01:40, 25.11s/it] 88%|████████▊ | 28/32 [11:29<01:38, 24.52s/it] 88%|████████▊ | 28/32 [11:32<01:39, 24.97s/it] 75%|███████▌  | 24/32 [11:34<03:44, 28.11s/it] 50%|█████     | 16/32 [11:35<11:43, 43.96s/it] 53%|█████▎    | 17/32 [11:45<10:16, 41.10s/it] 94%|█████████▍| 30/32 [11:49<00:49, 24.58s/it] 91%|█████████ | 29/32 [11:50<01:11, 23.85s/it] 94%|█████████▍| 30/32 [11:51<00:38, 19.19s/it] 91%|█████████ | 29/32 [11:53<01:13, 24.39s/it] 91%|█████████ | 29/32 [11:57<01:14, 24.98s/it] 78%|███████▊  | 25/32 [11:57<03:04, 26.42s/it] 97%|█████████▋| 31/32 [12:15<00:24, 24.97s/it] 94%|█████████▍| 30/32 [12:16<00:48, 24.26s/it] 53%|█████▎    | 17/32 [12:16<10:46, 43.08s/it] 97%|█████████▋| 31/32 [12:16<00:20, 20.67s/it] 94%|█████████▍| 30/32 [12:18<00:49, 24.51s/it] 56%|█████▋    | 18/32 [12:21<09:15, 39.71s/it] 81%|████████▏ | 26/32 [12:22<02:36, 26.02s/it] 94%|█████████▍| 30/32 [12:22<00:50, 25.17s/it]100%|██████████| 32/32 [12:38<00:00, 24.34s/it]100%|██████████| 32/32 [12:38<00:00, 23.70s/it]
 97%|█████████▋| 31/32 [12:40<00:24, 24.39s/it] 97%|█████████▋| 31/32 [12:42<00:24, 24.21s/it]100%|██████████| 32/32 [12:44<00:00, 22.55s/it]100%|██████████| 32/32 [12:44<00:00, 23.90s/it]
 84%|████████▍ | 27/32 [12:46<02:07, 25.57s/it] 97%|█████████▋| 31/32 [12:48<00:25, 25.33s/it] 56%|█████▋    | 18/32 [12:58<09:59, 42.80s/it] 59%|█████▉    | 19/32 [13:00<08:32, 39.44s/it]100%|██████████| 32/32 [13:03<00:00, 24.02s/it]100%|██████████| 32/32 [13:03<00:00, 24.50s/it]
100%|██████████| 32/32 [13:07<00:00, 24.54s/it]100%|██████████| 32/32 [13:07<00:00, 24.61s/it]
 88%|████████▊ | 28/32 [13:12<01:42, 25.52s/it]100%|██████████| 32/32 [13:16<00:00, 26.17s/it]100%|██████████| 32/32 [13:16<00:00, 24.90s/it]
 91%|█████████ | 29/32 [13:44<01:22, 27.51s/it] 59%|█████▉    | 19/32 [13:45<09:31, 43.99s/it] 62%|██████▎   | 20/32 [13:47<08:20, 41.67s/it] 94%|█████████▍| 30/32 [14:17<00:58, 29.13s/it] 66%|██████▌   | 21/32 [14:30<07:43, 42.12s/it] 62%|██████▎   | 20/32 [14:35<09:09, 45.81s/it] 97%|█████████▋| 31/32 [14:50<00:30, 30.29s/it] 69%|██████▉   | 22/32 [15:09<06:50, 41.06s/it]100%|██████████| 32/32 [15:17<00:00, 29.33s/it]100%|██████████| 32/32 [15:17<00:00, 28.67s/it]
 66%|██████▌   | 21/32 [15:17<08:12, 44.81s/it] 72%|███████▏  | 23/32 [15:51<06:12, 41.38s/it] 69%|██████▉   | 22/32 [16:00<07:23, 44.36s/it] 75%|███████▌  | 24/32 [16:31<05:27, 40.92s/it] 72%|███████▏  | 23/32 [16:43<06:33, 43.74s/it] 78%|███████▊  | 25/32 [17:14<04:52, 41.74s/it] 75%|███████▌  | 24/32 [17:19<05:32, 41.60s/it] 81%|████████▏ | 26/32 [17:55<04:08, 41.49s/it] 78%|███████▊  | 25/32 [18:04<04:56, 42.41s/it] 84%|████████▍ | 27/32 [18:40<03:31, 42.39s/it] 81%|████████▏ | 26/32 [18:47<04:15, 42.63s/it] 88%|████████▊ | 28/32 [19:26<02:54, 43.65s/it] 84%|████████▍ | 27/32 [19:32<03:37, 43.41s/it] 91%|█████████ | 29/32 [20:12<02:13, 44.35s/it] 88%|████████▊ | 28/32 [20:20<02:58, 44.73s/it] 94%|█████████▍| 30/32 [20:55<01:27, 43.99s/it] 91%|█████████ | 29/32 [21:02<02:11, 43.84s/it] 97%|█████████▋| 31/32 [21:36<00:42, 42.82s/it] 94%|█████████▍| 30/32 [21:36<01:22, 41.05s/it]100%|██████████| 32/32 [22:01<00:00, 37.61s/it]100%|██████████| 32/32 [22:01<00:00, 41.30s/it]
 97%|█████████▋| 31/32 [22:10<00:38, 38.92s/it][rank6]:[E ProcessGroupNCCL.cpp:523] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600829 milliseconds before timing out.
100%|██████████| 32/32 [22:42<00:00, 36.69s/it]100%|██████████| 32/32 [22:42<00:00, 42.57s/it]
[rank6]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E ProcessGroupNCCL.cpp:1182] [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600829 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f0346393d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f034753b6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f034753ec3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f034753f839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f0391252df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f0392641609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f039240c353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600829 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f0346393d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f034753b6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f034753ec3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f034753f839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f0391252df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f0392641609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f039240c353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f0346393d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f0347295b11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f0391252df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f0392641609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f039240c353 in /lib/x86_64-linux-gnu/libc.so.6)

[2024-07-09 15:42:31,447] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 149993 closing signal SIGTERM
[2024-07-09 15:42:31,450] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 149994 closing signal SIGTERM
[2024-07-09 15:42:31,450] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 149995 closing signal SIGTERM
[2024-07-09 15:42:31,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 149996 closing signal SIGTERM
[2024-07-09 15:42:31,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 149997 closing signal SIGTERM
[2024-07-09 15:42:31,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 149998 closing signal SIGTERM
[2024-07-09 15:42:31,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 150000 closing signal SIGTERM
[2024-07-09 15:42:33,081] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 6 (pid: 149999) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
main.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-09_15:42:31
  host      : a100-st-p4de24xlarge-708.fair-a100.hpcaas
  rank      : 6 (local_rank: 6)
  exitcode  : -6 (pid: 149999)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 149999
=======================================================
