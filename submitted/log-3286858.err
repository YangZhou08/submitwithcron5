Already on 'yangexp2two'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-06:21:34:11,556 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:21:34:11,556 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:21:34:11,556 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:21:34:11,556 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:21:34:11,556 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:21:34:11,556 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:21:34:11,556 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:21:34:11,556 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:21:34:20,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:21:34:20,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:21:34:20,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:21:34:20,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:21:34:20,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:21:34:20,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:21:34:20,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:21:34:20,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:21:34:20,849 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:21:34:20,849 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:21:34:20,849 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:21:34:20,849 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:21:34:20,849 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:21:34:20,849 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:21:34:20,849 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:21:34:20,849 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 6, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:21:34:20,849 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 6, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:21:34:20,849 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 6, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:21:34:20,849 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 6, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:21:34:20,849 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 6, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:21:34:20,849 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 6, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:21:34:20,849 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 6, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:21:34:20,849 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:21:34:20,849 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 6, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:16, 25.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:27<01:21, 27.16s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.54s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:20, 26.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.52s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.50s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:41, 20.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:44<00:43, 21.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:44<00:42, 21.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:44<00:42, 21.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:44<00:42, 21.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:44<00:43, 21.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:45<00:43, 21.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:44<00:43, 21.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:18, 18.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:00<00:18, 18.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.22s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.22s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 11.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 15.40s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 11.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 15.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 11.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 15.29s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-06:21:36:12,254 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
2024-07-06:21:36:12,254 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
2024-07-06:21:36:12,254 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
2024-07-06:21:36:12,267 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/164 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 186.84it/s] 12%|█▏        | 19/165 [00:00<00:00, 187.77it/s] 12%|█▏        | 19/165 [00:00<00:00, 187.09it/s] 12%|█▏        | 19/164 [00:00<00:00, 182.59it/s] 23%|██▎       | 38/165 [00:00<00:00, 188.38it/s] 23%|██▎       | 38/165 [00:00<00:00, 187.29it/s] 23%|██▎       | 38/165 [00:00<00:00, 187.41it/s] 23%|██▎       | 38/164 [00:00<00:00, 180.68it/s] 35%|███▍      | 57/165 [00:00<00:00, 188.34it/s] 35%|███▌      | 58/165 [00:00<00:00, 189.18it/s] 35%|███▍      | 57/165 [00:00<00:00, 180.11it/s] 35%|███▍      | 57/164 [00:00<00:00, 180.23it/s] 47%|████▋     | 77/165 [00:00<00:00, 190.16it/s] 47%|████▋     | 78/165 [00:00<00:00, 191.48it/s] 46%|████▋     | 76/164 [00:00<00:00, 183.28it/s] 46%|████▌     | 76/165 [00:00<00:00, 167.31it/s] 59%|█████▉    | 97/165 [00:00<00:00, 190.17it/s] 59%|█████▉    | 98/165 [00:00<00:00, 191.61it/s] 59%|█████▊    | 96/164 [00:00<00:00, 187.10it/s] 58%|█████▊    | 96/165 [00:00<00:00, 175.49it/s] 71%|███████   | 117/165 [00:00<00:00, 190.67it/s] 72%|███████▏  | 118/165 [00:00<00:00, 191.53it/s] 70%|██████▉   | 115/165 [00:00<00:00, 179.76it/s] 71%|███████   | 116/164 [00:00<00:00, 188.30it/s] 83%|████████▎ | 137/165 [00:00<00:00, 191.01it/s] 84%|████████▎ | 138/165 [00:00<00:00, 192.69it/s] 82%|████████▏ | 135/165 [00:00<00:00, 183.93it/s] 83%|████████▎ | 136/164 [00:00<00:00, 190.37it/s] 95%|█████████▌| 157/165 [00:00<00:00, 190.00it/s] 96%|█████████▌| 158/165 [00:00<00:00, 191.48it/s] 93%|█████████▎| 154/165 [00:00<00:00, 184.80it/s] 95%|█████████▌| 156/164 [00:00<00:00, 189.37it/s]100%|██████████| 165/165 [00:00<00:00, 191.23it/s]
100%|██████████| 165/165 [00:00<00:00, 189.94it/s]
100%|██████████| 164/164 [00:00<00:00, 187.32it/s]
100%|██████████| 165/165 [00:00<00:00, 181.90it/s]
2024-07-06:21:36:13,262 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
2024-07-06:21:36:13,273 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.23it/s] 12%|█▏        | 20/165 [00:00<00:00, 192.22it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.02it/s] 24%|██▍       | 40/165 [00:00<00:00, 192.31it/s] 36%|███▌      | 59/165 [00:00<00:00, 191.58it/s] 36%|███▋      | 60/165 [00:00<00:00, 193.35it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.44it/s] 48%|████▊     | 80/165 [00:00<00:00, 194.45it/s] 60%|██████    | 99/165 [00:00<00:00, 192.90it/s] 61%|██████    | 100/165 [00:00<00:00, 195.09it/s] 72%|███████▏  | 119/165 [00:00<00:00, 190.70it/s] 73%|███████▎  | 120/165 [00:00<00:00, 195.57it/s] 84%|████████▍ | 139/165 [00:00<00:00, 191.44it/s] 85%|████████▍ | 140/165 [00:00<00:00, 195.88it/s] 97%|█████████▋| 160/165 [00:00<00:00, 196.07it/s] 96%|█████████▋| 159/165 [00:00<00:00, 189.37it/s]100%|██████████| 165/165 [00:00<00:00, 195.12it/s]
100%|██████████| 165/165 [00:00<00:00, 190.51it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-06:21:36:32,633 INFO     [xhuggingface.py:323] Using 8 devices with data parallelism
2024-07-06:21:36:34,257 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/165 [00:00<?, ?it/s]  8%|▊         | 13/165 [00:00<00:01, 123.20it/s] 16%|█▌        | 26/165 [00:00<00:01, 126.43it/s] 24%|██▎       | 39/165 [00:00<00:00, 127.53it/s] 32%|███▏      | 52/165 [00:00<00:00, 128.05it/s] 39%|███▉      | 65/165 [00:00<00:00, 127.63it/s] 47%|████▋     | 78/165 [00:00<00:00, 128.37it/s] 56%|█████▌    | 92/165 [00:00<00:00, 128.96it/s] 64%|██████▎   | 105/165 [00:00<00:00, 129.11it/s] 72%|███████▏  | 118/165 [00:00<00:00, 129.33it/s] 80%|████████  | 132/165 [00:01<00:00, 131.70it/s] 91%|█████████ | 150/165 [00:01<00:00, 143.21it/s]100%|██████████| 165/165 [00:01<00:00, 135.41it/s]100%|██████████| 165/165 [00:01<00:00, 131.69it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-06:21:36:37,764 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/165 [00:00<?, ?it/s]  5%|▌         | 9/165 [00:00<00:01, 85.51it/s] 12%|█▏        | 20/165 [00:00<00:01, 99.57it/s] 18%|█▊        | 30/165 [00:00<00:01, 93.66it/s] 25%|██▌       | 42/165 [00:00<00:01, 100.26it/s] 32%|███▏      | 53/165 [00:00<00:01, 93.56it/s]  38%|███▊      | 63/165 [00:00<00:01, 91.50it/s] 44%|████▍     | 73/165 [00:00<00:00, 93.30it/s] 54%|█████▍    | 89/165 [00:00<00:00, 112.99it/s] 64%|██████▍   | 106/165 [00:00<00:00, 129.64it/s] 75%|███████▍  | 123/165 [00:01<00:00, 140.64it/s] 85%|████████▍ | 140/165 [00:01<00:00, 149.01it/s] 95%|█████████▌| 157/165 [00:01<00:00, 154.93it/s]100%|██████████| 165/165 [00:01<00:00, 124.16it/s]
2024-07-06:21:36:53,222 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:21:36:53,222 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:21:36:53,222 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:21:36:53,222 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:21:36:53,223 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:21:36:53,223 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:21:36:53,224 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:21:36:53,224 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/165 [00:13<35:47, 13.10s/it]Running generate_until requests:   1%|          | 2/165 [00:22<29:07, 10.72s/it]Running generate_until requests:   2%|▏         | 3/165 [00:41<39:40, 14.69s/it]Running generate_until requests:   2%|▏         | 4/165 [00:53<36:04, 13.45s/it]Running generate_until requests:   3%|▎         | 5/165 [00:59<29:20, 11.00s/it]Running generate_until requests:   4%|▎         | 6/165 [01:13<31:18, 11.82s/it]Running generate_until requests:   4%|▍         | 7/165 [01:21<27:45, 10.54s/it]Running generate_until requests:   5%|▍         | 8/165 [01:29<26:09, 10.00s/it]Running generate_until requests:   5%|▌         | 9/165 [01:35<22:04,  8.49s/it]Running generate_until requests:   6%|▌         | 10/165 [01:42<21:21,  8.27s/it]Running generate_until requests:   7%|▋         | 11/165 [01:50<20:36,  8.03s/it]Running generate_until requests:   7%|▋         | 12/165 [01:55<18:06,  7.10s/it]Running generate_until requests:   7%|▋         | 12/165 [02:02<25:56, 10.17s/it]
