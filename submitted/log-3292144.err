Already on 'yangexp2two'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-08:17:24:01,122 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:17:24:01,122 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:17:24:01,122 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:17:24:01,123 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:17:24:01,124 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:17:24:01,128 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:17:24:01,135 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:17:24:01,137 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:17:24:11,364 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:17:24:11,364 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:17:24:11,364 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:17:24:11,364 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:17:24:11,364 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:17:24:11,364 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:17:24:11,364 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:17:24:11,364 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:17:24:11,387 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:17:24:11,387 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:17:24:11,387 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:17:24:11,387 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:17:24:11,387 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:17:24:11,387 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:17:24:11,387 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:17:24:11,387 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:17:24:11,387 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:17:24:11,387 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:17:24:11,387 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:17:24:11,387 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:17:24:11,387 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:17:24:11,387 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:17:24:11,387 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:17:24:11,387 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:08, 22.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:10, 23.65s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.71s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:40<00:39, 19.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:41, 20.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:41, 20.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:41, 20.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:41, 20.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:41, 20.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:41, 20.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:41, 20.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:57<00:18, 18.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:18, 18.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:19, 19.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:19, 19.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 11.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 14.93s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 12.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 15.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 12.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 15.41s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-08:17:26:01,147 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 188.95it/s]2024-07-08:17:26:01,322 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
2024-07-08:17:26:01,339 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/165 [00:00<?, ?it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.19it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.25it/s] 12%|█▏        | 20/165 [00:00<00:00, 192.55it/s]2024-07-08:17:26:01,499 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
 36%|███▌      | 59/165 [00:00<00:00, 191.90it/s]  0%|          | 0/165 [00:00<?, ?it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.12it/s] 24%|██▍       | 40/165 [00:00<00:00, 194.22it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.79it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.36it/s] 36%|███▌      | 59/165 [00:00<00:00, 191.51it/s] 36%|███▋      | 60/165 [00:00<00:00, 194.57it/s] 60%|██████    | 99/165 [00:00<00:00, 193.27it/s] 24%|██▎       | 39/165 [00:00<00:00, 190.53it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.28it/s] 48%|████▊     | 80/165 [00:00<00:00, 195.32it/s] 72%|███████▏  | 119/165 [00:00<00:00, 193.64it/s] 36%|███▌      | 59/165 [00:00<00:00, 191.15it/s] 60%|██████    | 99/165 [00:00<00:00, 192.81it/s]2024-07-08:17:26:01,866 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
 61%|██████    | 100/165 [00:00<00:00, 195.76it/s]  0%|          | 0/165 [00:00<?, ?it/s] 84%|████████▍ | 139/165 [00:00<00:00, 193.97it/s] 48%|████▊     | 79/165 [00:00<00:00, 191.57it/s] 72%|███████▏  | 119/165 [00:00<00:00, 193.22it/s] 73%|███████▎  | 120/165 [00:00<00:00, 196.09it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.68it/s] 96%|█████████▋| 159/165 [00:00<00:00, 194.01it/s] 60%|██████    | 99/165 [00:00<00:00, 192.13it/s] 84%|████████▍ | 139/165 [00:00<00:00, 193.48it/s]100%|██████████| 165/165 [00:00<00:00, 193.23it/s]
 85%|████████▍ | 140/165 [00:00<00:00, 196.27it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.02it/s] 72%|███████▏  | 119/165 [00:00<00:00, 190.47it/s] 96%|█████████▋| 159/165 [00:00<00:00, 193.42it/s] 97%|█████████▋| 160/165 [00:00<00:00, 196.33it/s] 36%|███▌      | 59/165 [00:00<00:00, 191.81it/s]100%|██████████| 165/165 [00:00<00:00, 192.75it/s]
100%|██████████| 165/165 [00:00<00:00, 195.71it/s]
 84%|████████▍ | 139/165 [00:00<00:00, 180.10it/s] 48%|████▊     | 79/165 [00:00<00:00, 176.24it/s] 96%|█████████▌| 158/165 [00:00<00:00, 180.03it/s]100%|██████████| 165/165 [00:00<00:00, 185.50it/s]
 60%|██████    | 99/165 [00:00<00:00, 182.28it/s] 72%|███████▏  | 119/165 [00:00<00:00, 185.87it/s] 84%|████████▍ | 139/165 [00:00<00:00, 188.51it/s] 96%|█████████▋| 159/165 [00:00<00:00, 190.14it/s]100%|██████████| 165/165 [00:00<00:00, 187.59it/s]
2024-07-08:17:26:02,874 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/164 [00:00<?, ?it/s] 12%|█▏        | 19/164 [00:00<00:00, 189.86it/s] 24%|██▍       | 39/164 [00:00<00:00, 191.75it/s] 36%|███▌      | 59/164 [00:00<00:00, 192.30it/s] 48%|████▊     | 79/164 [00:00<00:00, 193.20it/s] 60%|██████    | 99/164 [00:00<00:00, 193.72it/s] 73%|███████▎  | 119/164 [00:00<00:00, 193.48it/s] 85%|████████▍ | 139/164 [00:00<00:00, 194.10it/s] 97%|█████████▋| 159/164 [00:00<00:00, 194.21it/s]100%|██████████| 164/164 [00:00<00:00, 193.47it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-08:17:26:29,914 INFO     [xhuggingface.py:323] Using 8 devices with data parallelism
2024-07-08:17:26:31,137 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/165 [00:00<?, ?it/s] 11%|█         | 18/165 [00:00<00:00, 175.21it/s] 22%|██▏       | 37/165 [00:00<00:00, 177.30it/s] 33%|███▎      | 55/165 [00:00<00:00, 156.59it/s] 43%|████▎     | 71/165 [00:00<00:00, 139.89it/s]2024-07-08:17:26:31,743 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
 52%|█████▏    | 86/165 [00:00<00:00, 130.05it/s]  0%|          | 0/165 [00:00<?, ?it/s] 61%|██████    | 100/165 [00:00<00:00, 126.10it/s]  7%|▋         | 12/165 [00:00<00:01, 116.36it/s] 15%|█▍        | 24/165 [00:00<00:01, 117.57it/s] 68%|██████▊   | 113/165 [00:00<00:00, 123.82it/s] 22%|██▏       | 36/165 [00:00<00:01, 117.98it/s] 76%|███████▋  | 126/165 [00:00<00:00, 122.20it/s] 29%|██▉       | 48/165 [00:00<00:00, 118.18it/s] 84%|████████▍ | 139/165 [00:01<00:00, 121.06it/s] 36%|███▋      | 60/165 [00:00<00:00, 118.33it/s] 92%|█████████▏| 152/165 [00:01<00:00, 120.11it/s] 44%|████▎     | 72/165 [00:00<00:00, 118.44it/s]100%|██████████| 165/165 [00:01<00:00, 119.76it/s]100%|██████████| 165/165 [00:01<00:00, 129.30it/s]
 53%|█████▎    | 87/165 [00:00<00:00, 127.38it/s] 61%|██████    | 100/165 [00:00<00:00, 125.52it/s] 68%|██████▊   | 113/165 [00:00<00:00, 117.86it/s] 76%|███████▌  | 125/165 [00:01<00:00, 112.74it/s] 85%|████████▌ | 141/165 [00:01<00:00, 124.48it/s] 95%|█████████▌| 157/165 [00:01<00:00, 134.15it/s]100%|██████████| 165/165 [00:01<00:00, 125.46it/s]
2024-07-08:17:26:46,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:17:26:46,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:17:26:46,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:17:26:46,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:17:26:46,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:17:26:46,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:17:26:46,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:17:26:46,937 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/165 [00:10<29:08, 10.66s/it]Running generate_until requests:   1%|          | 2/165 [00:19<25:30,  9.39s/it]Running generate_until requests:   2%|▏         | 3/165 [00:38<37:54, 14.04s/it]Running generate_until requests:   2%|▏         | 4/165 [00:49<34:08, 12.72s/it]Running generate_until requests:   3%|▎         | 5/165 [00:56<28:40, 10.76s/it]Running generate_until requests:   4%|▎         | 6/165 [01:08<29:33, 11.15s/it]Running generate_until requests:   4%|▍         | 7/165 [01:14<24:58,  9.48s/it]Running generate_until requests:   5%|▍         | 8/165 [01:22<23:48,  9.10s/it]Running generate_until requests:   5%|▌         | 9/165 [01:27<20:12,  7.77s/it]Running generate_until requests:   6%|▌         | 10/165 [01:35<19:46,  7.65s/it]Running generate_until requests:   7%|▋         | 11/165 [01:44<21:08,  8.23s/it]Running generate_until requests:   7%|▋         | 12/165 [01:54<22:02,  8.64s/it]Running generate_until requests:   8%|▊         | 13/165 [02:05<23:31,  9.29s/it]Running generate_until requests:   8%|▊         | 14/165 [02:18<26:28, 10.52s/it]Running generate_until requests:   9%|▉         | 15/165 [02:30<27:17, 10.92s/it]Running generate_until requests:  10%|▉         | 16/165 [02:38<25:05, 10.11s/it]Running generate_until requests:  10%|█         | 17/165 [02:56<30:42, 12.45s/it]Running generate_until requests:  11%|█         | 18/165 [03:11<32:37, 13.32s/it]Running generate_until requests:  12%|█▏        | 19/165 [03:19<27:57, 11.49s/it]Running generate_until requests:  12%|█▏        | 20/165 [03:25<23:48,  9.85s/it]Running generate_until requests:  13%|█▎        | 21/165 [03:39<27:13, 11.34s/it]Running generate_until requests:  13%|█▎        | 22/165 [03:45<23:16,  9.77s/it]Running generate_until requests:  14%|█▍        | 23/165 [03:50<19:39,  8.30s/it]Running generate_until requests:  15%|█▍        | 24/165 [03:57<18:35,  7.91s/it]Running generate_until requests:  15%|█▌        | 25/165 [04:06<18:50,  8.07s/it]Running generate_until requests:  16%|█▌        | 26/165 [04:13<18:05,  7.81s/it]Running generate_until requests:  16%|█▋        | 27/165 [04:22<18:57,  8.24s/it]Running generate_until requests:  17%|█▋        | 28/165 [04:27<16:29,  7.23s/it]Running generate_until requests:  18%|█▊        | 29/165 [04:37<18:28,  8.15s/it]Running generate_until requests:  18%|█▊        | 30/165 [04:44<17:31,  7.79s/it]Running generate_until requests:  19%|█▉        | 31/165 [04:48<14:37,  6.55s/it]Running generate_until requests:  19%|█▉        | 31/165 [04:55<21:18,  9.54s/it]
