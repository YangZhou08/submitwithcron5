Already on 'yangex3'
error: cannot lock ref 'refs/remotes/origin/yangex3': is at 15730cdb32d85da229204f8cabfeaeac3d605f7f but expected 14b04653fc1899c6aac80c654640402684b87669
From github.com:Infini-AI-Lab/GRIFFIN2
 ! 14b0465..15730cd  yangex3    -> origin/yangex3  (unable to update local ref)
warning: fetch updated the current branch head.
fast-forwarding your working tree from
commit 14b04653fc1899c6aac80c654640402684b87669.
error: Your local changes to the following files would be overwritten by merge:
	xhuggingface.py
Please commit your changes or stash them before you merge.
error: The following untracked working tree files would be overwritten by merge:
	llama12_static_cache_sdpa_with_check33.py
Please move or remove them before you merge.
Aborting
fatal: Cannot fast-forward your working tree.
After making sure that you saved anything precious from
$ git diff 14b04653fc1899c6aac80c654640402684b87669
output, run
$ git reset --hard
to recover.
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
2024-07-31:11:46:36,447 INFO     [main.py:288] Verbosity set to INFO
2024-07-31:11:46:47,149 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-07-31:11:46:47,150 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-31:11:46:47,180 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-31:11:46:47,180 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'check': False, 'contextlength': 1500, 'kernel_size': 10, 'thr': 0.05}
2024-07-31:11:46:47,189 INFO     [xhuggingface.py:172] Using device 'cuda'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:23,  7.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.38s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-31:11:47:49,274 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-31:11:47:49,274 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-31:11:47:49,367 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/396 [00:00<?, ?it/s] 39%|███▉      | 156/396 [00:00<00:00, 1558.49it/s] 82%|████████▏ | 326/396 [00:00<00:00, 1640.65it/s]100%|██████████| 396/396 [00:00<00:00, 1639.65it/s]
2024-07-31:11:47:49,616 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/396 [00:00<?, ?it/s]Running generate_until requests:   0%|          | 1/396 [02:06<13:51:47, 126.35s/it]Running generate_until requests:   1%|          | 2/396 [03:51<12:26:08, 113.63s/it]Running generate_until requests:   1%|          | 3/396 [05:36<11:58:55, 109.76s/it]Running generate_until requests:   1%|          | 4/396 [07:20<11:43:55, 107.74s/it]Running generate_until requests:   1%|▏         | 5/396 [09:05<11:35:43, 106.76s/it]Running generate_until requests:   2%|▏         | 6/396 [10:51<11:30:45, 106.27s/it]Running generate_until requests:   2%|▏         | 7/396 [12:36<11:27:42, 106.07s/it]Running generate_until requests:   2%|▏         | 8/396 [14:22<11:25:08, 105.95s/it]Running generate_until requests:   2%|▏         | 9/396 [16:08<11:22:35, 105.83s/it]Running generate_until requests:   3%|▎         | 10/396 [17:53<11:20:26, 105.77s/it]Running generate_until requests:   3%|▎         | 11/396 [19:39<11:18:45, 105.78s/it]Running generate_until requests:   3%|▎         | 12/396 [21:25<11:16:42, 105.73s/it]Running generate_until requests:   3%|▎         | 13/396 [23:10<11:14:48, 105.71s/it]Running generate_until requests:   4%|▎         | 14/396 [24:59<11:17:54, 106.48s/it]Running generate_until requests:   4%|▍         | 15/396 [26:43<11:12:33, 105.92s/it]