Switched to branch 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-25:03:49:36,427 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:49:36,493 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:49:36,590 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:49:36,633 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:49:36,637 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:49:36,638 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:49:36,644 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:49:36,647 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:49:45,520 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:49:45,520 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:49:45,520 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:49:45,520 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:49:45,520 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:49:45,521 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:49:45,530 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:49:45,531 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:49:45,541 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:49:45,541 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:49:45,542 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:49:45,542 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:49:45,542 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:49:45,542 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:49:45,542 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:49:45,542 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:49:45,542 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:49:45,542 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:49:45,542 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:49:45,542 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:49:45,542 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:49:45,543 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:49:45,542 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:49:45,543 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]