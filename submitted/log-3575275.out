No local changes to save
Your branch is up to date with 'origin/yangex3'.
Updating 9ec93ff..53354fc
Fast-forward
 debugging1.sh                            |   12 +-
 getcompilego.py                          |   77 +-
 llama12_static_cache_sdpa_with_check.py  |   21 +-
 llama12_static_cache_sdpa_with_check3.py | 1602 ++++++++++++++++++++++++++++++
 xhuggingface.py                          |    3 +-
 5 files changed, 1690 insertions(+), 25 deletions(-)
 create mode 100644 llama12_static_cache_sdpa_with_check3.py
Already up to date.
/fsx-storygen/beidic/anaconda3/envs/griffin/bin/python
/fsx-storygen/beidic/anaconda3/envs/griffin/bin/python
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /data/home/beidic/.cache/huggingface/token
Login successful
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=True,check=False,contextlength=1500,kernel_size=16,thr=0.05', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.009619379043579101
executinginputidsfull shape torch.Size([1, 16])
full time taken for a single forward pass 0.013197541236877441
acceptance length on average 12.5
time taken for a single forward pass 1.0430495738983154
per token generation time 0.013203159163269815
input_ids shape torch.Size([1, 781])
warming up
time taken for a single forward pass 0.009624385833740234
executinginputidsfull shape torch.Size([1, 16])
full time taken for a single forward pass 0.013175249099731445
acceptance length on average 13.9
time taken for a single forward pass 0.8225140571594238
per token generation time 0.011807364753529996
input_ids shape torch.Size([1, 781])
warming up
time taken for a single forward pass 0.009621810913085938
executinginputidsfull shape torch.Size([1, 16])
full time taken for a single forward pass 0.013169670104980468
acceptance length on average 13.5
time taken for a single forward pass 1.0242974758148193
per token generation time 0.012193506780052989
input_ids shape torch.Size([1, 781])
warming up
time taken for a single forward pass 0.009628391265869141
executinginputidsfull shape torch.Size([1, 16])
full time taken for a single forward pass 0.013171958923339843
acceptance length on average 12.954545454545455
time taken for a single forward pass 1.106050968170166
per token generation time 0.012645291376717483
input_ids shape torch.Size([1, 780])
