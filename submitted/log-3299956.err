Already on 'addinggriffin'
From github.com:YangZhou08/CommonSenseReasoning
   c553cb6..1644937  addinggriffin -> origin/addinggriffin
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:50<00:50, 50.47s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.66s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.64s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:53<00:53, 53.65s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.66s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:53<00:53, 53.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 28.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 31.37s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 27.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 27.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 27.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 27.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 28.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.15s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 28.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 28.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.26s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:43<22:37, 43.80s/it]  3%|▎         | 1/32 [00:43<22:39, 43.87s/it]  3%|▎         | 1/32 [00:44<22:54, 44.34s/it]  3%|▎         | 1/32 [00:44<23:08, 44.78s/it]  3%|▎         | 1/32 [00:45<23:15, 45.03s/it]  3%|▎         | 1/32 [00:44<23:12, 44.93s/it]  3%|▎         | 1/32 [01:02<32:27, 62.82s/it]  3%|▎         | 1/32 [01:06<34:16, 66.34s/it]  6%|▋         | 2/32 [01:25<21:17, 42.58s/it]  6%|▋         | 2/32 [01:25<21:19, 42.64s/it]  6%|▋         | 2/32 [01:26<21:23, 42.79s/it]  6%|▋         | 2/32 [01:28<22:07, 44.25s/it]  6%|▋         | 2/32 [01:28<22:07, 44.24s/it]  6%|▋         | 2/32 [01:30<22:38, 45.27s/it]  6%|▋         | 2/32 [02:02<30:21, 60.73s/it]  6%|▋         | 2/32 [02:06<31:23, 62.80s/it]  9%|▉         | 3/32 [02:07<20:18, 42.00s/it]  9%|▉         | 3/32 [02:07<20:30, 42.42s/it]  9%|▉         | 3/32 [02:07<20:19, 42.05s/it]  9%|▉         | 3/32 [02:09<20:40, 42.79s/it]  9%|▉         | 3/32 [02:09<20:46, 42.98s/it]  9%|▉         | 3/32 [02:10<20:50, 43.13s/it] 12%|█▎        | 4/32 [02:48<19:23, 41.55s/it] 12%|█▎        | 4/32 [02:49<19:33, 41.93s/it] 12%|█▎        | 4/32 [02:50<19:36, 42.03s/it] 12%|█▎        | 4/32 [02:51<19:41, 42.21s/it] 12%|█▎        | 4/32 [02:51<19:56, 42.72s/it] 12%|█▎        | 4/32 [02:54<20:20, 43.60s/it]  9%|▉         | 3/32 [03:01<29:01, 60.06s/it]  9%|▉         | 3/32 [03:09<30:16, 62.65s/it] 16%|█▌        | 5/32 [03:29<18:40, 41.52s/it] 16%|█▌        | 5/32 [03:29<18:40, 41.51s/it] 16%|█▌        | 5/32 [03:30<18:40, 41.50s/it] 16%|█▌        | 5/32 [03:32<18:49, 41.82s/it] 16%|█▌        | 5/32 [03:32<19:00, 42.24s/it] 16%|█▌        | 5/32 [03:35<19:14, 42.75s/it] 12%|█▎        | 4/32 [04:01<27:57, 59.91s/it] 12%|█▎        | 4/32 [04:09<28:53, 61.93s/it] 19%|█▉        | 6/32 [04:10<17:54, 41.31s/it] 19%|█▉        | 6/32 [04:11<18:05, 41.74s/it] 19%|█▉        | 6/32 [04:11<17:55, 41.35s/it] 19%|█▉        | 6/32 [04:13<17:54, 41.32s/it] 19%|█▉        | 6/32 [04:13<18:09, 41.89s/it] 19%|█▉        | 6/32 [04:21<19:02, 43.94s/it] 22%|██▏       | 7/32 [04:52<17:12, 41.30s/it] 22%|██▏       | 7/32 [04:52<17:11, 41.25s/it] 22%|██▏       | 7/32 [04:54<17:21, 41.68s/it] 22%|██▏       | 7/32 [04:57<17:34, 42.16s/it] 16%|█▌        | 5/32 [05:00<26:53, 59.74s/it] 22%|██▏       | 7/32 [05:02<17:53, 42.94s/it] 22%|██▏       | 7/32 [05:03<18:48, 45.14s/it] 16%|█▌        | 5/32 [05:09<27:33, 61.24s/it] 25%|██▌       | 8/32 [05:33<16:35, 41.47s/it] 25%|██▌       | 8/32 [05:35<16:36, 41.53s/it] 25%|██▌       | 8/32 [05:36<16:35, 41.50s/it] 25%|██▌       | 8/32 [05:38<16:47, 42.00s/it] 25%|██▌       | 8/32 [05:45<17:09, 42.88s/it] 25%|██▌       | 8/32 [05:55<18:56, 47.34s/it] 19%|█▉        | 6/32 [06:01<26:06, 60.24s/it] 19%|█▉        | 6/32 [06:09<26:20, 60.77s/it] 28%|██▊       | 9/32 [06:15<15:49, 41.30s/it] 28%|██▊       | 9/32 [06:16<16:00, 41.77s/it] 28%|██▊       | 9/32 [06:17<15:53, 41.46s/it] 28%|██▊       | 9/32 [06:21<16:13, 42.32s/it] 28%|██▊       | 9/32 [06:26<16:15, 42.40s/it] 28%|██▊       | 9/32 [06:44<18:21, 47.89s/it] 31%|███▏      | 10/32 [06:56<15:03, 41.07s/it] 31%|███▏      | 10/32 [06:56<15:10, 41.38s/it] 31%|███▏      | 10/32 [06:58<15:11, 41.45s/it] 22%|██▏       | 7/32 [07:01<24:59, 59.97s/it] 31%|███▏      | 10/32 [07:03<15:29, 42.25s/it] 31%|███▏      | 10/32 [07:07<15:20, 41.85s/it] 22%|██▏       | 7/32 [07:12<25:32, 61.32s/it] 31%|███▏      | 10/32 [07:34<17:41, 48.24s/it] 34%|███▍      | 11/32 [07:37<14:24, 41.19s/it] 34%|███▍      | 11/32 [07:37<14:22, 41.09s/it] 34%|███▍      | 11/32 [07:40<14:33, 41.57s/it] 34%|███▍      | 11/32 [07:44<14:37, 41.81s/it] 34%|███▍      | 11/32 [07:48<14:34, 41.63s/it] 25%|██▌       | 8/32 [08:00<23:56, 59.85s/it] 25%|██▌       | 8/32 [08:14<24:40, 61.69s/it] 38%|███▊      | 12/32 [08:18<13:40, 41.00s/it] 38%|███▊      | 12/32 [08:18<13:43, 41.17s/it] 34%|███▍      | 11/32 [08:23<16:59, 48.54s/it] 38%|███▊      | 12/32 [08:25<13:51, 41.56s/it] 38%|███▊      | 12/32 [08:28<14:26, 43.33s/it] 38%|███▊      | 12/32 [08:29<13:48, 41.45s/it] 41%|████      | 13/32 [08:59<13:00, 41.06s/it] 41%|████      | 13/32 [08:59<12:59, 41.03s/it] 28%|██▊       | 9/32 [09:03<23:17, 60.77s/it] 41%|████      | 13/32 [09:06<13:05, 41.34s/it] 41%|████      | 13/32 [09:10<13:36, 43.00s/it] 41%|████      | 13/32 [09:11<13:08, 41.50s/it] 38%|███▊      | 12/32 [09:12<16:13, 48.67s/it] 28%|██▊       | 9/32 [09:15<23:32, 61.40s/it] 44%|████▍     | 14/32 [09:40<12:16, 40.90s/it] 44%|████▍     | 14/32 [09:41<12:26, 41.48s/it] 44%|████▍     | 14/32 [09:47<12:23, 41.30s/it] 44%|████▍     | 14/32 [09:52<12:23, 41.32s/it] 44%|████▍     | 14/32 [09:52<12:47, 42.65s/it] 41%|████      | 13/32 [10:01<15:28, 48.84s/it] 31%|███▏      | 10/32 [10:04<22:20, 60.93s/it] 31%|███▏      | 10/32 [10:17<22:36, 61.67s/it] 47%|████▋     | 15/32 [10:20<11:31, 40.69s/it] 47%|████▋     | 15/32 [10:22<11:41, 41.28s/it] 47%|████▋     | 15/32 [10:28<11:39, 41.12s/it] 47%|████▋     | 15/32 [10:33<11:43, 41.38s/it] 47%|████▋     | 15/32 [10:33<11:56, 42.18s/it] 44%|████▍     | 14/32 [10:52<14:48, 49.37s/it] 50%|█████     | 16/32 [11:01<10:51, 40.74s/it] 34%|███▍      | 11/32 [11:04<21:09, 60.43s/it] 50%|█████     | 16/32 [11:06<11:11, 41.97s/it] 50%|█████     | 16/32 [11:09<10:58, 41.13s/it] 50%|█████     | 16/32 [11:14<11:01, 41.33s/it] 50%|█████     | 16/32 [11:15<11:13, 42.09s/it] 34%|███▍      | 11/32 [11:18<21:26, 61.27s/it] 47%|████▋     | 15/32 [11:41<13:58, 49.33s/it] 53%|█████▎    | 17/32 [11:43<10:16, 41.12s/it] 53%|█████▎    | 17/32 [11:48<10:28, 41.88s/it] 53%|█████▎    | 17/32 [11:54<10:32, 42.20s/it] 53%|█████▎    | 17/32 [11:57<10:26, 41.74s/it] 53%|█████▎    | 17/32 [11:57<10:32, 42.17s/it] 38%|███▊      | 12/32 [12:03<20:02, 60.14s/it] 38%|███▊      | 12/32 [12:18<20:20, 61.00s/it] 56%|█████▋    | 18/32 [12:23<09:33, 40.95s/it] 56%|█████▋    | 18/32 [12:29<09:43, 41.69s/it] 50%|█████     | 16/32 [12:30<13:08, 49.30s/it] 56%|█████▋    | 18/32 [12:34<09:43, 41.66s/it] 56%|█████▋    | 18/32 [12:38<09:45, 41.81s/it] 56%|█████▋    | 18/32 [12:41<09:51, 42.26s/it] 41%|████      | 13/32 [13:02<18:58, 59.92s/it] 59%|█████▉    | 19/32 [13:04<08:52, 41.00s/it] 59%|█████▉    | 19/32 [13:10<08:58, 41.46s/it] 59%|█████▉    | 19/32 [13:15<08:58, 41.42s/it] 53%|█████▎    | 17/32 [13:19<12:18, 49.23s/it] 59%|█████▉    | 19/32 [13:19<09:02, 41.72s/it] 59%|█████▉    | 19/32 [13:22<09:05, 41.94s/it] 41%|████      | 13/32 [13:22<19:36, 61.94s/it] 62%|██████▎   | 20/32 [13:47<08:17, 41.46s/it] 62%|██████▎   | 20/32 [13:51<08:16, 41.34s/it] 62%|██████▎   | 20/32 [13:56<08:15, 41.25s/it] 62%|██████▎   | 20/32 [14:01<08:18, 41.56s/it] 44%|████▍     | 14/32 [14:02<17:55, 59.75s/it] 62%|██████▎   | 20/32 [14:03<08:20, 41.73s/it] 56%|█████▋    | 18/32 [14:08<11:28, 49.21s/it] 44%|████▍     | 14/32 [14:26<18:47, 62.61s/it] 66%|██████▌   | 21/32 [14:28<07:34, 41.29s/it] 66%|██████▌   | 21/32 [14:32<07:34, 41.31s/it] 66%|██████▌   | 21/32 [14:38<07:36, 41.49s/it] 66%|██████▌   | 21/32 [14:42<07:35, 41.38s/it] 66%|██████▌   | 21/32 [14:44<07:36, 41.50s/it] 59%|█████▉    | 19/32 [14:57<10:39, 49.20s/it] 47%|████▋     | 15/32 [15:01<16:52, 59.56s/it] 69%|██████▉   | 22/32 [15:09<06:51, 41.16s/it] 69%|██████▉   | 22/32 [15:13<06:52, 41.24s/it] 69%|██████▉   | 22/32 [15:19<06:53, 41.38s/it] 69%|██████▉   | 22/32 [15:23<06:52, 41.26s/it] 69%|██████▉   | 22/32 [15:25<06:53, 41.39s/it] 47%|████▋     | 15/32 [15:27<17:33, 62.00s/it] 62%|██████▎   | 20/32 [15:46<09:49, 49.11s/it] 72%|███████▏  | 23/32 [15:49<06:08, 40.97s/it] 72%|███████▏  | 23/32 [15:54<06:10, 41.21s/it] 72%|███████▏  | 23/32 [16:00<06:11, 41.30s/it] 50%|█████     | 16/32 [16:00<15:52, 59.55s/it] 72%|███████▏  | 23/32 [16:04<06:11, 41.24s/it] 72%|███████▏  | 23/32 [16:06<06:12, 41.33s/it] 50%|█████     | 16/32 [16:27<16:25, 61.57s/it] 78%|███████▊  | 25/32 [16:30<03:40, 31.43s/it] 75%|███████▌  | 24/32 [16:35<05:28, 41.11s/it] 66%|██████▌   | 21/32 [16:35<09:00, 49.09s/it] 75%|███████▌  | 24/32 [16:42<05:31, 41.41s/it] 75%|███████▌  | 24/32 [16:46<05:31, 41.49s/it] 75%|███████▌  | 24/32 [16:47<05:29, 41.21s/it] 53%|█████▎    | 17/32 [17:00<14:52, 59.47s/it] 81%|████████▏ | 26/32 [17:12<03:24, 34.07s/it] 78%|███████▊  | 25/32 [17:16<04:47, 41.03s/it] 78%|███████▊  | 25/32 [17:23<04:48, 41.25s/it] 69%|██████▉   | 22/32 [17:27<08:19, 50.00s/it] 78%|███████▊  | 25/32 [17:27<04:50, 41.44s/it] 78%|███████▊  | 25/32 [17:28<04:47, 41.09s/it] 53%|█████▎    | 17/32 [17:29<15:25, 61.70s/it] 84%|████████▍ | 27/32 [17:52<02:58, 35.79s/it] 56%|█████▋    | 18/32 [17:59<13:52, 59.46s/it] 81%|████████▏ | 26/32 [18:00<04:11, 42.00s/it] 81%|████████▏ | 26/32 [18:03<04:06, 41.10s/it] 81%|████████▏ | 26/32 [18:09<04:06, 41.07s/it] 81%|████████▏ | 26/32 [18:09<04:08, 41.43s/it] 72%|███████▏  | 23/32 [18:16<07:27, 49.67s/it] 56%|█████▋    | 18/32 [18:30<14:20, 61.47s/it] 88%|████████▊ | 28/32 [18:34<02:29, 37.26s/it] 84%|████████▍ | 27/32 [18:43<03:30, 42.14s/it] 84%|████████▍ | 27/32 [18:48<03:29, 42.00s/it] 84%|████████▍ | 27/32 [18:49<03:26, 41.24s/it] 84%|████████▍ | 27/32 [18:50<03:25, 41.10s/it] 59%|█████▉    | 19/32 [18:58<12:52, 59.40s/it] 75%|███████▌  | 24/32 [19:02<06:27, 48.39s/it] 91%|█████████ | 29/32 [19:15<01:55, 38.36s/it] 88%|████████▊ | 28/32 [19:23<02:46, 41.72s/it] 88%|████████▊ | 28/32 [19:28<02:46, 41.60s/it] 88%|████████▊ | 28/32 [19:31<02:45, 41.27s/it] 88%|████████▊ | 28/32 [19:32<02:45, 41.28s/it] 59%|█████▉    | 19/32 [19:35<13:29, 62.28s/it] 78%|███████▊  | 25/32 [19:43<05:23, 46.23s/it] 94%|█████████▍| 30/32 [19:57<01:18, 39.36s/it] 62%|██████▎   | 20/32 [19:58<11:52, 59.37s/it] 94%|█████████▍| 30/32 [20:06<01:04, 32.19s/it] 91%|█████████ | 29/32 [20:12<02:03, 41.25s/it] 91%|█████████ | 29/32 [20:13<02:07, 42.50s/it] 91%|█████████ | 29/32 [20:13<02:04, 41.34s/it] 81%|████████▏ | 26/32 [20:24<04:27, 44.65s/it] 62%|██████▎   | 20/32 [20:35<12:21, 61.76s/it] 97%|█████████▋| 31/32 [20:39<00:40, 40.13s/it] 97%|█████████▋| 31/32 [20:47<00:34, 34.36s/it] 94%|█████████▍| 30/32 [20:54<01:23, 41.97s/it] 94%|█████████▍| 30/32 [20:54<01:22, 41.50s/it] 94%|█████████▍| 30/32 [20:55<01:22, 41.43s/it] 66%|██████▌   | 21/32 [20:58<10:54, 59.50s/it] 84%|████████▍ | 27/32 [21:05<03:38, 43.62s/it]100%|██████████| 32/32 [21:20<00:00, 40.34s/it]100%|██████████| 32/32 [21:20<00:00, 40.00s/it]
100%|██████████| 32/32 [21:27<00:00, 36.05s/it]100%|██████████| 32/32 [21:27<00:00, 40.24s/it]
 97%|█████████▋| 31/32 [21:34<00:41, 41.59s/it] 66%|██████▌   | 21/32 [21:36<11:14, 61.36s/it] 97%|█████████▋| 31/32 [21:36<00:41, 41.36s/it] 97%|█████████▋| 31/32 [21:39<00:42, 42.69s/it] 88%|████████▊ | 28/32 [21:47<02:51, 42.96s/it] 69%|██████▉   | 22/32 [21:59<10:00, 60.03s/it]100%|██████████| 32/32 [22:15<00:00, 41.36s/it]100%|██████████| 32/32 [22:15<00:00, 41.74s/it]
100%|██████████| 32/32 [22:18<00:00, 41.37s/it]100%|██████████| 32/32 [22:18<00:00, 41.82s/it]
 91%|█████████ | 29/32 [22:28<02:07, 42.40s/it]100%|██████████| 32/32 [22:30<00:00, 45.02s/it]100%|██████████| 32/32 [22:30<00:00, 42.20s/it]
 69%|██████▉   | 22/32 [22:37<10:13, 61.39s/it] 72%|███████▏  | 23/32 [22:58<08:58, 59.81s/it] 94%|█████████▍| 30/32 [23:09<01:24, 42.04s/it] 72%|███████▏  | 23/32 [23:37<09:09, 61.05s/it] 97%|█████████▋| 31/32 [23:50<00:41, 41.75s/it] 75%|███████▌  | 24/32 [23:57<07:57, 59.67s/it]100%|██████████| 32/32 [24:32<00:00, 41.86s/it]100%|██████████| 32/32 [24:32<00:00, 46.02s/it]
 75%|███████▌  | 24/32 [24:38<08:06, 60.84s/it] 78%|███████▊  | 25/32 [24:57<06:57, 59.62s/it] 78%|███████▊  | 25/32 [25:38<07:05, 60.75s/it] 81%|████████▏ | 26/32 [25:57<05:57, 59.62s/it] 81%|████████▏ | 26/32 [26:38<06:03, 60.61s/it] 84%|████████▍ | 27/32 [26:56<04:57, 59.53s/it] 84%|████████▍ | 27/32 [27:39<05:02, 60.51s/it] 88%|████████▊ | 28/32 [27:55<03:58, 59.51s/it] 88%|████████▊ | 28/32 [28:41<04:04, 61.02s/it] 91%|█████████ | 29/32 [28:55<02:58, 59.51s/it] 91%|█████████ | 29/32 [29:45<03:05, 61.95s/it] 94%|█████████▍| 30/32 [29:55<01:59, 59.56s/it] 94%|█████████▍| 30/32 [30:46<02:03, 61.51s/it] 97%|█████████▋| 31/32 [30:54<00:59, 59.63s/it][rank6]:[E ProcessGroupNCCL.cpp:523] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
[rank6]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E ProcessGroupNCCL.cpp:1182] [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f8bff62bd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f8c007d36e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f8c007d6c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f8c007d7839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f8c4a4eadf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f8c4b8d9609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f8c4b6a4353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f8bff62bd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f8c007d36e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f8c007d6c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f8c007d7839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f8c4a4eadf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f8c4b8d9609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f8c4b6a4353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f8bff62bd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f8c0052db11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f8c4a4eadf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f8c4b8d9609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f8c4b6a4353 in /lib/x86_64-linux-gnu/libc.so.6)

[rank5]:[E ProcessGroupNCCL.cpp:523] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600366 milliseconds before timing out.
[2024-07-09 15:58:46,661] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2410812 closing signal SIGTERM
[2024-07-09 15:58:46,663] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2410813 closing signal SIGTERM
[2024-07-09 15:58:46,663] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2410814 closing signal SIGTERM
[2024-07-09 15:58:46,664] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2410815 closing signal SIGTERM
[2024-07-09 15:58:46,664] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2410816 closing signal SIGTERM
[2024-07-09 15:58:46,664] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2410817 closing signal SIGTERM
[2024-07-09 15:58:46,664] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2410819 closing signal SIGTERM
[2024-07-09 15:58:48,481] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 6 (pid: 2410818) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
main.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-09_15:58:46
  host      : a100-st-p4de24xlarge-164.fair-a100.hpcaas
  rank      : 6 (local_rank: 6)
  exitcode  : -6 (pid: 2410818)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2410818
========================================================
