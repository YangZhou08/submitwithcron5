/fsx-storygen/beidic/anaconda3/envs/griffin/bin/python
/fsx-storygen/beidic/anaconda3/envs/griffin/bin/python
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /data/home/beidic/.cache/huggingface/token
Login successful
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
is_distributed True
Namespace(tasks='strategyqa', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=10, spr=0.4, thr=0.01, widthtree=1, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
We now use eos_token as pad token
beam width is 8
is_distributed True
Namespace(tasks='strategyqa', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=10, spr=0.4, thr=0.01, widthtree=1, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='strategyqa', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=10, spr=0.4, thr=0.01, widthtree=1, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='strategyqa', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=10, spr=0.4, thr=0.01, widthtree=1, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='strategyqa', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=10, spr=0.4, thr=0.01, widthtree=1, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='strategyqa', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=10, spr=0.4, thr=0.01, widthtree=1, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='strategyqa', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=10, spr=0.4, thr=0.01, widthtree=1, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='strategyqa', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=10, spr=0.4, thr=0.01, widthtree=1, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
We now use eos_token as pad token
We now use eos_token as pad token
We now use eos_token as pad token
We now use eos_token as pad token
We now use eos_token as pad token
We now use eos_token as pad token
We now use eos_token as pad token
beam width is 8
beam width is 8
beam width is 8
beam width is 8
beam width is 8
beam width is 8
beam width is 8
tasks ['strategyqa']tasks ['strategyqa']tasks ['strategyqa']

tasks ['strategyqa']

tasks ['strategyqa']tasks ['strategyqa']

tasks ['strategyqa']tasks ['strategyqa']

beamwidth is 1beamwidth is 1

beamwidth is 1
beamwidth is 1
beamwidth is 1
beamwidth is 1
beamwidth is 1
beamwidth is 1
Answer no expected no
Answer no expected no
Answer no expected no
Skipping the batch
Answer no expected yes
Skipping the batch
Answer no expected no
Answer yes expected no
Skipping the batch
Answer no expected yes
Answer yes expected yes
Answer no expected yes
Answer no expected no
Answer no expected no
Answer no expected no
Answer no expected yes
Answer no expected yes
Answer no expected yes
Answer no expected yes
Answer no expected no
Answer no expected no
Skipping the batch
Answer no expected no
Answer no expected no
Answer no expected no
Answer no expected no
Answer no expected no
Answer no expected yes
Answer no expected no
Answer no expected no
Answer no expected no
Answer no expected yes
Answer no expected no
Answer yes expected no
Skipping the batch
Answer no expected no
Answer no expected no
Skipping the batch
Answer no expected yes
Answer no expected no
Answer no expected no
Answer yes expected yes
Answer no expected no
Answer no expected no
Skipping the batch
Answer no expected no
Answer yes expected yes
Answer yes expected yes
Answer volution occurred in portugal. the estimated death toll for the carnation revolution is between 1000 and 2000. the estimated death toll for the russian revolution is between 20 million and 30 million. the estimated death toll for the french revolution is between 100,000 and 200,000. the estimated death toll for the chinese revolution is between 20 million and 40 million. the estimated death toll for the iranian revolution is between 3000 and 5000. the estimated death toll for the ukrainian revolution is between 4000 and 5000. the estimated death toll for the arab spring is between 100,000 and 300,000. the estimated death toll for the syrian civil war is between 300,000 and 500,000. the estimated death toll for the iraq war is between 300,000 and 500,000. the estimated death toll for the vietnam war is between 3 million and 5 million. th expected no
Answer no expected no
Answer no expected no
Answer yes expected yes
Answer no expected no
Answer no expected no
Answer no expected no
Answer no expected no
index 6 start communication
index 4 start communication
Answer yes expected yes
Answer yes expected yes
index 2 start communication
Answer no expected yes
Answer no expected no
Answer yes expected yes
index 5 start communication
Answer no expected yes
index 1 start communication
Answer yes expected no
Answer no expected no
index 0 start communication
index 3 start communication
index 7 start communication
Here are the statistics for inference
Here are the statistics for inference
Here are the statistics for inference
Here are the statistics for inference
Here are the statistics for inference
Here are the statistics for inference
Here are the statistics for inference
Here are the statistics for inference
+------------+----------------+---------------------------+-----------------------------+---------------+-------------+---------+--------------------------------+------------------+----------------------------------+-----------------------+----------------------+
| Task       |   Num Sentence |   Total Generation Length |   Average Generation Length |   Total Steps |   Num Steps |     AAL |   Total Roll Back Length Error |   Error Instance |   Average Roll Back Length Error |   Effective Tree Size |   Drafting Tree Size |
+============+================+===========================+=============================+===============+=============+=========+================================+==================+==================================+=======================+======================+
| strategyqa |            457 |                     42334 |                     92.6346 |         37421 |        5706 | 6.55818 |                          19639 |             3131 |                          6.27244 |                    10 |                    1 |
+------------+----------------+---------------------------+-----------------------------+---------------+-------------+---------+--------------------------------+------------------+----------------------------------+-----------------------+----------------------+
Namespace(tasks='strategyqa', model='meta-llama/Llama-2-7b-chat-hf', device='cuda:0', limit=None, griffin=True, cats=False, check=True, kernel_size=10, spr=0.4, thr=0.01, widthtree=1, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
+------------+---------+-----------+--------------+
| Task       |   Total |   Correct |   Solve Rate |
+============+=========+===========+==============+
| strategyqa |     457 |       306 |     0.669584 |
+------------+---------+-----------+--------------+
