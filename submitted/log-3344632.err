Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:49<01:38, 49.02s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:50<01:41, 50.58s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:51<01:42, 51.00s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:50<01:40, 50.39s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:50<01:40, 50.35s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:50<01:40, 50.33s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:50<01:40, 50.40s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:51<01:42, 51.13s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:50<00:56, 56.16s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:52<00:57, 57.23s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:51<00:56, 56.97s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:51<00:56, 56.96s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:52<00:57, 57.06s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:51<00:56, 56.98s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:51<00:56, 56.99s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:52<00:57, 57.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 45.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 47.72s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:24<00:00, 45.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:24<00:00, 48.19s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:24<00:00, 45.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:24<00:00, 48.05s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 45.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 47.96s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 45.56s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 47.99s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 45.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 47.99s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 45.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 47.99s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 45.95s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:23<00:00, 47.99s/it]
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for tasksource/bigbench contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tasksource/bigbench
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for tasksource/bigbench contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tasksource/bigbench
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for tasksource/bigbench contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tasksource/bigbench
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for tasksource/bigbench contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tasksource/bigbench
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for tasksource/bigbench contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tasksource/bigbench
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for tasksource/bigbench contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tasksource/bigbench
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for tasksource/bigbench contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tasksource/bigbench
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for tasksource/bigbench contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tasksource/bigbench
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:421: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/58 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:421: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/58 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:421: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/58 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:421: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/58 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:421: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/58 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:421: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/58 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:421: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/58 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:421: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/58 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  2%|▏         | 1/58 [00:03<03:21,  3.53s/it]  2%|▏         | 1/58 [00:03<03:25,  3.60s/it]  2%|▏         | 1/58 [00:04<03:49,  4.03s/it]  2%|▏         | 1/58 [00:05<04:48,  5.06s/it]  2%|▏         | 1/58 [00:05<05:24,  5.70s/it]  2%|▏         | 1/58 [00:06<05:53,  6.20s/it]  3%|▎         | 2/58 [00:08<03:42,  3.97s/it]  3%|▎         | 2/58 [00:08<03:54,  4.18s/it]  3%|▎         | 2/58 [00:08<04:04,  4.36s/it]  3%|▎         | 2/58 [00:09<04:09,  4.45s/it]  3%|▎         | 2/58 [00:09<04:27,  4.77s/it]  3%|▎         | 2/58 [00:09<04:23,  4.70s/it]  2%|▏         | 1/58 [00:09<09:22,  9.86s/it]  5%|▌         | 3/58 [00:10<03:07,  3.40s/it]  5%|▌         | 3/58 [00:12<03:32,  3.86s/it]  5%|▌         | 3/58 [00:12<03:40,  4.01s/it]  5%|▌         | 3/58 [00:13<04:08,  4.51s/it]  5%|▌         | 3/58 [00:14<04:15,  4.64s/it]  7%|▋         | 4/58 [00:14<03:00,  3.35s/it]  5%|▌         | 3/58 [00:14<04:37,  5.05s/it]  7%|▋         | 4/58 [00:15<03:18,  3.67s/it]  7%|▋         | 4/58 [00:16<03:30,  3.90s/it]  7%|▋         | 4/58 [00:16<03:22,  3.75s/it]  3%|▎         | 2/58 [00:16<07:37,  8.16s/it]  7%|▋         | 4/58 [00:17<03:55,  4.36s/it]  9%|▊         | 5/58 [00:18<03:07,  3.53s/it]  7%|▋         | 4/58 [00:19<04:28,  4.96s/it]  9%|▊         | 5/58 [00:20<03:31,  3.99s/it]  5%|▌         | 3/58 [00:20<05:44,  6.27s/it]  9%|▊         | 5/58 [00:21<03:39,  4.14s/it] 10%|█         | 6/58 [00:21<02:46,  3.20s/it]  9%|▊         | 5/58 [00:21<04:18,  4.88s/it] 10%|█         | 6/58 [00:22<02:59,  3.45s/it]  2%|▏         | 1/58 [00:24<23:01, 24.24s/it] 10%|█         | 6/58 [00:25<03:31,  4.07s/it]  7%|▋         | 4/58 [00:25<04:54,  5.46s/it]  9%|▊         | 5/58 [00:25<04:46,  5.41s/it] 10%|█         | 6/58 [00:25<03:56,  4.56s/it] 12%|█▏        | 7/58 [00:25<02:51,  3.36s/it] 12%|█▏        | 7/58 [00:26<02:47,  3.28s/it] 12%|█▏        | 7/58 [00:27<03:38,  4.29s/it]  9%|▊         | 5/58 [00:28<04:02,  4.58s/it] 14%|█▍        | 8/58 [00:28<02:32,  3.06s/it]  3%|▎         | 2/58 [00:28<11:54, 12.76s/it] 16%|█▌        | 9/58 [00:29<01:56,  2.38s/it] 10%|█         | 6/58 [00:29<04:14,  4.90s/it] 10%|█         | 6/58 [00:31<03:30,  4.05s/it] 17%|█▋        | 10/58 [00:31<01:54,  2.39s/it] 12%|█▏        | 7/58 [00:31<04:22,  5.15s/it]  5%|▌         | 3/58 [00:32<07:40,  8.37s/it] 12%|█▏        | 7/58 [00:32<03:40,  4.32s/it] 19%|█▉        | 11/58 [00:34<01:52,  2.40s/it] 12%|█▏        | 7/58 [00:34<03:09,  3.72s/it] 14%|█▍        | 8/58 [00:34<04:07,  4.95s/it]  7%|▋         | 4/58 [00:34<05:24,  6.01s/it] 14%|█▍        | 8/58 [00:35<03:57,  4.74s/it] 14%|█▍        | 8/58 [00:37<02:55,  3.50s/it] 21%|██        | 12/58 [00:38<02:14,  2.93s/it] 14%|█▍        | 8/58 [00:38<03:55,  4.72s/it] 16%|█▌        | 9/58 [00:38<03:46,  4.63s/it]  9%|▊         | 5/58 [00:40<05:10,  5.86s/it]  9%|▊         | 5/58 [00:40<09:38, 10.91s/it] 16%|█▌        | 9/58 [00:41<02:59,  3.66s/it] 16%|█▌        | 9/58 [00:41<03:26,  4.22s/it] 22%|██▏       | 13/58 [00:42<02:27,  3.28s/it] 10%|█         | 6/58 [00:42<04:03,  4.68s/it] 17%|█▋        | 10/58 [00:43<03:49,  4.78s/it] 10%|█         | 6/58 [00:43<07:10,  8.28s/it] 17%|█▋        | 10/58 [00:44<02:46,  3.47s/it] 16%|█▌        | 9/58 [00:44<04:55,  6.04s/it] 24%|██▍       | 14/58 [00:45<02:22,  3.25s/it] 19%|█▉        | 11/58 [00:46<02:22,  3.04s/it] 12%|█▏        | 7/58 [00:46<03:46,  4.43s/it] 17%|█▋        | 10/58 [00:46<03:39,  4.57s/it] 12%|█▏        | 7/58 [00:47<05:49,  6.86s/it] 14%|█▍        | 8/58 [00:49<07:46,  9.33s/it] 21%|██        | 12/58 [00:49<02:21,  3.08s/it] 19%|█▉        | 11/58 [00:49<03:14,  4.14s/it] 14%|█▍        | 8/58 [00:49<04:32,  5.44s/it] 19%|█▉        | 11/58 [00:49<04:11,  5.34s/it] 26%|██▌       | 15/58 [00:50<02:38,  3.69s/it] 16%|█▌        | 9/58 [00:50<02:38,  3.24s/it] 17%|█▋        | 10/58 [00:51<04:57,  6.20s/it] 16%|█▌        | 9/58 [00:52<06:02,  7.40s/it] 21%|██        | 12/58 [00:52<02:45,  3.60s/it] 16%|█▌        | 9/58 [00:52<03:40,  4.49s/it] 28%|██▊       | 16/58 [00:53<02:28,  3.54s/it] 17%|█▋        | 10/58 [00:53<02:34,  3.23s/it] 22%|██▏       | 13/58 [00:54<02:27,  3.29s/it] 17%|█▋        | 10/58 [00:55<04:54,  6.13s/it] 22%|██▏       | 13/58 [00:55<02:58,  3.96s/it] 17%|█▋        | 10/58 [00:55<03:16,  4.10s/it] 29%|██▉       | 17/58 [00:55<02:11,  3.21s/it] 19%|█▉        | 11/58 [00:56<02:23,  3.04s/it] 21%|██        | 12/58 [00:56<04:21,  5.67s/it] 19%|█▉        | 11/58 [00:57<04:54,  6.26s/it] 24%|██▍       | 14/58 [00:57<02:22,  3.25s/it] 24%|██▍       | 14/58 [00:58<02:41,  3.68s/it] 19%|█▉        | 11/58 [00:58<04:05,  5.21s/it] 19%|█▉        | 11/58 [00:58<03:00,  3.85s/it] 31%|███       | 18/58 [00:59<02:10,  3.25s/it] 21%|██        | 12/58 [00:59<02:31,  3.29s/it] 22%|██▏       | 13/58 [01:00<03:50,  5.13s/it] 33%|███▎      | 19/58 [01:01<01:56,  3.00s/it] 22%|██▏       | 13/58 [01:01<02:06,  2.81s/it] 21%|██        | 12/58 [01:01<03:30,  4.58s/it] 21%|██        | 12/58 [01:01<04:14,  5.53s/it] 26%|██▌       | 15/58 [01:02<02:42,  3.78s/it] 26%|██▌       | 15/58 [01:03<02:47,  3.90s/it] 21%|██        | 12/58 [01:03<03:09,  4.12s/it] 22%|██▏       | 13/58 [01:04<02:56,  3.92s/it] 28%|██▊       | 16/58 [01:04<02:17,  3.26s/it] 24%|██▍       | 14/58 [01:04<02:08,  2.92s/it] 22%|██▏       | 13/58 [01:05<03:47,  5.05s/it] 34%|███▍      | 20/58 [01:06<02:14,  3.53s/it] 24%|██▍       | 14/58 [01:06<02:31,  3.45s/it] 28%|██▊       | 16/58 [01:06<02:34,  3.67s/it] 24%|██▍       | 14/58 [01:06<04:02,  5.52s/it] 26%|██▌       | 15/58 [01:07<01:59,  2.77s/it] 22%|██▏       | 13/58 [01:08<03:13,  4.30s/it] 26%|██▌       | 15/58 [01:09<02:14,  3.14s/it] 26%|██▌       | 15/58 [01:09<03:20,  4.65s/it] 24%|██▍       | 14/58 [01:09<03:26,  4.69s/it] 29%|██▉       | 17/58 [01:09<02:24,  3.52s/it] 24%|██▍       | 14/58 [01:10<02:44,  3.73s/it] 28%|██▊       | 16/58 [01:11<02:11,  3.12s/it] 28%|██▊       | 16/58 [01:11<02:02,  2.91s/it] 29%|██▉       | 17/58 [01:11<03:00,  4.40s/it] 31%|███       | 18/58 [01:12<02:17,  3.44s/it] 26%|██▌       | 15/58 [01:12<02:23,  3.33s/it] 28%|██▊       | 16/58 [01:13<03:05,  4.42s/it] 26%|██▌       | 15/58 [01:13<03:11,  4.45s/it] 29%|██▉       | 17/58 [01:14<02:08,  3.13s/it] 31%|███       | 18/58 [01:15<01:39,  2.49s/it] 31%|███       | 18/58 [01:15<02:51,  4.28s/it] 33%|███▎      | 19/58 [01:16<02:19,  3.57s/it] 29%|██▉       | 17/58 [01:17<02:54,  4.26s/it] 33%|███▎      | 19/58 [01:17<01:36,  2.47s/it] 31%|███       | 18/58 [01:17<02:07,  3.18s/it] 28%|██▊       | 16/58 [01:17<02:39,  3.80s/it] 28%|██▊       | 16/58 [01:18<03:14,  4.64s/it] 33%|███▎      | 19/58 [01:19<02:43,  4.20s/it] 36%|███▌      | 21/58 [01:20<01:11,  1.94s/it] 33%|███▎      | 19/58 [01:19<01:54,  2.94s/it] 29%|██▉       | 17/58 [01:20<02:18,  3.38s/it] 31%|███       | 18/58 [01:22<02:17,  3.44s/it] 38%|███▊      | 22/58 [01:23<01:19,  2.22s/it] 34%|███▍      | 20/58 [01:23<02:03,  3.25s/it] 34%|███▍      | 20/58 [01:24<02:49,  4.45s/it] 31%|███       | 18/58 [01:24<03:33,  5.34s/it] 31%|███       | 18/58 [01:25<02:31,  3.79s/it] 40%|███▉      | 23/58 [01:25<01:19,  2.26s/it] 36%|███▌      | 21/58 [01:27<02:28,  4.02s/it] 41%|████▏     | 24/58 [01:28<01:24,  2.49s/it] 33%|███▎      | 19/58 [01:28<02:29,  3.84s/it] 36%|███▌      | 21/58 [01:29<05:46,  9.37s/it] 33%|███▎      | 19/58 [01:30<03:25,  5.28s/it] 33%|███▎      | 19/58 [01:30<02:54,  4.49s/it] 38%|███▊      | 22/58 [01:30<02:14,  3.72s/it] 34%|███▍      | 20/58 [01:30<02:00,  3.18s/it] 36%|███▌      | 21/58 [01:30<02:41,  4.37s/it] 38%|███▊      | 22/58 [01:32<04:30,  7.52s/it] 43%|████▎     | 25/58 [01:32<01:36,  2.92s/it] 40%|███▉      | 23/58 [01:33<02:03,  3.52s/it] 36%|███▌      | 21/58 [01:33<01:59,  3.22s/it] 38%|███▊      | 22/58 [01:34<02:25,  4.05s/it] 34%|███▍      | 20/58 [01:35<02:56,  4.65s/it] 40%|███▉      | 23/58 [01:35<03:39,  6.28s/it] 38%|███▊      | 22/58 [01:36<01:47,  2.98s/it] 45%|████▍     | 26/58 [01:36<01:42,  3.20s/it] 34%|███▍      | 20/58 [01:38<04:01,  6.35s/it] 41%|████▏     | 24/58 [01:39<03:01,  5.35s/it] 47%|████▋     | 27/58 [01:39<01:31,  2.97s/it] 36%|███▌      | 21/58 [01:39<02:44,  4.43s/it] 40%|███▉      | 23/58 [01:39<01:46,  3.03s/it] 41%|████▏     | 24/58 [01:39<02:26,  4.30s/it] 34%|███▍      | 20/58 [01:40<06:00,  9.48s/it] 40%|███▉      | 23/58 [01:40<02:44,  4.71s/it] 43%|████▎     | 25/58 [01:41<02:27,  4.46s/it] 36%|███▌      | 21/58 [01:41<04:23,  7.12s/it] 41%|████▏     | 24/58 [01:42<02:16,  4.01s/it] 48%|████▊     | 28/58 [01:43<01:36,  3.23s/it] 41%|████▏     | 24/58 [01:43<01:52,  3.30s/it] 45%|████▍     | 26/58 [01:44<02:10,  4.07s/it] 43%|████▎     | 25/58 [01:45<01:56,  3.52s/it] 36%|███▌      | 21/58 [01:45<03:55,  6.36s/it] 38%|███▊      | 22/58 [01:45<02:58,  4.97s/it] 43%|████▎     | 25/58 [01:45<02:38,  4.79s/it] 38%|███▊      | 22/58 [01:46<03:49,  6.36s/it] 47%|████▋     | 27/58 [01:47<01:50,  3.57s/it] 43%|████▎     | 25/58 [01:48<02:02,  3.71s/it] 45%|████▍     | 26/58 [01:48<01:49,  3.41s/it] 40%|███▉      | 23/58 [01:49<03:09,  5.42s/it] 45%|████▍     | 26/58 [01:49<02:25,  4.55s/it] 48%|████▊     | 28/58 [01:50<01:43,  3.45s/it] 38%|███▊      | 22/58 [01:50<03:38,  6.06s/it] 45%|████▍     | 26/58 [01:51<01:54,  3.59s/it] 47%|████▋     | 27/58 [01:51<01:44,  3.38s/it] 40%|███▉      | 23/58 [01:51<03:10,  5.43s/it] 41%|████▏     | 24/58 [01:53<02:48,  4.96s/it] 47%|████▋     | 27/58 [01:53<01:40,  3.24s/it] 47%|████▋     | 27/58 [01:54<02:25,  4.69s/it] 43%|████▎     | 25/58 [01:55<02:17,  4.18s/it] 48%|████▊     | 28/58 [01:55<01:46,  3.56s/it] 40%|███▉      | 23/58 [01:55<03:22,  5.79s/it] 41%|████▏     | 24/58 [01:55<02:49,  4.99s/it] 48%|████▊     | 28/58 [01:56<01:36,  3.22s/it] 50%|█████     | 29/58 [01:58<01:33,  3.21s/it] 45%|████▍     | 26/58 [01:58<02:03,  3.86s/it] 41%|████▏     | 24/58 [02:00<03:10,  5.59s/it] 47%|████▋     | 27/58 [02:02<01:59,  3.87s/it] 43%|████▎     | 25/58 [02:03<03:10,  5.78s/it] 50%|█████     | 29/58 [02:04<04:04,  8.44s/it] 48%|████▊     | 28/58 [02:06<01:57,  3.91s/it] 43%|████▎     | 25/58 [02:07<03:13,  5.86s/it] 50%|█████     | 29/58 [02:09<01:46,  3.67s/it] 50%|█████     | 29/58 [02:11<04:18,  8.90s/it] 45%|████▍     | 26/58 [02:12<03:00,  5.65s/it] 50%|█████     | 29/58 [02:18<04:13,  8.75s/it] 47%|████▋     | 27/58 [02:19<03:04,  5.96s/it] 48%|████▊     | 28/58 [02:23<05:58, 11.96s/it] 50%|█████     | 29/58 [02:27<04:36,  9.55s/it] 45%|████▍     | 26/58 [02:40<08:03, 15.11s/it] 47%|████▋     | 27/58 [02:46<06:16, 12.14s/it] 48%|████▊     | 28/58 [02:51<04:59, 10.00s/it] 48%|████▊     | 28/58 [02:53<07:13, 14.46s/it] 50%|█████     | 29/58 [02:55<03:59,  8.25s/it] 50%|█████     | 29/58 [03:00<05:49, 12.05s/it] 52%|█████▏    | 30/58 [03:03<10:06, 21.65s/it] 52%|█████▏    | 30/58 [03:03<04:22,  9.37s/it] 52%|█████▏    | 30/58 [03:02<03:47,  8.12s/it] 52%|█████▏    | 30/58 [03:03<10:09, 21.75s/it] 52%|█████▏    | 30/58 [03:03<10:54, 23.39s/it] 52%|█████▏    | 30/58 [03:03<08:40, 18.57s/it] 52%|█████▏    | 30/58 [03:03<08:05, 17.33s/it] 52%|█████▏    | 30/58 [03:03<09:04, 19.45s/it] 53%|█████▎    | 31/58 [03:05<07:08, 15.88s/it] 53%|█████▎    | 31/58 [03:05<05:47, 12.87s/it] 53%|█████▎    | 31/58 [03:06<06:16, 13.95s/it] 53%|█████▎    | 31/58 [03:06<06:33, 14.59s/it] 53%|█████▎    | 31/58 [03:06<07:51, 17.45s/it] 53%|█████▎    | 31/58 [03:07<07:23, 16.42s/it] 55%|█████▌    | 32/58 [03:08<04:19,  9.97s/it] 55%|█████▌    | 32/58 [03:09<04:38, 10.71s/it] 55%|█████▌    | 32/58 [03:09<02:32,  5.86s/it] 55%|█████▌    | 32/58 [03:09<05:43, 13.20s/it] 53%|█████▎    | 31/58 [03:10<03:58,  8.85s/it] 55%|█████▌    | 32/58 [03:11<05:31, 12.75s/it] 55%|█████▌    | 32/58 [03:11<05:30, 12.71s/it] 55%|█████▌    | 32/58 [03:11<05:03, 11.67s/it] 57%|█████▋    | 33/58 [03:11<03:25,  8.21s/it] 57%|█████▋    | 33/58 [03:13<04:01,  9.64s/it] 57%|█████▋    | 33/58 [03:13<03:30,  8.43s/it] 59%|█████▊    | 34/58 [03:13<02:44,  6.84s/it] 57%|█████▋    | 33/58 [03:14<04:05,  9.80s/it] 59%|█████▊    | 34/58 [03:14<02:35,  6.46s/it] 57%|█████▋    | 33/58 [03:14<02:21,  5.66s/it] 57%|█████▋    | 33/58 [03:15<04:32, 10.89s/it] 59%|█████▊    | 34/58 [03:16<03:00,  7.51s/it] 59%|█████▊    | 34/58 [03:16<03:05,  7.74s/it] 60%|██████    | 35/58 [03:17<02:06,  5.52s/it] 59%|█████▊    | 34/58 [03:17<02:49,  7.08s/it] 60%|██████    | 35/58 [03:17<02:20,  6.12s/it] 59%|█████▊    | 34/58 [03:18<03:25,  8.58s/it] 55%|█████▌    | 32/58 [03:18<03:42,  8.55s/it] 60%|██████    | 35/58 [03:19<02:21,  6.13s/it] 60%|██████    | 35/58 [03:20<02:34,  6.73s/it] 62%|██████▏   | 36/58 [03:20<01:45,  4.81s/it] 60%|██████    | 35/58 [03:20<02:15,  5.90s/it] 59%|█████▊    | 34/58 [03:20<02:19,  5.83s/it] 60%|██████    | 35/58 [03:21<02:35,  6.75s/it] 62%|██████▏   | 36/58 [03:21<01:50,  5.01s/it] 62%|██████▏   | 36/58 [03:21<02:02,  5.55s/it] 62%|██████▏   | 36/58 [03:24<02:05,  5.72s/it] 64%|██████▍   | 37/58 [03:24<01:43,  4.91s/it] 57%|█████▋    | 33/58 [03:25<03:17,  7.91s/it] 62%|██████▏   | 36/58 [03:25<02:10,  5.94s/it] 64%|██████▍   | 37/58 [03:25<01:38,  4.69s/it] 64%|██████▍   | 37/58 [03:25<01:40,  4.79s/it] 64%|██████▍   | 37/58 [03:27<01:43,  4.95s/it] 66%|██████▌   | 38/58 [03:27<01:19,  4.00s/it] 66%|██████▌   | 38/58 [03:27<01:21,  4.07s/it] 64%|██████▍   | 37/58 [03:28<01:46,  5.07s/it] 62%|██████▏   | 36/58 [03:28<02:22,  6.46s/it] 60%|██████    | 35/58 [03:28<02:26,  6.37s/it] 66%|██████▌   | 38/58 [03:28<01:33,  4.67s/it] 67%|██████▋   | 39/58 [03:29<01:03,  3.33s/it] 66%|██████▌   | 38/58 [03:29<01:23,  4.18s/it] 59%|█████▊    | 34/58 [03:30<02:50,  7.09s/it] 64%|██████▍   | 37/58 [03:30<01:50,  5.24s/it] 67%|██████▋   | 39/58 [03:31<01:11,  3.74s/it] 67%|██████▋   | 39/58 [03:32<01:09,  3.65s/it] 66%|██████▌   | 38/58 [03:32<01:34,  4.74s/it] 69%|██████▉   | 40/58 [03:32<00:58,  3.27s/it] 67%|██████▋   | 39/58 [03:32<01:24,  4.46s/it] 69%|██████▉   | 40/58 [03:33<01:00,  3.33s/it] 66%|██████▌   | 38/58 [03:34<01:33,  4.66s/it] 69%|██████▉   | 40/58 [03:34<00:58,  3.27s/it] 69%|██████▉   | 40/58 [03:35<01:09,  3.87s/it] 60%|██████    | 35/58 [03:35<02:29,  6.50s/it] 71%|███████   | 41/58 [03:35<00:55,  3.27s/it] 67%|██████▋   | 39/58 [03:36<01:25,  4.52s/it] 62%|██████▏   | 36/58 [03:36<02:28,  6.75s/it] 67%|██████▋   | 39/58 [03:37<01:19,  4.21s/it] 71%|███████   | 41/58 [03:37<01:00,  3.55s/it] 71%|███████   | 41/58 [03:37<00:58,  3.44s/it] 62%|██████▏   | 36/58 [03:37<01:57,  5.33s/it] 69%|██████▉   | 40/58 [03:39<01:14,  4.12s/it] 72%|███████▏  | 42/58 [03:39<00:55,  3.45s/it] 71%|███████   | 41/58 [03:39<01:06,  3.92s/it] 72%|███████▏  | 42/58 [03:39<00:51,  3.21s/it] 72%|███████▏  | 42/58 [03:39<00:50,  3.14s/it] 69%|██████▉   | 40/58 [03:41<01:14,  4.13s/it] 64%|██████▍   | 37/58 [03:41<01:42,  4.89s/it] 74%|███████▍  | 43/58 [03:42<00:46,  3.13s/it] 74%|███████▍  | 43/58 [03:42<00:44,  2.96s/it] 71%|███████   | 41/58 [03:42<01:04,  3.79s/it] 76%|███████▌  | 44/58 [03:44<00:40,  2.90s/it] 74%|███████▍  | 43/58 [03:44<00:54,  3.60s/it] 72%|███████▏  | 42/58 [03:45<01:10,  4.41s/it] 67%|██████▋   | 39/58 [03:45<01:06,  3.52s/it] 76%|███████▌  | 44/58 [03:46<00:42,  3.02s/it] 76%|███████▌  | 44/58 [03:46<00:48,  3.47s/it] 72%|███████▏  | 42/58 [03:47<01:06,  4.14s/it] 78%|███████▊  | 45/58 [03:47<00:38,  2.97s/it] 74%|███████▍  | 43/58 [03:47<00:57,  3.80s/it] 78%|███████▊  | 45/58 [03:49<00:40,  3.14s/it] 78%|███████▊  | 45/58 [03:49<00:40,  3.11s/it] 64%|██████▍   | 37/58 [03:50<03:04,  8.80s/it] 79%|███████▉  | 46/58 [03:50<00:36,  3.02s/it] 79%|███████▉  | 46/58 [03:51<00:34,  2.90s/it] 69%|██████▉   | 40/58 [03:52<01:16,  4.27s/it] 74%|███████▍  | 43/58 [03:52<01:05,  4.39s/it] 76%|███████▌  | 44/58 [03:53<01:00,  4.31s/it] 81%|████████  | 47/58 [03:53<00:31,  2.88s/it] 71%|███████   | 41/58 [03:54<01:05,  3.86s/it] 83%|████████▎ | 48/58 [03:55<00:27,  2.73s/it] 66%|██████▌   | 38/58 [03:55<02:36,  7.81s/it] 69%|██████▉   | 40/58 [03:55<01:46,  5.90s/it]
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py", line 453, in <module>
    outputs = model.module.generate(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/utils.py", line 1544, in generate
    return self.greedy_search(
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/llama12addingtree.py", line 1640, in greedy_search
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) 
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/llama12addingtree.py", line 1226, in prepare_inputs_for_generation
    past_key_values = GriffinCache.stackcache(active_cache) 
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/cache.py", line 168, in stackcache
    new_key_cache.append(torch.cat(k_layer, dim = 0)) 
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 3 has a total capacity of 79.15 GiB of which 1.69 MiB is free. Including non-PyTorch memory, this process has 79.14 GiB memory in use. Of the allocated memory 69.53 GiB is allocated by PyTorch, and 7.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 81%|████████  | 47/58 [03:55<00:35,  3.23s/it] 78%|███████▊  | 45/58 [03:57<00:54,  4.20s/it] 84%|████████▍ | 49/58 [03:58<00:23,  2.63s/it] 76%|███████▌  | 44/58 [03:58<01:08,  4.87s/it] 83%|████████▎ | 48/58 [03:59<00:32,  3.22s/it] 79%|███████▉  | 46/58 [03:59<00:44,  3.70s/it] 72%|███████▏  | 42/58 [03:59<01:07,  4.19s/it] 67%|██████▋   | 39/58 [04:00<02:13,  7.01s/it] 86%|████████▌ | 50/58 [04:01<00:22,  2.77s/it][2024-07-15 18:06:09,847] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105292 closing signal SIGTERM
[2024-07-15 18:06:09,847] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105293 closing signal SIGTERM
[2024-07-15 18:06:09,848] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105294 closing signal SIGTERM
[2024-07-15 18:06:09,848] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105296 closing signal SIGTERM
[2024-07-15 18:06:09,848] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105297 closing signal SIGTERM
[2024-07-15 18:06:09,848] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105298 closing signal SIGTERM
[2024-07-15 18:06:09,849] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105299 closing signal SIGTERM
[2024-07-15 18:06:11,879] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 3 (pid: 105295) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-15_18:06:09
  host      : a100-st-p4de24xlarge-64.fair-a100.hpcaas
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 105295)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
