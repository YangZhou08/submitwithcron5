Already on 'addinggriffin'
From github.com:YangZhou08/CommonSenseReasoning
   7d29213..e10b598  addinggriffin -> origin/addinggriffin
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:02, 20.77s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:02, 20.96s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:02, 20.88s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:02, 20.97s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:02, 20.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:35<00:33, 16.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:35<00:34, 17.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:35<00:34, 17.45s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:34, 17.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:34, 17.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:35, 17.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:35<00:34, 17.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:35, 17.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:15, 15.25s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:49<00:15, 15.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:49<00:15, 15.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:49<00:15, 15.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:50<00:15, 15.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:49<00:15, 15.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:49<00:15, 15.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:49<00:15, 15.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 10.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 10.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 12.74s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 12.76s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 10.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 12.76s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 10.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 12.77s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 10.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 12.78s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 10.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 10.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 12.73s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 12.77s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 10.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 12.91s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/153 [00:00<?, ?it/s]  0%|          | 0/153 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/153 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/153 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/153 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/153 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/153 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/153 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/153 [00:06<15:28,  6.11s/it]  0%|          | 0/153 [00:06<?, ?it/s]
  0%|          | 0/153 [00:06<?, ?it/s]
  1%|          | 1/153 [00:07<18:45,  7.41s/it]  0%|          | 0/153 [00:07<?, ?it/s]
  1%|          | 1/153 [00:08<20:21,  8.03s/it]  1%|          | 1/153 [00:08<21:55,  8.66s/it]  1%|          | 1/153 [00:09<24:28,  9.66s/it]  1%|▏         | 2/153 [00:12<14:35,  5.80s/it]  1%|▏         | 2/153 [00:12<15:11,  6.03s/it]  1%|▏         | 2/153 [00:13<16:33,  6.58s/it]  1%|▏         | 2/153 [00:14<17:34,  6.98s/it]  2%|▏         | 3/153 [00:15<11:35,  4.64s/it]  1%|▏         | 2/153 [00:15<18:21,  7.30s/it]  2%|▏         | 3/153 [00:15<11:37,  4.65s/it]  3%|▎         | 4/153 [00:18<10:31,  4.24s/it]  2%|▏         | 3/153 [00:19<15:21,  6.14s/it]  2%|▏         | 3/153 [00:20<16:14,  6.50s/it]  2%|▏         | 3/153 [00:20<15:55,  6.37s/it]  3%|▎         | 4/153 [00:21<13:10,  5.31s/it]  3%|▎         | 5/153 [00:23<10:49,  4.39s/it]  3%|▎         | 4/153 [00:23<13:40,  5.51s/it]  2%|▏         | 3/153 [00:26<21:44,  8.69s/it]
  3%|▎         | 5/153 [00:26<12:26,  5.04s/it]  3%|▎         | 4/153 [00:28<17:44,  7.15s/it]  4%|▍         | 6/153 [00:28<11:42,  4.78s/it]  3%|▎         | 5/153 [00:29<13:35,  5.51s/it]  4%|▍         | 6/153 [00:31<12:38,  5.16s/it]  5%|▍         | 7/153 [00:32<10:55,  4.49s/it]  3%|▎         | 5/153 [00:33<15:18,  6.21s/it]  4%|▍         | 6/153 [00:34<13:40,  5.58s/it]  5%|▍         | 7/153 [00:37<12:47,  5.26s/it]  5%|▌         | 8/153 [00:39<12:23,  5.13s/it]  5%|▍         | 7/153 [00:39<12:54,  5.30s/it]  4%|▍         | 6/153 [00:41<17:07,  6.99s/it]  5%|▌         | 8/153 [00:43<13:42,  5.67s/it]  5%|▌         | 8/153 [00:45<13:35,  5.63s/it]
  5%|▍         | 7/153 [00:45<15:43,  6.46s/it]
  5%|▍         | 7/153 [00:47<16:03,  6.60s/it]  6%|▌         | 9/153 [00:49<13:50,  5.77s/it]  5%|▌         | 8/153 [00:54<16:03,  6.64s/it]  7%|▋         | 10/153 [00:55<13:55,  5.84s/it]  6%|▌         | 9/153 [00:58<14:15,  5.94s/it]  7%|▋         | 11/153 [01:00<13:17,  5.62s/it]  7%|▋         | 10/153 [01:03<12:58,  5.45s/it]  8%|▊         | 12/153 [01:04<12:00,  5.11s/it]  7%|▋         | 11/153 [01:08<12:58,  5.48s/it]  8%|▊         | 12/153 [01:12<11:44,  5.00s/it]  8%|▊         | 13/153 [01:17<11:38,  4.99s/it]  8%|▊         | 13/153 [01:23<15:02,  6.45s/it]
  8%|▊         | 13/153 [01:34<29:09, 12.50s/it]  9%|▉         | 14/153 [01:39<23:44, 10.24s/it] 10%|▉         | 15/153 [01:43<19:41,  8.56s/it] 10%|▉         | 15/153 [01:51<17:03,  7.42s/it]
