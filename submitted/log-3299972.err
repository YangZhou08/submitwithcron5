Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:44<00:44, 44.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:44<00:44, 44.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:46<00:46, 46.02s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.54s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 28.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.79s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 28.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.88s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 28.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 31.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 28.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 28.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.87s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 28.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 28.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 31.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 28.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.96s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:42<21:50, 42.27s/it]  3%|▎         | 1/32 [00:41<21:32, 41.69s/it]  3%|▎         | 1/32 [00:42<22:07, 42.82s/it]  3%|▎         | 1/32 [00:42<22:04, 42.73s/it]  3%|▎         | 1/32 [00:44<22:54, 44.33s/it]  3%|▎         | 1/32 [00:49<25:23, 49.13s/it]  3%|▎         | 1/32 [00:53<27:43, 53.67s/it]  3%|▎         | 1/32 [00:56<29:08, 56.41s/it]  6%|▋         | 2/32 [01:21<20:06, 40.21s/it]  6%|▋         | 2/32 [01:22<20:24, 40.81s/it]  6%|▋         | 2/32 [01:20<20:06, 40.22s/it]  6%|▋         | 2/32 [01:22<20:22, 40.76s/it]  6%|▋         | 2/32 [01:22<20:20, 40.70s/it]  6%|▋         | 2/32 [01:37<24:25, 48.84s/it]  6%|▋         | 2/32 [01:50<27:43, 55.44s/it]  6%|▋         | 2/32 [01:55<28:54, 57.82s/it]  9%|▉         | 3/32 [01:59<19:00, 39.32s/it]  9%|▉         | 3/32 [02:00<19:13, 39.76s/it]  9%|▉         | 3/32 [01:59<19:02, 39.40s/it]  9%|▉         | 3/32 [02:00<19:13, 39.76s/it]  9%|▉         | 3/32 [02:01<19:16, 39.87s/it]  9%|▉         | 3/32 [02:24<23:02, 47.66s/it] 12%|█▎        | 4/32 [02:37<18:10, 38.93s/it] 12%|█▎        | 4/32 [02:37<18:05, 38.77s/it] 12%|█▎        | 4/32 [02:39<18:21, 39.32s/it] 12%|█▎        | 4/32 [02:38<18:15, 39.14s/it] 12%|█▎        | 4/32 [02:41<18:38, 39.95s/it]  9%|▉         | 3/32 [02:49<27:37, 57.15s/it]  9%|▉         | 3/32 [02:51<27:39, 57.21s/it] 12%|█▎        | 4/32 [03:13<22:29, 48.21s/it] 16%|█▌        | 5/32 [03:14<17:17, 38.41s/it] 16%|█▌        | 5/32 [03:16<17:33, 39.02s/it] 16%|█▌        | 5/32 [03:18<17:36, 39.13s/it] 16%|█▌        | 5/32 [03:17<17:35, 39.08s/it] 16%|█▌        | 5/32 [03:19<17:44, 39.44s/it] 12%|█▎        | 4/32 [03:46<26:41, 57.19s/it] 12%|█▎        | 4/32 [03:48<26:36, 57.03s/it] 19%|█▉        | 6/32 [03:52<16:36, 38.31s/it] 19%|█▉        | 6/32 [03:56<16:57, 39.14s/it] 19%|█▉        | 6/32 [03:56<16:49, 38.83s/it] 19%|█▉        | 6/32 [03:57<16:58, 39.19s/it] 16%|█▌        | 5/32 [04:00<21:34, 47.94s/it] 19%|█▉        | 6/32 [03:58<16:57, 39.15s/it] 22%|██▏       | 7/32 [04:32<16:06, 38.66s/it] 22%|██▏       | 7/32 [04:34<16:10, 38.80s/it] 22%|██▏       | 7/32 [04:34<16:08, 38.73s/it] 22%|██▏       | 7/32 [04:37<16:24, 39.38s/it] 22%|██▏       | 7/32 [04:36<16:10, 38.82s/it] 16%|█▌        | 5/32 [04:43<25:38, 57.00s/it] 16%|█▌        | 5/32 [04:46<25:54, 57.56s/it] 19%|█▉        | 6/32 [04:48<20:43, 47.84s/it] 25%|██▌       | 8/32 [05:10<15:23, 38.47s/it] 25%|██▌       | 8/32 [05:13<15:26, 38.60s/it] 25%|██▌       | 8/32 [05:17<16:06, 40.26s/it] 25%|██▌       | 8/32 [05:15<15:31, 38.82s/it] 25%|██▌       | 8/32 [05:19<16:11, 40.47s/it] 22%|██▏       | 7/32 [05:39<20:25, 49.02s/it] 19%|█▉        | 6/32 [05:40<24:40, 56.94s/it] 19%|█▉        | 6/32 [05:43<24:46, 57.19s/it] 28%|██▊       | 9/32 [05:49<14:47, 38.60s/it] 28%|██▊       | 9/32 [05:51<14:45, 38.51s/it] 28%|██▊       | 9/32 [05:55<15:09, 39.56s/it] 28%|██▊       | 9/32 [05:58<15:16, 39.86s/it] 28%|██▊       | 9/32 [05:57<15:15, 39.81s/it] 25%|██▌       | 8/32 [06:27<19:26, 48.61s/it] 31%|███▏      | 10/32 [06:27<14:04, 38.37s/it] 31%|███▏      | 10/32 [06:29<14:06, 38.49s/it] 31%|███▏      | 10/32 [06:33<14:17, 38.99s/it] 31%|███▏      | 10/32 [06:36<14:26, 39.36s/it] 22%|██▏       | 7/32 [06:37<23:43, 56.95s/it] 22%|██▏       | 7/32 [06:39<23:43, 56.95s/it] 31%|███▏      | 10/32 [06:39<14:48, 40.36s/it] 34%|███▍      | 11/32 [07:06<13:31, 38.63s/it] 34%|███▍      | 11/32 [07:11<13:32, 38.71s/it] 28%|██▊       | 9/32 [07:14<18:30, 48.30s/it] 34%|███▍      | 11/32 [07:15<13:44, 39.28s/it] 34%|███▍      | 11/32 [07:15<14:15, 40.72s/it] 34%|███▍      | 11/32 [07:24<14:38, 41.83s/it] 25%|██▌       | 8/32 [07:35<23:00, 57.52s/it] 25%|██▌       | 8/32 [07:38<23:01, 57.55s/it] 38%|███▊      | 12/32 [07:45<12:56, 38.80s/it] 38%|███▊      | 12/32 [07:53<13:19, 39.99s/it] 38%|███▊      | 12/32 [07:55<13:07, 39.39s/it] 38%|███▊      | 12/32 [07:55<13:25, 40.26s/it] 31%|███▏      | 10/32 [08:01<17:27, 47.60s/it] 38%|███▊      | 12/32 [08:03<13:43, 41.16s/it] 41%|████      | 13/32 [08:29<12:47, 40.38s/it] 41%|████      | 13/32 [08:32<12:32, 39.60s/it] 41%|████      | 13/32 [08:33<12:31, 39.53s/it] 41%|████      | 13/32 [08:34<12:27, 39.36s/it] 28%|██▊       | 9/32 [08:34<22:09, 57.80s/it] 28%|██▊       | 9/32 [08:35<21:56, 57.23s/it] 34%|███▍      | 11/32 [08:47<16:29, 47.11s/it] 41%|████      | 13/32 [08:45<13:01, 41.15s/it] 44%|████▍     | 14/32 [09:07<11:53, 39.64s/it] 44%|████▍     | 14/32 [09:11<11:47, 39.28s/it] 44%|████▍     | 14/32 [09:11<11:44, 39.14s/it] 44%|████▍     | 14/32 [09:13<11:46, 39.24s/it] 44%|████▍     | 14/32 [09:23<12:04, 40.26s/it] 31%|███▏      | 10/32 [09:30<21:02, 57.37s/it] 31%|███▏      | 10/32 [09:31<20:53, 56.98s/it] 38%|███▊      | 12/32 [09:36<15:54, 47.74s/it] 47%|████▋     | 15/32 [09:46<11:08, 39.33s/it] 47%|████▋     | 15/32 [09:49<10:59, 38.79s/it] 47%|████▋     | 15/32 [09:49<11:02, 38.97s/it] 47%|████▋     | 15/32 [09:53<11:08, 39.34s/it] 47%|████▋     | 15/32 [10:01<11:16, 39.78s/it] 41%|████      | 13/32 [10:22<14:57, 47.24s/it] 50%|█████     | 16/32 [10:25<10:28, 39.30s/it] 50%|█████     | 16/32 [10:27<10:17, 38.57s/it] 50%|█████     | 16/32 [10:28<10:23, 38.97s/it] 34%|███▍      | 11/32 [10:29<20:10, 57.63s/it] 34%|███▍      | 11/32 [10:31<20:17, 57.95s/it] 50%|█████     | 16/32 [10:35<10:43, 40.24s/it] 50%|█████     | 16/32 [10:40<10:30, 39.41s/it] 53%|█████▎    | 17/32 [11:04<09:48, 39.22s/it] 53%|█████▎    | 17/32 [11:06<09:38, 38.58s/it] 53%|█████▎    | 17/32 [11:06<09:41, 38.80s/it] 44%|████▍     | 14/32 [11:08<14:05, 46.99s/it] 53%|█████▎    | 17/32 [11:14<09:55, 39.73s/it] 53%|█████▎    | 17/32 [11:19<09:48, 39.23s/it] 38%|███▊      | 12/32 [11:25<19:05, 57.29s/it] 38%|███▊      | 12/32 [11:30<19:23, 58.17s/it] 56%|█████▋    | 18/32 [11:43<09:08, 39.20s/it] 56%|█████▋    | 18/32 [11:45<09:03, 38.82s/it] 56%|█████▋    | 18/32 [11:46<09:07, 39.12s/it] 56%|█████▋    | 18/32 [11:53<09:14, 39.63s/it] 47%|████▋     | 15/32 [11:55<13:15, 46.80s/it] 56%|█████▋    | 18/32 [12:03<09:29, 40.70s/it] 41%|████      | 13/32 [12:22<18:03, 57.04s/it] 59%|█████▉    | 19/32 [12:21<08:25, 38.89s/it] 59%|█████▉    | 19/32 [12:23<08:21, 38.59s/it] 59%|█████▉    | 19/32 [12:25<08:25, 38.90s/it] 41%|████      | 13/32 [12:27<18:16, 57.69s/it] 59%|█████▉    | 19/32 [12:34<08:41, 40.10s/it] 50%|█████     | 16/32 [12:42<12:32, 47.03s/it] 59%|█████▉    | 19/32 [12:42<08:42, 40.17s/it] 62%|██████▎   | 20/32 [12:59<07:44, 38.70s/it] 62%|██████▎   | 20/32 [13:01<07:41, 38.42s/it] 62%|██████▎   | 20/32 [13:03<07:43, 38.65s/it] 62%|██████▎   | 20/32 [13:13<07:55, 39.61s/it] 44%|████▍     | 14/32 [13:22<17:23, 57.98s/it] 44%|████▍     | 14/32 [13:23<17:12, 57.38s/it] 62%|██████▎   | 20/32 [13:20<07:55, 39.62s/it] 53%|█████▎    | 17/32 [13:31<11:54, 47.60s/it] 66%|██████▌   | 21/32 [13:40<07:02, 38.45s/it] 66%|██████▌   | 21/32 [13:39<07:07, 38.89s/it] 66%|██████▌   | 21/32 [13:41<07:04, 38.56s/it] 66%|██████▌   | 21/32 [13:51<07:11, 39.22s/it] 66%|██████▌   | 21/32 [13:58<07:11, 39.21s/it] 69%|██████▉   | 22/32 [14:17<06:26, 38.60s/it] 56%|█████▋    | 18/32 [14:19<11:06, 47.62s/it] 47%|████▋     | 15/32 [14:18<16:18, 57.58s/it] 69%|██████▉   | 22/32 [14:19<06:26, 38.65s/it] 69%|██████▉   | 22/32 [14:19<06:25, 38.54s/it] 47%|████▋     | 15/32 [14:22<16:20, 57.67s/it] 69%|██████▉   | 22/32 [14:29<06:29, 38.91s/it] 69%|██████▉   | 22/32 [14:42<06:44, 40.44s/it] 72%|███████▏  | 23/32 [14:54<05:44, 38.31s/it] 72%|███████▏  | 23/32 [14:57<05:46, 38.55s/it] 72%|███████▏  | 23/32 [14:58<05:46, 38.47s/it] 72%|███████▏  | 23/32 [15:08<05:48, 38.77s/it] 59%|█████▉    | 19/32 [15:09<10:29, 48.39s/it] 50%|█████     | 16/32 [15:17<15:26, 57.89s/it] 50%|█████     | 16/32 [15:18<15:18, 57.42s/it] 72%|███████▏  | 23/32 [15:20<05:59, 39.91s/it] 78%|███████▊  | 25/32 [15:32<03:25, 29.35s/it] 75%|███████▌  | 24/32 [15:36<05:08, 38.54s/it] 75%|███████▌  | 24/32 [15:39<05:13, 39.19s/it] 75%|███████▌  | 24/32 [15:46<05:08, 38.61s/it] 62%|██████▎   | 20/32 [15:55<09:32, 47.74s/it] 75%|███████▌  | 24/32 [16:00<05:18, 39.80s/it] 81%|████████▏ | 26/32 [16:11<03:10, 31.80s/it] 53%|█████▎    | 17/32 [16:17<14:25, 57.72s/it] 78%|███████▊  | 25/32 [16:17<04:33, 39.06s/it] 78%|███████▊  | 25/32 [16:19<04:38, 39.85s/it] 53%|█████▎    | 17/32 [16:21<14:54, 59.62s/it] 78%|███████▊  | 25/32 [16:24<04:30, 38.62s/it] 78%|███████▊  | 25/32 [16:38<04:35, 39.30s/it] 66%|██████▌   | 21/32 [16:42<08:40, 47.34s/it] 84%|████████▍ | 27/32 [16:49<02:46, 33.35s/it] 81%|████████▏ | 26/32 [16:56<03:52, 38.83s/it] 81%|████████▏ | 26/32 [17:01<04:03, 40.54s/it] 81%|████████▏ | 26/32 [17:03<03:51, 38.64s/it] 56%|█████▋    | 18/32 [17:17<13:42, 58.77s/it] 56%|█████▋    | 18/32 [17:19<13:45, 58.98s/it] 81%|████████▏ | 26/32 [17:16<03:54, 39.03s/it] 88%|████████▊ | 28/32 [17:29<02:20, 35.00s/it] 69%|██████▉   | 22/32 [17:31<07:58, 47.86s/it] 84%|████████▍ | 27/32 [17:34<03:13, 38.80s/it] 84%|████████▍ | 27/32 [17:39<03:19, 39.96s/it] 84%|████████▍ | 27/32 [17:41<03:12, 38.49s/it] 84%|████████▍ | 27/32 [17:55<03:14, 38.88s/it] 91%|█████████ | 29/32 [18:09<01:49, 36.56s/it] 88%|████████▊ | 28/32 [18:13<02:34, 38.71s/it] 59%|█████▉    | 19/32 [18:14<12:35, 58.11s/it] 72%|███████▏  | 23/32 [18:17<07:05, 47.32s/it] 88%|████████▊ | 28/32 [18:17<02:37, 39.35s/it] 59%|█████▉    | 19/32 [18:19<12:51, 59.34s/it] 88%|████████▊ | 28/32 [18:20<02:34, 38.52s/it] 88%|████████▊ | 28/32 [18:35<02:37, 39.26s/it] 94%|█████████▍| 30/32 [18:49<01:15, 37.61s/it] 91%|█████████ | 29/32 [18:53<01:57, 39.02s/it] 94%|█████████▍| 30/32 [18:55<00:59, 29.99s/it] 91%|█████████ | 29/32 [19:00<01:56, 38.89s/it] 75%|███████▌  | 24/32 [19:04<06:19, 47.44s/it] 91%|█████████ | 29/32 [19:14<01:57, 39.09s/it] 62%|██████▎   | 20/32 [19:17<11:49, 59.10s/it] 62%|██████▎   | 20/32 [19:18<11:57, 59.76s/it] 97%|█████████▋| 31/32 [19:27<00:37, 37.70s/it] 94%|█████████▍| 30/32 [19:31<01:17, 38.95s/it] 97%|█████████▋| 31/32 [19:35<00:32, 32.29s/it] 94%|█████████▍| 30/32 [19:39<01:17, 38.99s/it] 78%|███████▊  | 25/32 [19:54<05:35, 47.95s/it] 94%|█████████▍| 30/32 [19:53<01:18, 39.05s/it]100%|██████████| 32/32 [20:09<00:00, 38.80s/it]100%|██████████| 32/32 [20:09<00:00, 37.79s/it]
 97%|█████████▋| 31/32 [20:10<00:38, 38.83s/it]100%|██████████| 32/32 [20:13<00:00, 33.75s/it]100%|██████████| 32/32 [20:13<00:00, 37.91s/it]
 66%|██████▌   | 21/32 [20:16<10:48, 58.92s/it] 66%|██████▌   | 21/32 [20:16<10:53, 59.39s/it] 97%|█████████▋| 31/32 [20:17<00:38, 38.76s/it] 97%|█████████▋| 31/32 [20:31<00:38, 38.89s/it] 81%|████████▏ | 26/32 [20:42<04:48, 48.15s/it]100%|██████████| 32/32 [20:48<00:00, 38.69s/it]100%|██████████| 32/32 [20:48<00:00, 39.03s/it]
100%|██████████| 32/32 [20:55<00:00, 38.51s/it]100%|██████████| 32/32 [20:55<00:00, 39.23s/it]
 69%|██████▉   | 22/32 [21:13<09:42, 58.21s/it]100%|██████████| 32/32 [21:10<00:00, 38.77s/it]100%|██████████| 32/32 [21:10<00:00, 39.70s/it]
 69%|██████▉   | 22/32 [21:13<09:46, 58.68s/it] 84%|████████▍ | 27/32 [21:29<03:59, 47.86s/it] 72%|███████▏  | 23/32 [22:09<08:39, 57.75s/it] 72%|███████▏  | 23/32 [22:10<08:42, 58.07s/it] 88%|████████▊ | 28/32 [22:15<03:09, 47.28s/it] 91%|█████████ | 29/32 [23:01<02:20, 46.68s/it] 75%|███████▌  | 24/32 [23:07<07:41, 57.63s/it] 75%|███████▌  | 24/32 [23:06<07:41, 57.66s/it] 94%|█████████▍| 30/32 [23:47<01:32, 46.46s/it] 78%|███████▊  | 25/32 [24:05<06:44, 57.75s/it] 78%|███████▊  | 25/32 [24:05<06:45, 57.87s/it] 97%|█████████▋| 31/32 [24:32<00:46, 46.26s/it] 81%|████████▏ | 26/32 [25:01<05:44, 57.38s/it] 81%|████████▏ | 26/32 [25:01<05:44, 57.48s/it]100%|██████████| 32/32 [25:18<00:00, 45.98s/it]100%|██████████| 32/32 [25:18<00:00, 47.44s/it]
 84%|████████▍ | 27/32 [25:58<04:46, 57.22s/it] 84%|████████▍ | 27/32 [25:58<04:45, 57.20s/it] 88%|████████▊ | 28/32 [26:54<03:47, 56.99s/it] 88%|████████▊ | 28/32 [26:54<03:48, 57.00s/it] 91%|█████████ | 29/32 [27:51<02:50, 56.96s/it] 91%|█████████ | 29/32 [27:53<02:52, 57.41s/it] 94%|█████████▍| 30/32 [28:48<01:53, 56.91s/it] 94%|█████████▍| 30/32 [28:49<01:54, 57.17s/it] 97%|█████████▋| 31/32 [29:45<00:56, 56.86s/it] 97%|█████████▋| 31/32 [29:46<00:56, 56.98s/it]