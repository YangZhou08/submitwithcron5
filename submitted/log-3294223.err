Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.63s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:50<00:50, 50.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.83s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.62s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.57s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 28.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.59s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 28.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.88s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 28.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 28.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.79s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 28.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.84s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 28.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 29.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.52s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:41<21:31, 41.65s/it]  3%|▎         | 1/32 [00:41<21:31, 41.67s/it]  3%|▎         | 1/32 [00:41<21:27, 41.53s/it]  3%|▎         | 1/32 [00:42<21:56, 42.47s/it]  3%|▎         | 1/32 [00:42<22:04, 42.72s/it]  3%|▎         | 1/32 [00:43<22:15, 43.08s/it]  3%|▎         | 1/32 [00:56<29:09, 56.42s/it]  3%|▎         | 1/32 [00:59<30:30, 59.05s/it]  6%|▋         | 2/32 [01:20<20:03, 40.10s/it]  6%|▋         | 2/32 [01:20<20:08, 40.27s/it]  6%|▋         | 2/32 [01:21<20:13, 40.44s/it]  6%|▋         | 2/32 [01:21<20:17, 40.58s/it]  6%|▋         | 2/32 [01:26<21:37, 43.26s/it]  6%|▋         | 2/32 [01:34<23:57, 47.91s/it]  6%|▋         | 2/32 [01:56<28:59, 57.99s/it]  6%|▋         | 2/32 [01:58<30:01, 60.04s/it]  9%|▉         | 3/32 [01:59<19:09, 39.64s/it]  9%|▉         | 3/32 [02:01<19:25, 40.19s/it]  9%|▉         | 3/32 [02:01<19:22, 40.09s/it]  9%|▉         | 3/32 [02:01<19:28, 40.29s/it]  9%|▉         | 3/32 [02:05<19:54, 41.19s/it]  9%|▉         | 3/32 [02:21<23:01, 47.65s/it] 12%|█▎        | 4/32 [02:39<18:26, 39.53s/it] 12%|█▎        | 4/32 [02:40<18:33, 39.78s/it] 12%|█▎        | 4/32 [02:40<18:32, 39.74s/it] 12%|█▎        | 4/32 [02:41<18:44, 40.15s/it] 12%|█▎        | 4/32 [02:44<18:50, 40.36s/it]  9%|▉         | 3/32 [02:55<28:13, 58.40s/it]  9%|▉         | 3/32 [02:56<28:25, 58.79s/it] 12%|█▎        | 4/32 [03:13<22:55, 49.13s/it] 16%|█▌        | 5/32 [03:18<17:48, 39.58s/it] 16%|█▌        | 5/32 [03:19<17:45, 39.45s/it] 16%|█▌        | 5/32 [03:20<17:54, 39.80s/it] 16%|█▌        | 5/32 [03:21<18:03, 40.14s/it] 16%|█▌        | 5/32 [03:23<17:59, 39.99s/it] 12%|█▎        | 4/32 [03:52<26:58, 57.81s/it] 12%|█▎        | 4/32 [03:55<27:30, 58.94s/it] 19%|█▉        | 6/32 [03:58<17:03, 39.36s/it] 19%|█▉        | 6/32 [03:58<17:08, 39.55s/it] 16%|█▌        | 5/32 [04:00<21:47, 48.43s/it] 19%|█▉        | 6/32 [04:00<17:20, 40.03s/it] 19%|█▉        | 6/32 [04:01<17:19, 39.98s/it] 19%|█▉        | 6/32 [04:01<17:06, 39.47s/it] 22%|██▏       | 7/32 [04:37<16:24, 39.37s/it] 22%|██▏       | 7/32 [04:37<16:27, 39.50s/it] 22%|██▏       | 7/32 [04:40<16:36, 39.87s/it] 22%|██▏       | 7/32 [04:42<16:55, 40.62s/it] 22%|██▏       | 7/32 [04:43<16:47, 40.30s/it] 16%|█▌        | 5/32 [04:49<25:56, 57.63s/it] 16%|█▌        | 5/32 [04:52<26:16, 58.38s/it] 19%|█▉        | 6/32 [04:53<21:39, 49.99s/it] 25%|██▌       | 8/32 [05:17<15:49, 39.55s/it] 25%|██▌       | 8/32 [05:17<15:49, 39.55s/it] 25%|██▌       | 8/32 [05:20<15:53, 39.71s/it] 25%|██▌       | 8/32 [05:23<16:03, 40.16s/it] 25%|██▌       | 8/32 [05:24<16:21, 40.91s/it] 22%|██▏       | 7/32 [05:40<20:25, 49.01s/it] 19%|█▉        | 6/32 [05:46<24:56, 57.55s/it] 19%|█▉        | 6/32 [05:50<25:07, 57.99s/it] 28%|██▊       | 9/32 [05:57<15:15, 39.81s/it] 28%|██▊       | 9/32 [05:58<15:23, 40.17s/it] 28%|██▊       | 9/32 [05:59<15:12, 39.67s/it] 28%|██▊       | 9/32 [06:03<15:29, 40.40s/it] 28%|██▊       | 9/32 [06:04<15:29, 40.40s/it] 25%|██▌       | 8/32 [06:29<19:34, 48.92s/it] 31%|███▏      | 10/32 [06:36<14:28, 39.48s/it] 31%|███▏      | 10/32 [06:39<14:47, 40.33s/it] 31%|███▏      | 10/32 [06:39<14:32, 39.65s/it] 31%|███▏      | 10/32 [06:42<14:41, 40.07s/it] 31%|███▏      | 10/32 [06:44<14:47, 40.33s/it] 22%|██▏       | 7/32 [06:45<24:09, 58.00s/it] 22%|██▏       | 7/32 [06:47<24:02, 57.68s/it] 34%|███▍      | 11/32 [07:15<13:44, 39.25s/it] 28%|██▊       | 9/32 [07:16<18:32, 48.37s/it] 34%|███▍      | 11/32 [07:18<14:00, 40.01s/it] 34%|███▍      | 11/32 [07:19<13:54, 39.75s/it] 34%|███▍      | 11/32 [07:22<13:57, 39.90s/it] 34%|███▍      | 11/32 [07:23<13:58, 39.92s/it] 25%|██▌       | 8/32 [07:44<23:20, 58.34s/it] 25%|██▌       | 8/32 [07:46<23:14, 58.11s/it] 38%|███▊      | 12/32 [07:54<13:03, 39.20s/it] 38%|███▊      | 12/32 [07:58<13:18, 39.93s/it] 38%|███▊      | 12/32 [08:01<13:13, 39.69s/it] 31%|███▏      | 10/32 [08:03<17:35, 47.96s/it] 38%|███▊      | 12/32 [08:03<13:14, 39.73s/it] 38%|███▊      | 12/32 [08:04<13:49, 41.46s/it] 41%|████      | 13/32 [08:33<12:23, 39.14s/it] 41%|████      | 13/32 [08:38<12:36, 39.83s/it] 41%|████      | 13/32 [08:41<12:34, 39.71s/it] 41%|████      | 13/32 [08:42<12:31, 39.56s/it] 28%|██▊       | 9/32 [08:42<22:15, 58.04s/it] 28%|██▊       | 9/32 [08:43<22:10, 57.84s/it] 41%|████      | 13/32 [08:45<13:03, 41.24s/it] 34%|███▍      | 11/32 [08:50<16:40, 47.65s/it] 44%|████▍     | 14/32 [09:13<11:51, 39.52s/it] 44%|████▍     | 14/32 [09:18<11:58, 39.90s/it] 44%|████▍     | 14/32 [09:21<11:58, 39.90s/it] 44%|████▍     | 14/32 [09:21<11:51, 39.55s/it] 44%|████▍     | 14/32 [09:27<12:26, 41.48s/it] 38%|███▊      | 12/32 [09:37<15:49, 47.48s/it] 31%|███▏      | 10/32 [09:40<21:06, 57.57s/it] 31%|███▏      | 10/32 [09:41<21:22, 58.31s/it] 47%|████▋     | 15/32 [09:52<11:08, 39.31s/it] 47%|████▋     | 15/32 [09:57<11:16, 39.79s/it] 47%|████▋     | 15/32 [10:01<11:15, 39.75s/it] 47%|████▋     | 15/32 [10:00<11:09, 39.40s/it] 47%|████▋     | 15/32 [10:06<11:34, 40.86s/it] 41%|████      | 13/32 [10:24<15:01, 47.43s/it] 50%|█████     | 16/32 [10:34<10:40, 40.01s/it] 50%|█████     | 16/32 [10:37<10:36, 39.79s/it] 34%|███▍      | 11/32 [10:37<20:09, 57.58s/it] 34%|███▍      | 11/32 [10:38<20:18, 58.04s/it] 50%|█████     | 16/32 [10:40<10:30, 39.42s/it] 50%|█████     | 16/32 [10:40<10:35, 39.74s/it] 50%|█████     | 16/32 [10:47<10:51, 40.69s/it] 44%|████▍     | 14/32 [11:11<14:11, 47.29s/it] 53%|█████▎    | 17/32 [11:13<09:58, 39.89s/it] 53%|█████▎    | 17/32 [11:16<09:54, 39.65s/it] 53%|█████▎    | 17/32 [11:20<09:54, 39.64s/it] 53%|█████▎    | 17/32 [11:23<10:06, 40.41s/it] 53%|█████▎    | 17/32 [11:27<10:10, 40.69s/it] 38%|███▊      | 12/32 [11:35<19:08, 57.41s/it] 38%|███▊      | 12/32 [11:35<19:15, 57.76s/it] 56%|█████▋    | 18/32 [11:53<09:16, 39.73s/it] 56%|█████▋    | 18/32 [11:56<09:15, 39.71s/it] 47%|████▋     | 15/32 [11:58<13:23, 47.27s/it] 56%|█████▋    | 18/32 [11:59<09:15, 39.67s/it] 56%|█████▋    | 18/32 [12:01<09:18, 39.89s/it] 56%|█████▋    | 18/32 [12:07<09:23, 40.28s/it] 59%|█████▉    | 19/32 [12:32<08:33, 39.51s/it] 41%|████      | 13/32 [12:32<18:10, 57.39s/it] 59%|█████▉    | 19/32 [12:35<08:34, 39.56s/it] 41%|████      | 13/32 [12:36<18:36, 58.74s/it] 59%|█████▉    | 19/32 [12:39<08:33, 39.53s/it] 59%|█████▉    | 19/32 [12:40<08:35, 39.64s/it] 50%|█████     | 16/32 [12:45<12:35, 47.22s/it] 59%|█████▉    | 19/32 [12:46<08:41, 40.13s/it] 62%|██████▎   | 20/32 [13:11<07:52, 39.38s/it] 62%|██████▎   | 20/32 [13:15<07:55, 39.60s/it] 62%|██████▎   | 20/32 [13:18<07:52, 39.36s/it] 62%|██████▎   | 20/32 [13:19<07:53, 39.46s/it] 62%|██████▎   | 20/32 [13:26<07:59, 39.95s/it] 44%|████▍     | 14/32 [13:29<17:11, 57.31s/it] 53%|█████▎    | 17/32 [13:34<11:55, 47.71s/it] 44%|████▍     | 14/32 [13:37<17:49, 59.39s/it] 66%|██████▌   | 21/32 [13:50<07:12, 39.35s/it] 66%|██████▌   | 21/32 [13:55<07:16, 39.73s/it] 66%|██████▌   | 21/32 [13:57<07:12, 39.31s/it] 66%|██████▌   | 21/32 [14:00<07:16, 39.69s/it] 66%|██████▌   | 21/32 [14:05<07:17, 39.76s/it] 56%|█████▋    | 18/32 [14:25<11:19, 48.50s/it] 47%|████▋     | 15/32 [14:26<16:12, 57.19s/it] 69%|██████▉   | 22/32 [14:29<06:33, 39.32s/it] 47%|████▋     | 15/32 [14:35<16:40, 58.86s/it] 69%|██████▉   | 22/32 [14:36<06:41, 40.12s/it] 69%|██████▉   | 22/32 [14:39<06:40, 40.06s/it] 69%|██████▉   | 22/32 [14:39<06:35, 39.58s/it] 69%|██████▉   | 22/32 [14:45<06:36, 39.70s/it] 72%|███████▏  | 23/32 [15:09<05:53, 39.30s/it] 59%|█████▉    | 19/32 [15:12<10:25, 48.10s/it] 72%|███████▏  | 23/32 [15:16<05:59, 39.93s/it] 72%|███████▏  | 23/32 [15:18<05:58, 39.84s/it] 72%|███████▏  | 23/32 [15:18<05:55, 39.50s/it] 50%|█████     | 16/32 [15:23<15:14, 57.19s/it] 72%|███████▏  | 23/32 [15:24<05:56, 39.66s/it] 50%|█████     | 16/32 [15:32<15:34, 58.43s/it] 75%|███████▌  | 24/32 [15:48<05:13, 39.22s/it] 75%|███████▌  | 24/32 [15:55<05:18, 39.84s/it] 75%|███████▌  | 24/32 [15:57<05:17, 39.65s/it] 75%|███████▌  | 24/32 [15:58<05:16, 39.62s/it] 62%|██████▎   | 20/32 [15:59<09:33, 47.80s/it] 75%|███████▌  | 24/32 [16:05<05:18, 39.86s/it] 53%|█████▎    | 17/32 [16:22<14:24, 57.62s/it] 78%|███████▊  | 25/32 [16:27<04:33, 39.12s/it] 53%|█████▎    | 17/32 [16:31<14:39, 58.63s/it] 78%|███████▊  | 25/32 [16:35<04:39, 39.86s/it] 78%|███████▊  | 25/32 [16:37<04:37, 39.62s/it] 78%|███████▊  | 25/32 [16:37<04:36, 39.48s/it] 66%|██████▌   | 21/32 [16:39<08:19, 45.44s/it] 78%|███████▊  | 25/32 [16:44<04:38, 39.78s/it] 81%|████████▏ | 26/32 [17:09<04:00, 40.02s/it] 81%|████████▏ | 26/32 [17:15<03:59, 39.84s/it] 81%|████████▏ | 26/32 [17:16<03:57, 39.55s/it] 81%|████████▏ | 26/32 [17:16<03:55, 39.30s/it] 69%|██████▉   | 22/32 [17:18<07:17, 43.71s/it] 56%|█████▋    | 18/32 [17:19<13:23, 57.42s/it] 81%|████████▏ | 26/32 [17:24<03:58, 39.82s/it] 56%|█████▋    | 18/32 [17:29<13:36, 58.30s/it] 84%|████████▍ | 27/32 [17:49<03:20, 40.14s/it] 84%|████████▍ | 27/32 [17:55<03:18, 39.78s/it] 84%|████████▍ | 27/32 [17:55<03:17, 39.50s/it] 72%|███████▏  | 23/32 [17:58<06:22, 42.51s/it] 84%|████████▍ | 27/32 [17:58<03:20, 40.13s/it] 84%|████████▍ | 27/32 [18:04<03:18, 39.68s/it] 59%|█████▉    | 19/32 [18:16<12:25, 57.37s/it] 88%|████████▊ | 28/32 [18:28<02:39, 39.80s/it] 59%|█████▉    | 19/32 [18:30<12:48, 59.08s/it] 88%|████████▊ | 28/32 [18:35<02:39, 39.82s/it] 88%|████████▊ | 28/32 [18:35<02:38, 39.51s/it] 88%|████████▊ | 28/32 [18:37<02:39, 39.76s/it] 75%|███████▌  | 24/32 [18:38<05:32, 41.56s/it] 88%|████████▊ | 28/32 [18:43<02:38, 39.65s/it] 94%|█████████▍| 30/32 [19:08<01:01, 30.69s/it] 91%|█████████ | 29/32 [19:14<01:59, 39.67s/it] 91%|█████████ | 29/32 [19:14<01:58, 39.51s/it] 62%|██████▎   | 20/32 [19:15<11:34, 57.89s/it] 78%|███████▊  | 25/32 [19:17<04:45, 40.85s/it] 91%|█████████ | 29/32 [19:20<02:01, 40.61s/it] 91%|█████████ | 29/32 [19:23<01:58, 39.62s/it] 62%|██████▎   | 20/32 [19:27<11:42, 58.53s/it] 97%|█████████▋| 31/32 [19:47<00:32, 32.75s/it] 94%|█████████▍| 30/32 [19:54<01:19, 39.70s/it] 94%|█████████▍| 30/32 [19:54<01:19, 39.53s/it] 81%|████████▏ | 26/32 [19:56<04:02, 40.42s/it] 94%|█████████▍| 30/32 [19:59<01:20, 40.07s/it] 94%|█████████▍| 30/32 [20:03<01:19, 39.89s/it] 66%|██████▌   | 21/32 [20:12<10:34, 57.70s/it] 66%|██████▌   | 21/32 [20:24<10:39, 58.12s/it]100%|██████████| 32/32 [20:26<00:00, 34.38s/it]100%|██████████| 32/32 [20:26<00:00, 38.34s/it]
 97%|█████████▋| 31/32 [20:33<00:39, 39.74s/it] 97%|█████████▋| 31/32 [20:34<00:39, 39.56s/it] 84%|████████▍ | 27/32 [20:36<03:21, 40.37s/it] 97%|█████████▋| 31/32 [20:37<00:39, 39.72s/it] 97%|█████████▋| 31/32 [20:42<00:39, 39.67s/it] 69%|██████▉   | 22/32 [21:09<09:34, 57.47s/it]100%|██████████| 32/32 [21:13<00:00, 39.53s/it]100%|██████████| 32/32 [21:13<00:00, 39.78s/it]
100%|██████████| 32/32 [21:14<00:00, 39.87s/it]100%|██████████| 32/32 [21:14<00:00, 39.84s/it]
100%|██████████| 32/32 [21:16<00:00, 39.50s/it]100%|██████████| 32/32 [21:16<00:00, 39.91s/it]
 88%|████████▊ | 28/32 [21:21<02:46, 41.56s/it] 69%|██████▉   | 22/32 [21:22<09:41, 58.12s/it]100%|██████████| 32/32 [21:23<00:00, 39.94s/it]100%|██████████| 32/32 [21:23<00:00, 40.11s/it]
 91%|█████████ | 29/32 [22:00<02:03, 41.02s/it] 72%|███████▏  | 23/32 [22:06<08:35, 57.25s/it] 72%|███████▏  | 23/32 [22:19<08:40, 57.85s/it] 94%|█████████▍| 30/32 [22:40<01:21, 40.67s/it] 78%|███████▊  | 25/32 [23:03<05:07, 44.00s/it] 75%|███████▌  | 24/32 [23:17<07:42, 57.75s/it] 97%|█████████▋| 31/32 [23:20<00:40, 40.32s/it]100%|██████████| 32/32 [23:59<00:00, 40.09s/it]100%|██████████| 32/32 [23:59<00:00, 45.00s/it]
 81%|████████▏ | 26/32 [24:02<04:46, 47.70s/it] 78%|███████▊  | 25/32 [24:15<06:44, 57.72s/it] 84%|████████▍ | 27/32 [24:59<04:10, 50.11s/it] 81%|████████▏ | 26/32 [25:12<05:45, 57.54s/it] 88%|████████▊ | 28/32 [25:56<03:27, 51.99s/it] 84%|████████▍ | 27/32 [26:09<04:47, 57.51s/it] 91%|█████████ | 29/32 [26:53<02:40, 53.41s/it] 88%|████████▊ | 28/32 [27:08<03:51, 57.93s/it] 94%|█████████▍| 30/32 [27:50<01:49, 54.52s/it] 91%|█████████ | 29/32 [28:09<02:56, 58.82s/it] 97%|█████████▋| 31/32 [28:47<00:55, 55.21s/it] 94%|█████████▍| 30/32 [29:07<01:57, 58.54s/it]100%|██████████| 32/32 [29:44<00:00, 55.67s/it]100%|██████████| 32/32 [29:44<00:00, 55.76s/it]
 97%|█████████▋| 31/32 [30:00<00:57, 57.07s/it][rank5]:[E ProcessGroupNCCL.cpp:523] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600345 milliseconds before timing out.
[rank5]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E ProcessGroupNCCL.cpp:1182] [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600345 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f8611e0dd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f8612fb56e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f8612fb8c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f8612fb9839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f865ccccdf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f865e0bb609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f865de86353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600345 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f8611e0dd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f8612fb56e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f8612fb8c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f8612fb9839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f865ccccdf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f865e0bb609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f865de86353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f8611e0dd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f8612d0fb11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f865ccccdf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f865e0bb609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f865de86353 in /lib/x86_64-linux-gnu/libc.so.6)

100%|██████████| 32/32 [30:48<00:00, 54.11s/it]100%|██████████| 32/32 [30:48<00:00, 57.75s/it]
[2024-07-08 22:05:58,763] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1544759 closing signal SIGTERM
[2024-07-08 22:05:58,763] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1544760 closing signal SIGTERM
[2024-07-08 22:05:58,764] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1544761 closing signal SIGTERM
[2024-07-08 22:05:58,764] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1544762 closing signal SIGTERM
[2024-07-08 22:05:58,765] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1544763 closing signal SIGTERM
[2024-07-08 22:05:58,765] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1544765 closing signal SIGTERM
[2024-07-08 22:05:58,765] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1544766 closing signal SIGTERM
[2024-07-08 22:06:00,495] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 5 (pid: 1544764) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
main.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-08_22:05:58
  host      : a100-st-p4de24xlarge-164.fair-a100.hpcaas
  rank      : 5 (local_rank: 5)
  exitcode  : -6 (pid: 1544764)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 1544764
========================================================
