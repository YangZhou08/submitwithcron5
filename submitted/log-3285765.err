Already on 'yangexp2two'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-06:01:46:39,265 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:46:39,265 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:46:39,265 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:46:39,265 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:46:39,266 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:46:39,266 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:46:39,267 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:46:39,275 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:46:48,668 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:46:48,668 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:46:48,668 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:46:48,668 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:46:48,668 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:46:48,668 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:46:48,668 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:46:48,668 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:46:48,694 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:46:48,694 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:46:48,694 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:46:48,694 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:46:48,694 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:46:48,694 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:46:48,694 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:46:48,694 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:01:46:48,694 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:01:46:48,694 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:01:46:48,694 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:01:46:48,694 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:01:46:48,694 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:01:46:48,694 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-06:01:46:48,694 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:46:48,694 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:50, 16.85s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:52, 17.42s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:52, 17.49s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:52, 17.40s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:51, 17.31s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:52, 17.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:53, 17.75s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:53, 17.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:32<00:31, 16.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:33, 16.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:47<00:15, 15.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:15, 15.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:15, 15.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:15, 15.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:16, 16.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:15, 15.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:15, 15.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:15, 15.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 10.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 10.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.48s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 10.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 10.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 12.53s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 12.50s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 10.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.49s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 10.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.47s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 10.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.47s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 10.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 12.70s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-06:01:48:28,299 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/164 [00:00<?, ?it/s] 12%|█▏        | 19/164 [00:00<00:00, 188.37it/s] 24%|██▍       | 39/164 [00:00<00:00, 189.93it/s] 36%|███▌      | 59/164 [00:00<00:00, 190.29it/s]2024-07-06:01:48:28,662 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/165 [00:00<?, ?it/s] 48%|████▊     | 79/164 [00:00<00:00, 190.98it/s] 12%|█▏        | 20/165 [00:00<00:00, 191.39it/s]2024-07-06:01:48:28,819 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
 60%|██████    | 99/164 [00:00<00:00, 191.71it/s]  0%|          | 0/165 [00:00<?, ?it/s]2024-07-06:01:48:28,886 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
 24%|██▍       | 40/165 [00:00<00:00, 193.02it/s]  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 188.02it/s] 73%|███████▎  | 119/164 [00:00<00:00, 192.16it/s]2024-07-06:01:48:28,973 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
 36%|███▋      | 60/165 [00:00<00:00, 193.55it/s]  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 188.55it/s] 85%|████████▍ | 139/164 [00:00<00:00, 192.41it/s] 24%|██▎       | 39/165 [00:00<00:00, 189.73it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.53it/s] 48%|████▊     | 80/165 [00:00<00:00, 194.48it/s] 24%|██▎       | 39/165 [00:00<00:00, 190.06it/s]2024-07-06:01:48:29,138 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
 97%|█████████▋| 159/164 [00:00<00:00, 192.52it/s] 36%|███▌      | 59/165 [00:00<00:00, 190.20it/s]  0%|          | 0/165 [00:00<?, ?it/s]100%|██████████| 164/164 [00:00<00:00, 191.72it/s]
 61%|██████    | 100/165 [00:00<00:00, 194.95it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.41it/s] 36%|███▌      | 59/165 [00:00<00:00, 190.61it/s] 48%|████▊     | 79/165 [00:00<00:00, 189.93it/s] 12%|█▏        | 20/165 [00:00<00:00, 191.38it/s] 73%|███████▎  | 120/165 [00:00<00:00, 195.32it/s] 36%|███▌      | 59/165 [00:00<00:00, 191.91it/s] 48%|████▊     | 79/165 [00:00<00:00, 191.36it/s] 60%|██████    | 99/165 [00:00<00:00, 190.44it/s] 24%|██▍       | 40/165 [00:00<00:00, 193.00it/s] 85%|████████▍ | 140/165 [00:00<00:00, 195.53it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.67it/s] 60%|██████    | 99/165 [00:00<00:00, 192.04it/s] 72%|███████▏  | 119/165 [00:00<00:00, 190.90it/s] 36%|███▋      | 60/165 [00:00<00:00, 192.59it/s] 97%|█████████▋| 160/165 [00:00<00:00, 195.59it/s] 60%|██████    | 99/165 [00:00<00:00, 193.13it/s] 72%|███████▏  | 119/165 [00:00<00:00, 192.37it/s]100%|██████████| 165/165 [00:00<00:00, 194.88it/s]
 84%|████████▍ | 139/165 [00:00<00:00, 191.24it/s] 48%|████▊     | 80/165 [00:00<00:00, 193.33it/s] 72%|███████▏  | 119/165 [00:00<00:00, 193.35it/s] 84%|████████▍ | 139/165 [00:00<00:00, 192.76it/s] 96%|█████████▋| 159/165 [00:00<00:00, 191.23it/s] 61%|██████    | 100/165 [00:00<00:00, 193.80it/s]100%|██████████| 165/165 [00:00<00:00, 190.67it/s]
 84%|████████▍ | 139/165 [00:00<00:00, 193.53it/s] 96%|█████████▋| 159/165 [00:00<00:00, 192.83it/s]100%|██████████| 165/165 [00:00<00:00, 192.05it/s]
 73%|███████▎  | 120/165 [00:00<00:00, 194.06it/s] 96%|█████████▋| 159/165 [00:00<00:00, 193.47it/s]100%|██████████| 165/165 [00:00<00:00, 192.90it/s]
 85%|████████▍ | 140/165 [00:00<00:00, 194.30it/s] 97%|█████████▋| 160/165 [00:00<00:00, 194.51it/s]100%|██████████| 165/165 [00:00<00:00, 193.89it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-06:01:48:55,901 INFO     [xhuggingface.py:323] Using 8 devices with data parallelism
2024-07-06:01:48:56,770 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/165 [00:00<?, ?it/s] 10%|▉         | 16/165 [00:00<00:00, 155.81it/s] 19%|█▉        | 32/165 [00:00<00:01, 131.95it/s] 28%|██▊       | 46/165 [00:00<00:00, 121.97it/s]2024-07-06:01:48:57,211 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/165 [00:00<?, ?it/s] 36%|███▌      | 59/165 [00:00<00:00, 120.25it/s]  7%|▋         | 12/165 [00:00<00:01, 115.79it/s] 44%|████▎     | 72/165 [00:00<00:00, 119.32it/s] 15%|█▍        | 24/165 [00:00<00:01, 117.10it/s] 51%|█████     | 84/165 [00:00<00:00, 119.02it/s] 22%|██▏       | 36/165 [00:00<00:01, 117.73it/s] 58%|█████▊    | 96/165 [00:00<00:00, 118.82it/s] 29%|██▉       | 48/165 [00:00<00:00, 118.00it/s] 65%|██████▌   | 108/165 [00:00<00:00, 118.10it/s] 36%|███▋      | 60/165 [00:00<00:00, 118.08it/s] 73%|███████▎  | 120/165 [00:00<00:00, 118.19it/s] 44%|████▎     | 72/165 [00:00<00:00, 118.28it/s] 80%|████████  | 132/165 [00:01<00:00, 118.29it/s] 51%|█████     | 84/165 [00:00<00:00, 118.49it/s] 87%|████████▋ | 144/165 [00:01<00:00, 118.14it/s] 58%|█████▊    | 96/165 [00:00<00:00, 118.50it/s] 95%|█████████▍| 156/165 [00:01<00:00, 118.31it/s]100%|██████████| 165/165 [00:01<00:00, 120.24it/s]
 66%|██████▌   | 109/165 [00:00<00:00, 120.26it/s] 78%|███████▊  | 129/165 [00:01<00:00, 142.43it/s] 90%|█████████ | 149/165 [00:01<00:00, 157.48it/s]100%|██████████| 165/165 [00:01<00:00, 136.31it/s]
2024-07-06:01:49:09,687 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:49:09,687 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:49:09,687 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:49:09,687 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:49:09,688 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:49:09,688 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:49:09,688 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:49:09,688 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/165 [00:11<30:30, 11.16s/it]Running generate_until requests:   1%|          | 2/165 [00:19<26:14,  9.66s/it]Running generate_until requests:   2%|▏         | 3/165 [00:39<38:38, 14.31s/it]Running generate_until requests:   2%|▏         | 4/165 [00:50<34:52, 13.00s/it]Running generate_until requests:   3%|▎         | 5/165 [00:58<29:20, 11.00s/it]Running generate_until requests:   4%|▎         | 6/165 [01:10<30:11, 11.39s/it]Running generate_until requests:   4%|▍         | 7/165 [01:16<25:29,  9.68s/it]Running generate_until requests:   5%|▍         | 8/165 [01:24<24:14,  9.27s/it]Running generate_until requests:   5%|▌         | 9/165 [01:29<20:35,  7.92s/it]Running generate_until requests:   6%|▌         | 10/165 [01:37<20:05,  7.78s/it]Running generate_until requests:   7%|▋         | 11/165 [01:46<21:29,  8.37s/it]Running generate_until requests:   7%|▋         | 12/165 [01:56<22:25,  8.80s/it]Running generate_until requests:   8%|▊         | 13/165 [02:07<23:58,  9.46s/it]Running generate_until requests:   8%|▊         | 14/165 [02:21<26:54, 10.69s/it]Running generate_until requests:   9%|▉         | 15/165 [02:33<27:46, 11.11s/it]Running generate_until requests:  10%|▉         | 16/165 [02:41<25:32, 10.28s/it]Running generate_until requests:  10%|█         | 17/165 [02:59<31:21, 12.71s/it]Running generate_until requests:  11%|█         | 18/165 [03:15<33:13, 13.56s/it]Running generate_until requests:  12%|█▏        | 19/165 [03:22<28:27, 11.70s/it]Running generate_until requests:  12%|█▏        | 20/165 [03:29<24:15, 10.04s/it]Running generate_until requests:  13%|█▎        | 21/165 [03:44<27:42, 11.54s/it]Running generate_until requests:  13%|█▎        | 22/165 [03:50<23:43,  9.95s/it]Running generate_until requests:  14%|█▍        | 23/165 [03:55<20:02,  8.47s/it]Running generate_until requests:  15%|█▍        | 24/165 [04:02<18:58,  8.07s/it]Running generate_until requests:  15%|█▌        | 25/165 [04:11<19:12,  8.23s/it]Running generate_until requests:  16%|█▌        | 26/165 [04:18<18:26,  7.96s/it]Running generate_until requests:  16%|█▋        | 27/165 [04:27<19:18,  8.40s/it]Running generate_until requests:  17%|█▋        | 28/165 [04:32<16:47,  7.35s/it]Running generate_until requests:  18%|█▊        | 29/165 [04:43<18:48,  8.30s/it]Running generate_until requests:  18%|█▊        | 30/165 [04:50<17:51,  7.94s/it]Running generate_until requests:  19%|█▉        | 31/165 [04:54<14:55,  6.69s/it]Running generate_until requests:  19%|█▉        | 31/165 [05:01<21:43,  9.73s/it]
