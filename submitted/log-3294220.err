Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:09, 23.33s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.97s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:16, 25.45s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.76s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.43s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:35, 17.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:36, 18.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:36, 18.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:36, 18.27s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:36, 18.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:37, 18.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:36, 18.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:36, 18.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:50<00:15, 15.27s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:51<00:16, 16.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:51<00:16, 16.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:51<00:16, 16.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:51<00:16, 16.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:52<00:16, 16.35s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:51<00:15, 15.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:51<00:15, 15.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:52<00:00, 10.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:52<00:00, 13.04s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 10.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 13.33s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 10.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 13.35s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 10.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 13.34s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 10.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 13.36s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 10.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 13.48s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 10.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 13.28s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 10.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 13.40s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:34<17:37, 34.12s/it]  3%|▎         | 1/32 [00:35<18:35, 35.97s/it]  3%|▎         | 1/32 [00:36<19:02, 36.86s/it]  3%|▎         | 1/32 [00:37<19:26, 37.63s/it]  3%|▎         | 1/32 [00:38<19:54, 38.52s/it]  3%|▎         | 1/32 [00:39<20:33, 39.80s/it]  3%|▎         | 1/32 [00:52<27:16, 52.78s/it]  3%|▎         | 1/32 [00:54<28:02, 54.27s/it]  6%|▋         | 2/32 [01:09<17:21, 34.73s/it]  6%|▋         | 2/32 [01:09<17:17, 34.58s/it]  6%|▋         | 2/32 [01:11<17:49, 35.63s/it]  6%|▋         | 2/32 [01:13<18:10, 36.37s/it]  6%|▋         | 2/32 [01:14<18:20, 36.68s/it]  6%|▋         | 2/32 [01:16<19:00, 38.02s/it]  9%|▉         | 3/32 [01:42<16:23, 33.93s/it]  9%|▉         | 3/32 [01:45<16:47, 34.74s/it]  9%|▉         | 3/32 [01:46<17:09, 35.50s/it]  6%|▋         | 2/32 [01:47<26:48, 53.60s/it]  6%|▋         | 2/32 [01:47<26:58, 53.96s/it]  9%|▉         | 3/32 [01:48<17:19, 35.84s/it]  9%|▉         | 3/32 [01:49<17:28, 36.17s/it]  9%|▉         | 3/32 [01:52<18:04, 37.41s/it] 12%|█▎        | 4/32 [02:15<15:41, 33.61s/it] 12%|█▎        | 4/32 [02:18<16:00, 34.31s/it] 12%|█▎        | 4/32 [02:19<16:14, 34.79s/it] 12%|█▎        | 4/32 [02:21<16:15, 34.84s/it] 12%|█▎        | 4/32 [02:25<16:45, 35.91s/it] 12%|█▎        | 4/32 [02:26<16:44, 35.88s/it]  9%|▉         | 3/32 [02:40<25:53, 53.57s/it]  9%|▉         | 3/32 [02:43<26:31, 54.88s/it] 16%|█▌        | 5/32 [02:49<15:10, 33.74s/it] 16%|█▌        | 5/32 [02:54<15:36, 34.69s/it] 16%|█▌        | 5/32 [02:55<15:51, 35.24s/it] 16%|█▌        | 5/32 [02:56<15:45, 35.02s/it] 16%|█▌        | 5/32 [02:58<15:45, 35.01s/it] 16%|█▌        | 5/32 [02:59<15:45, 35.02s/it] 19%|█▉        | 6/32 [03:24<14:48, 34.19s/it] 19%|█▉        | 6/32 [03:27<14:52, 34.32s/it] 19%|█▉        | 6/32 [03:30<14:53, 34.35s/it] 19%|█▉        | 6/32 [03:31<15:20, 35.39s/it] 19%|█▉        | 6/32 [03:35<15:23, 35.51s/it] 19%|█▉        | 6/32 [03:37<15:34, 35.94s/it] 12%|█▎        | 4/32 [03:42<26:26, 56.66s/it] 12%|█▎        | 4/32 [03:43<26:31, 56.84s/it] 22%|██▏       | 7/32 [03:57<14:06, 33.85s/it] 22%|██▏       | 7/32 [04:01<14:12, 34.09s/it] 22%|██▏       | 7/32 [04:05<14:31, 34.85s/it] 22%|██▏       | 7/32 [04:09<14:40, 35.20s/it] 22%|██▏       | 7/32 [04:09<15:03, 36.15s/it] 22%|██▏       | 7/32 [04:13<14:56, 35.86s/it] 25%|██▌       | 8/32 [04:31<13:35, 33.96s/it] 25%|██▌       | 8/32 [04:35<13:33, 33.90s/it] 16%|█▌        | 5/32 [04:36<24:59, 55.54s/it] 16%|█▌        | 5/32 [04:38<25:28, 56.61s/it] 25%|██▌       | 8/32 [04:40<14:02, 35.12s/it] 25%|██▌       | 8/32 [04:44<13:59, 34.97s/it] 25%|██▌       | 8/32 [04:48<14:18, 35.78s/it] 25%|██▌       | 8/32 [04:56<15:50, 39.60s/it] 28%|██▊       | 9/32 [05:04<12:55, 33.72s/it] 28%|██▊       | 9/32 [05:10<13:12, 34.47s/it] 28%|██▊       | 9/32 [05:14<13:17, 34.69s/it] 28%|██▊       | 9/32 [05:17<13:12, 34.48s/it] 28%|██▊       | 9/32 [05:24<13:42, 35.76s/it] 19%|█▉        | 6/32 [05:33<24:11, 55.83s/it] 19%|█▉        | 6/32 [05:34<24:29, 56.54s/it] 31%|███▏      | 10/32 [05:41<12:38, 34.47s/it] 28%|██▊       | 9/32 [05:43<16:02, 41.86s/it] 31%|███▏      | 10/32 [05:46<12:45, 34.79s/it] 31%|███▏      | 10/32 [05:51<12:57, 35.33s/it] 31%|███▏      | 10/32 [05:53<12:45, 34.79s/it] 31%|███▏      | 10/32 [05:59<12:58, 35.37s/it] 34%|███▍      | 11/32 [06:15<12:01, 34.37s/it] 34%|███▍      | 11/32 [06:21<12:15, 35.03s/it] 34%|███▍      | 11/32 [06:26<12:17, 35.13s/it] 31%|███▏      | 10/32 [06:27<15:37, 42.60s/it] 34%|███▍      | 11/32 [06:28<12:15, 35.01s/it] 22%|██▏       | 7/32 [06:29<23:19, 55.98s/it] 22%|██▏       | 7/32 [06:31<23:34, 56.58s/it] 34%|███▍      | 11/32 [06:32<12:10, 34.79s/it] 38%|███▊      | 12/32 [06:51<11:38, 34.93s/it] 38%|███▊      | 12/32 [06:56<11:38, 34.91s/it] 38%|███▊      | 12/32 [06:59<11:33, 34.69s/it] 38%|███▊      | 12/32 [07:03<11:37, 34.87s/it] 38%|███▊      | 12/32 [07:07<11:34, 34.73s/it] 34%|███▍      | 11/32 [07:12<15:05, 43.10s/it] 25%|██▌       | 8/32 [07:23<22:03, 55.13s/it] 41%|████      | 13/32 [07:25<11:00, 34.76s/it] 25%|██▌       | 8/32 [07:26<22:27, 56.15s/it] 41%|████      | 13/32 [07:33<11:12, 35.40s/it] 41%|████      | 13/32 [07:33<10:53, 34.40s/it] 41%|████      | 13/32 [07:39<11:10, 35.28s/it] 41%|████      | 13/32 [07:40<10:54, 34.43s/it] 38%|███▊      | 12/32 [07:56<14:30, 43.53s/it] 44%|████▍     | 14/32 [07:59<10:18, 34.37s/it] 44%|████▍     | 14/32 [08:07<10:15, 34.18s/it] 44%|████▍     | 14/32 [08:08<10:38, 35.50s/it] 44%|████▍     | 14/32 [08:14<10:33, 35.19s/it] 44%|████▍     | 14/32 [08:14<10:15, 34.22s/it] 28%|██▊       | 9/32 [08:19<21:17, 55.54s/it] 28%|██▊       | 9/32 [08:19<21:10, 55.22s/it] 47%|████▋     | 15/32 [08:33<09:44, 34.39s/it] 47%|████▋     | 15/32 [08:40<09:38, 34.01s/it] 47%|████▋     | 15/32 [08:42<09:53, 34.91s/it] 41%|████      | 13/32 [08:43<14:06, 44.55s/it] 47%|████▋     | 15/32 [08:47<09:46, 34.50s/it] 47%|████▋     | 15/32 [08:51<09:55, 35.04s/it] 50%|█████     | 16/32 [09:11<09:25, 35.34s/it] 31%|███▏      | 10/32 [09:14<20:17, 55.32s/it] 50%|█████     | 16/32 [09:14<09:02, 33.89s/it] 31%|███▏      | 10/32 [09:14<20:12, 55.10s/it] 50%|█████     | 16/32 [09:15<09:11, 34.49s/it] 50%|█████     | 16/32 [09:20<09:04, 34.05s/it] 50%|█████     | 16/32 [09:25<09:14, 34.65s/it] 44%|████▍     | 14/32 [09:29<13:27, 44.88s/it] 53%|█████▎    | 17/32 [09:47<08:55, 35.69s/it] 53%|█████▎    | 17/32 [09:49<08:32, 34.17s/it] 53%|█████▎    | 17/32 [09:51<08:40, 34.71s/it] 53%|█████▎    | 17/32 [09:53<08:25, 33.71s/it] 53%|█████▎    | 17/32 [09:59<08:35, 34.39s/it] 34%|███▍      | 11/32 [10:07<19:05, 54.56s/it] 34%|███▍      | 11/32 [10:10<19:29, 55.71s/it] 47%|████▋     | 15/32 [10:13<12:40, 44.71s/it] 56%|█████▋    | 18/32 [10:21<08:10, 35.04s/it] 56%|█████▋    | 18/32 [10:23<07:56, 34.06s/it] 56%|█████▋    | 18/32 [10:26<07:48, 33.50s/it] 56%|█████▋    | 18/32 [10:28<08:15, 35.42s/it] 56%|█████▋    | 18/32 [10:32<07:58, 34.21s/it] 59%|█████▉    | 19/32 [10:55<07:32, 34.83s/it] 59%|█████▉    | 19/32 [10:58<07:28, 34.54s/it] 50%|█████     | 16/32 [10:59<11:59, 44.98s/it] 59%|█████▉    | 19/32 [10:59<07:14, 33.39s/it] 59%|█████▉    | 19/32 [11:01<07:32, 34.83s/it] 59%|█████▉    | 19/32 [11:06<07:23, 34.08s/it] 38%|███▊      | 12/32 [11:08<18:45, 56.26s/it] 38%|███▊      | 12/32 [11:14<19:19, 57.99s/it] 62%|██████▎   | 20/32 [11:30<06:56, 34.70s/it] 62%|██████▎   | 20/32 [11:32<06:50, 34.23s/it] 62%|██████▎   | 20/32 [11:33<06:43, 33.61s/it] 62%|██████▎   | 20/32 [11:35<06:53, 34.46s/it] 62%|██████▎   | 20/32 [11:43<06:58, 34.90s/it] 53%|█████▎    | 17/32 [11:44<11:17, 45.14s/it] 41%|████      | 13/32 [12:03<17:41, 55.87s/it] 66%|██████▌   | 21/32 [12:03<06:17, 34.35s/it] 66%|██████▌   | 21/32 [12:06<06:08, 33.46s/it] 66%|██████▌   | 21/32 [12:07<06:21, 34.66s/it] 66%|██████▌   | 21/32 [12:09<06:19, 34.51s/it] 41%|████      | 13/32 [12:10<18:14, 57.59s/it] 66%|██████▌   | 21/32 [12:17<06:19, 34.53s/it] 56%|█████▋    | 18/32 [12:30<10:33, 45.28s/it] 69%|██████▉   | 22/32 [12:36<05:40, 34.02s/it] 69%|██████▉   | 22/32 [12:41<05:39, 33.90s/it] 69%|██████▉   | 22/32 [12:41<05:43, 34.38s/it] 69%|██████▉   | 22/32 [12:43<05:42, 34.25s/it] 69%|██████▉   | 22/32 [12:51<05:45, 34.59s/it] 44%|████▍     | 14/32 [12:58<16:41, 55.62s/it] 44%|████▍     | 14/32 [13:04<16:53, 56.29s/it] 72%|███████▏  | 23/32 [13:10<05:04, 33.83s/it] 72%|███████▏  | 23/32 [13:14<05:02, 33.66s/it] 59%|█████▉    | 19/32 [13:14<09:44, 44.97s/it] 72%|███████▏  | 23/32 [13:15<05:07, 34.13s/it] 72%|███████▏  | 23/32 [13:18<05:09, 34.37s/it] 72%|███████▏  | 23/32 [13:28<05:17, 35.25s/it] 75%|███████▌  | 24/32 [13:43<04:29, 33.67s/it] 78%|███████▊  | 25/32 [13:48<03:01, 25.97s/it] 75%|███████▌  | 24/32 [13:50<04:36, 34.58s/it] 47%|████▋     | 15/32 [13:53<15:42, 55.44s/it] 75%|███████▌  | 24/32 [13:54<04:40, 35.10s/it] 47%|████▋     | 15/32 [14:00<15:57, 56.35s/it] 62%|██████▎   | 20/32 [14:01<09:07, 45.60s/it] 75%|███████▌  | 24/32 [14:02<04:38, 34.78s/it] 78%|███████▊  | 25/32 [14:17<03:57, 33.88s/it] 81%|████████▏ | 26/32 [14:21<02:46, 27.73s/it] 78%|███████▊  | 25/32 [14:26<04:04, 34.88s/it] 78%|███████▊  | 25/32 [14:29<04:04, 34.95s/it] 78%|███████▊  | 25/32 [14:35<04:00, 34.42s/it] 50%|█████     | 16/32 [14:49<14:53, 55.84s/it] 66%|██████▌   | 21/32 [14:53<08:43, 47.59s/it] 81%|████████▏ | 26/32 [14:54<03:28, 34.71s/it] 84%|████████▍ | 27/32 [14:55<02:27, 29.42s/it] 50%|█████     | 16/32 [14:55<14:55, 55.94s/it] 81%|████████▏ | 26/32 [15:01<03:30, 35.09s/it] 81%|████████▏ | 26/32 [15:03<03:27, 34.57s/it] 81%|████████▏ | 26/32 [15:09<03:25, 34.21s/it] 88%|████████▊ | 28/32 [15:29<02:02, 30.66s/it] 84%|████████▍ | 27/32 [15:31<02:57, 35.48s/it] 84%|████████▍ | 27/32 [15:36<02:54, 34.90s/it] 84%|████████▍ | 27/32 [15:38<02:54, 34.89s/it] 69%|██████▉   | 22/32 [15:40<07:53, 47.38s/it] 84%|████████▍ | 27/32 [15:45<02:53, 34.66s/it] 53%|█████▎    | 17/32 [15:46<14:01, 56.09s/it] 53%|█████▎    | 17/32 [15:53<14:09, 56.62s/it] 88%|████████▊ | 28/32 [16:05<02:19, 34.82s/it] 91%|█████████ | 29/32 [16:06<01:36, 32.21s/it] 88%|████████▊ | 28/32 [16:11<02:20, 35.08s/it] 88%|████████▊ | 28/32 [16:13<02:19, 34.84s/it] 88%|████████▊ | 28/32 [16:20<02:18, 34.74s/it] 72%|███████▏  | 23/32 [16:25<06:58, 46.48s/it] 94%|█████████▍| 30/32 [16:40<00:53, 26.89s/it] 94%|█████████▍| 30/32 [16:41<01:06, 33.03s/it] 56%|█████▋    | 18/32 [16:41<13:01, 55.80s/it] 91%|█████████ | 29/32 [16:46<01:44, 34.91s/it] 91%|█████████ | 29/32 [16:47<01:43, 34.50s/it] 91%|█████████ | 29/32 [16:55<01:44, 34.81s/it] 56%|█████▋    | 18/32 [16:55<13:34, 58.17s/it] 75%|███████▌  | 24/32 [17:08<06:03, 45.46s/it] 97%|█████████▋| 31/32 [17:14<00:33, 33.02s/it] 97%|█████████▋| 31/32 [17:15<00:29, 29.03s/it] 94%|█████████▍| 30/32 [17:20<01:09, 34.52s/it] 94%|█████████▍| 30/32 [17:22<01:09, 34.59s/it] 94%|█████████▍| 30/32 [17:31<01:10, 35.13s/it] 59%|█████▉    | 19/32 [17:35<11:56, 55.09s/it]100%|██████████| 32/32 [17:48<00:00, 33.31s/it]100%|██████████| 32/32 [17:48<00:00, 33.38s/it]
 59%|█████▉    | 19/32 [17:49<12:17, 56.77s/it]100%|██████████| 32/32 [17:50<00:00, 30.40s/it]100%|██████████| 32/32 [17:50<00:00, 33.44s/it]
 78%|███████▊  | 25/32 [17:51<05:12, 44.69s/it] 97%|█████████▋| 31/32 [17:55<00:34, 34.85s/it] 97%|█████████▋| 31/32 [17:56<00:34, 34.66s/it] 97%|█████████▋| 31/32 [18:05<00:35, 35.01s/it] 62%|██████▎   | 20/32 [18:30<11:01, 55.10s/it]100%|██████████| 32/32 [18:30<00:00, 34.33s/it]100%|██████████| 32/32 [18:30<00:00, 34.70s/it]
100%|██████████| 32/32 [18:32<00:00, 35.40s/it]100%|██████████| 32/32 [18:32<00:00, 34.76s/it]
 81%|████████▏ | 26/32 [18:33<04:24, 44.10s/it]100%|██████████| 32/32 [18:40<00:00, 34.97s/it]100%|██████████| 32/32 [18:40<00:00, 35.02s/it]
 62%|██████▎   | 20/32 [18:42<11:09, 55.80s/it] 84%|████████▍ | 27/32 [19:18<03:42, 44.43s/it] 66%|██████▌   | 21/32 [19:25<10:06, 55.11s/it] 66%|██████▌   | 21/32 [19:37<10:10, 55.54s/it] 88%|████████▊ | 28/32 [20:01<02:55, 43.86s/it] 69%|██████▉   | 22/32 [20:22<09:15, 55.58s/it] 69%|██████▉   | 22/32 [20:34<09:18, 55.84s/it] 91%|█████████ | 29/32 [20:45<02:11, 43.82s/it] 72%|███████▏  | 23/32 [21:15<08:14, 54.96s/it] 94%|█████████▍| 30/32 [21:27<01:26, 43.41s/it] 72%|███████▏  | 23/32 [21:28<08:19, 55.54s/it] 75%|███████▌  | 24/32 [22:08<07:15, 54.45s/it] 97%|█████████▋| 31/32 [22:14<00:44, 44.34s/it] 75%|███████▌  | 24/32 [22:22<07:18, 54.87s/it]100%|██████████| 32/32 [23:00<00:00, 44.98s/it]100%|██████████| 32/32 [23:00<00:00, 43.15s/it]
 78%|███████▊  | 25/32 [23:03<06:22, 54.65s/it] 78%|███████▊  | 25/32 [23:15<06:20, 54.39s/it] 81%|████████▏ | 26/32 [24:02<05:34, 55.77s/it] 81%|████████▏ | 26/32 [24:10<05:27, 54.56s/it] 84%|████████▍ | 27/32 [24:57<04:37, 55.52s/it] 84%|████████▍ | 27/32 [25:07<04:35, 55.15s/it] 88%|████████▊ | 28/32 [25:53<03:43, 55.86s/it] 88%|████████▊ | 28/32 [26:02<03:40, 55.09s/it] 91%|█████████ | 29/32 [26:50<02:48, 56.10s/it] 91%|█████████ | 29/32 [26:55<02:43, 54.58s/it] 94%|█████████▍| 30/32 [27:44<01:50, 55.29s/it][rank6]:[E ProcessGroupNCCL.cpp:523] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600322 milliseconds before timing out.
 94%|█████████▍| 30/32 [27:50<01:49, 54.63s/it][rank5]:[E ProcessGroupNCCL.cpp:523] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
[rank6]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E ProcessGroupNCCL.cpp:1182] [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600322 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f030f7efd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f03109976e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f031099ac3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f031099b839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f035a6aedf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f035ba9d609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f035b868353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600322 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f030f7efd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f03109976e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f031099ac3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f031099b839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f035a6aedf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f035ba9d609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f035b868353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f030f7efd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f03106f1b11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f035a6aedf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f035ba9d609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f035b868353 in /lib/x86_64-linux-gnu/libc.so.6)

[rank5]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E ProcessGroupNCCL.cpp:1182] [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f1c6e5a6d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f1c6f74e6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f1c6f751c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f1c6f752839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f1cb9465df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f1cba854609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f1cba61f353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f1c6e5a6d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f1c6f74e6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f1c6f751c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f1c6f752839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f1cb9465df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f1cba854609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f1cba61f353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f1c6e5a6d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f1c6f4a8b11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f1cb9465df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f1cba854609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f1cba61f353 in /lib/x86_64-linux-gnu/libc.so.6)

[2024-07-08 22:01:07,443] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2565821 closing signal SIGTERM
[2024-07-08 22:01:07,444] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2565824 closing signal SIGTERM
[2024-07-08 22:01:07,445] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2565825 closing signal SIGTERM
[2024-07-08 22:01:07,445] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2565826 closing signal SIGTERM
[2024-07-08 22:01:07,445] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2565830 closing signal SIGTERM
[2024-07-08 22:01:07,445] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2565832 closing signal SIGTERM
[2024-07-08 22:01:07,446] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2565833 closing signal SIGTERM
[2024-07-08 22:01:34,299] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 5 (pid: 2565831) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
main.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-08_22:01:07
  host      : a100-st-p4de24xlarge-771.fair-a100.hpcaas
  rank      : 5 (local_rank: 5)
  exitcode  : -6 (pid: 2565831)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2565831
========================================================
