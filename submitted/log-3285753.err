Switched to branch 'yangexp2two'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-06:01:25:00,371 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:25:00,371 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:25:00,372 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:25:00,372 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:25:00,372 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:25:00,372 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:25:00,372 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:25:00,374 INFO     [main.py:288] Verbosity set to INFO
2024-07-06:01:25:09,955 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:25:09,955 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:25:09,955 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:25:09,955 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:25:09,955 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:25:09,965 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:25:09,970 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:25:09,971 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-06:01:25:09,985 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:25:09,985 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:25:09,985 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:25:09,985 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:25:09,985 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:25:09,985 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:25:09,985 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:25:09,985 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 8, 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True, 'filteractiveenabled': True}
2024-07-06:01:25:09,985 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 8, 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True, 'filteractiveenabled': True}
2024-07-06:01:25:09,985 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 8, 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True, 'filteractiveenabled': True}
2024-07-06:01:25:09,985 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 8, 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True, 'filteractiveenabled': True}
2024-07-06:01:25:09,985 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 8, 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True, 'filteractiveenabled': True}
2024-07-06:01:25:09,985 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 8, 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True, 'filteractiveenabled': True}
2024-07-06:01:25:09,985 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 8, 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True, 'filteractiveenabled': True}
2024-07-06:01:25:09,985 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-06:01:25:09,985 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 8, 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True, 'filteractiveenabled': True}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:04, 21.34s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.29s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.16s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:04, 21.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.28s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:04, 21.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:39<00:39, 19.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:40<00:40, 20.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:40<00:40, 20.27s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:40<00:40, 20.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:41<00:41, 20.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:40<00:40, 20.27s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:41<00:40, 20.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:41<00:40, 20.35s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:19, 19.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:19, 19.39s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:19, 19.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:00<00:19, 19.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:19, 19.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:19, 19.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:19, 19.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:19, 19.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.15s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.19s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.18s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 12.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.24s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 12.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 15.41s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 12.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 15.29s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-06:01:26:59,975 INFO     [xhuggingface.py:323] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-06:01:27:01,242 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
2024-07-06:01:27:01,242 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
2024-07-06:01:27:01,276 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/164 [00:00<?, ?it/s]  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.33it/s] 12%|█▏        | 20/165 [00:00<00:00, 192.36it/s] 12%|█▏        | 20/164 [00:00<00:00, 190.88it/s]2024-07-06:01:27:01,486 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
 23%|██▎       | 38/165 [00:00<00:00, 189.67it/s]  0%|          | 0/165 [00:00<?, ?it/s] 24%|██▍       | 40/165 [00:00<00:00, 194.07it/s] 24%|██▍       | 40/164 [00:00<00:00, 192.89it/s]2024-07-06:01:27:01,534 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.38it/s] 35%|███▌      | 58/165 [00:00<00:00, 190.64it/s] 36%|███▋      | 60/165 [00:00<00:00, 194.58it/s] 37%|███▋      | 60/164 [00:00<00:00, 193.39it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.87it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.25it/s] 47%|████▋     | 78/165 [00:00<00:00, 191.71it/s] 48%|████▊     | 80/165 [00:00<00:00, 195.46it/s] 49%|████▉     | 80/164 [00:00<00:00, 194.11it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.64it/s]2024-07-06:01:27:01,764 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/165 [00:00<?, ?it/s] 36%|███▌      | 59/165 [00:00<00:00, 191.61it/s] 59%|█████▉    | 98/165 [00:00<00:00, 192.56it/s] 61%|██████    | 100/165 [00:00<00:00, 196.04it/s] 61%|██████    | 100/164 [00:00<00:00, 194.64it/s] 36%|███▌      | 59/165 [00:00<00:00, 192.22it/s] 12%|█▏        | 19/165 [00:00<00:00, 186.94it/s] 73%|███████▎  | 120/165 [00:00<00:00, 196.44it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.34it/s] 72%|███████▏  | 118/165 [00:00<00:00, 192.87it/s] 73%|███████▎  | 120/164 [00:00<00:00, 194.96it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.87it/s] 24%|██▎       | 39/165 [00:00<00:00, 188.97it/s] 85%|████████▍ | 140/165 [00:00<00:00, 196.65it/s] 60%|██████    | 99/165 [00:00<00:00, 192.89it/s] 84%|████████▎ | 138/165 [00:00<00:00, 193.13it/s] 85%|████████▌ | 140/164 [00:00<00:00, 195.13it/s] 60%|██████    | 99/165 [00:00<00:00, 193.51it/s] 36%|███▌      | 59/165 [00:00<00:00, 189.59it/s] 97%|█████████▋| 160/165 [00:00<00:00, 196.69it/s] 72%|███████▏  | 119/165 [00:00<00:00, 193.24it/s] 96%|█████████▌| 158/165 [00:00<00:00, 193.11it/s] 98%|█████████▊| 160/164 [00:00<00:00, 195.14it/s]100%|██████████| 165/165 [00:00<00:00, 195.96it/s]
100%|██████████| 164/164 [00:00<00:00, 194.50it/s]
100%|██████████| 165/165 [00:00<00:00, 192.32it/s]
 72%|███████▏  | 119/165 [00:00<00:00, 193.90it/s] 48%|████▊     | 79/165 [00:00<00:00, 190.38it/s] 84%|████████▍ | 139/165 [00:00<00:00, 193.52it/s] 84%|████████▍ | 139/165 [00:00<00:00, 194.07it/s] 60%|██████    | 99/165 [00:00<00:00, 191.14it/s] 96%|█████████▋| 159/165 [00:00<00:00, 193.54it/s]100%|██████████| 165/165 [00:00<00:00, 192.84it/s]
 96%|█████████▋| 159/165 [00:00<00:00, 194.19it/s]100%|██████████| 165/165 [00:00<00:00, 193.43it/s]
 72%|███████▏  | 119/165 [00:00<00:00, 191.24it/s] 84%|████████▍ | 139/165 [00:00<00:00, 190.91it/s] 96%|█████████▋| 159/165 [00:00<00:00, 190.89it/s]100%|██████████| 165/165 [00:00<00:00, 190.42it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-06:01:27:21,885 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/165 [00:00<?, ?it/s]  7%|▋         | 12/165 [00:00<00:01, 114.71it/s]2024-07-06:01:27:22,099 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
 15%|█▍        | 24/165 [00:00<00:01, 116.31it/s]  0%|          | 0/165 [00:00<?, ?it/s] 22%|██▏       | 36/165 [00:00<00:01, 117.26it/s]  7%|▋         | 12/165 [00:00<00:01, 115.05it/s] 29%|██▉       | 48/165 [00:00<00:00, 118.13it/s] 15%|█▍        | 24/165 [00:00<00:01, 113.44it/s] 36%|███▋      | 60/165 [00:00<00:00, 118.19it/s] 22%|██▏       | 36/165 [00:00<00:01, 115.63it/s] 44%|████▎     | 72/165 [00:00<00:00, 116.24it/s] 29%|██▉       | 48/165 [00:00<00:01, 116.31it/s] 51%|█████     | 84/165 [00:00<00:00, 116.69it/s] 36%|███▋      | 60/165 [00:00<00:00, 116.95it/s] 58%|█████▊    | 96/165 [00:00<00:00, 117.33it/s] 44%|████▎     | 72/165 [00:00<00:00, 117.18it/s] 65%|██████▌   | 108/165 [00:00<00:00, 117.89it/s] 51%|█████     | 84/165 [00:00<00:00, 117.42it/s] 73%|███████▎  | 120/165 [00:01<00:00, 118.20it/s] 58%|█████▊    | 96/165 [00:00<00:00, 117.87it/s] 80%|████████  | 132/165 [00:01<00:00, 118.47it/s] 65%|██████▌   | 108/165 [00:00<00:00, 118.24it/s] 87%|████████▋ | 144/165 [00:01<00:00, 118.65it/s] 73%|███████▎  | 120/165 [00:01<00:00, 118.51it/s] 95%|█████████▍| 156/165 [00:01<00:00, 118.76it/s] 80%|████████  | 132/165 [00:01<00:00, 118.64it/s]100%|██████████| 165/165 [00:01<00:00, 117.89it/s]
 87%|████████▋ | 144/165 [00:01<00:00, 118.91it/s] 95%|█████████▍| 156/165 [00:01<00:00, 117.89it/s]100%|██████████| 165/165 [00:01<00:00, 118.29it/s]
2024-07-06:01:27:38,889 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:27:38,889 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:27:38,889 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:27:38,889 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:27:38,889 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:27:38,890 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:27:38,889 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-06:01:27:38,890 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/165 [00:11<32:05, 11.74s/it]Running generate_until requests:   1%|          | 2/165 [00:22<29:47, 10.97s/it]Running generate_until requests:   1%|          | 2/165 [00:30<41:52, 15.41s/it]
