Your branch is up to date with 'origin/yangexp2threee'.
Updating 64dbe5c..27e7195
Fast-forward
 llama12_static_cache_sdpa.py | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)
Already up to date.
/fsx-storygen/beidic/anaconda3/envs/griffin/bin/python
/fsx-storygen/beidic/anaconda3/envs/griffin/bin/python
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /data/home/beidic/.cache/huggingface/token
Login successful
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=False,check=False,contextlength=128,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 28])
warming up
time taken for a single forward pass 0.013965157747268676
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.01538719654083252
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=True,check=False,contextlength=128,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 28])
warming up
time taken for a single forward pass 0.010204143285751343
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.015391159057617187
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=False,check=False,contextlength=256,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 156])
warming up
time taken for a single forward pass 0.014395196676254272
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.01577864646911621
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=True,check=False,contextlength=256,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 156])
warming up
time taken for a single forward pass 0.010631580114364624
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.015769383907318114
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=False,check=False,contextlength=512,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 412])
warming up
time taken for a single forward pass 0.015070427894592285
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.01650914430618286
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=True,check=False,contextlength=512,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 412])
warming up
time taken for a single forward pass 0.011311066627502442
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.016494388580322265
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=False,check=False,contextlength=1024,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.01651866030693054
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.01793520212173462
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=True,check=False,contextlength=1024,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.012771092176437378
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.017937729358673094
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=False,check=False,contextlength=1500,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.017951508045196532
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.01949878215789795
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=True,check=False,contextlength=1500,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.014204017877578735
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.019491105079650878
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=False,check=False,contextlength=2048,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.019172683477401732
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.02070683240890503
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=True,check=False,contextlength=2048,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.01542056703567505
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.020713074207305907
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=False,check=False,contextlength=3072,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.02182822608947754
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.02335412263870239
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=True,check=False,contextlength=3072,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.01807051706314087
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.023348772525787355
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=False,check=False,contextlength=4096,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.024378305435180665
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.02598294973373413
Namespace(model='xhf', tasks='gsm8k_cot', model_args='pretrained=meta-llama/Meta-Llama-3-8B-Instruct,griffin=True,check=False,contextlength=4096,kernel_size=10,thr=0.05,attentionimplementation=sdpa', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=0.3, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
SDPA
input_ids shape torch.Size([1, 789])
warming up
time taken for a single forward pass 0.020624980688095092
executinginputidsfull shape torch.Size([1, 10])
warming up full
full time taken for a single forward pass 0.025982985496520995
