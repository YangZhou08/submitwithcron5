Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:49, 16.62s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:55, 18.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:52, 17.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:53, 17.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:30<00:28, 14.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:31<00:30, 15.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:31<00:30, 15.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:31<00:30, 15.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:32<00:31, 15.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:31<00:31, 15.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:31<00:30, 15.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:31<00:30, 15.25s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:43<00:13, 13.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:45<00:14, 14.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00,  9.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00, 11.30s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00,  9.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00, 11.55s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00,  9.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00, 11.57s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00,  9.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00, 11.60s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00,  9.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00, 11.70s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00,  9.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00, 11.60s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00,  9.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00, 11.53s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00,  9.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00, 11.53s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:32<16:34, 32.07s/it]  3%|▎         | 1/32 [00:32<16:51, 32.63s/it]  3%|▎         | 1/32 [00:33<17:29, 33.84s/it]  3%|▎         | 1/32 [00:34<17:42, 34.26s/it]  3%|▎         | 1/32 [00:34<17:35, 34.06s/it]  3%|▎         | 1/32 [00:35<18:33, 35.92s/it]  3%|▎         | 1/32 [00:48<25:15, 48.90s/it]  3%|▎         | 1/32 [00:53<27:42, 53.62s/it]  6%|▋         | 2/32 [01:03<15:55, 31.85s/it]  6%|▋         | 2/32 [01:06<16:47, 33.57s/it]  6%|▋         | 2/32 [01:06<16:29, 32.99s/it]  6%|▋         | 2/32 [01:07<16:47, 33.58s/it]  6%|▋         | 2/32 [01:08<17:03, 34.12s/it]  6%|▋         | 2/32 [01:14<18:58, 37.96s/it]  9%|▉         | 3/32 [01:36<15:29, 32.06s/it]  9%|▉         | 3/32 [01:37<15:37, 32.32s/it]  9%|▉         | 3/32 [01:38<15:35, 32.25s/it]  9%|▉         | 3/32 [01:37<15:35, 32.25s/it]  9%|▉         | 3/32 [01:39<15:54, 32.91s/it]  6%|▋         | 2/32 [01:40<25:15, 50.50s/it]  6%|▋         | 2/32 [01:43<25:39, 51.32s/it]  9%|▉         | 3/32 [01:54<18:49, 38.97s/it] 12%|█▎        | 4/32 [02:10<15:08, 32.46s/it] 12%|█▎        | 4/32 [02:11<15:32, 33.30s/it] 12%|█▎        | 4/32 [02:12<15:19, 32.84s/it] 12%|█▎        | 4/32 [02:13<15:45, 33.77s/it] 12%|█▎        | 4/32 [02:14<15:42, 33.66s/it] 12%|█▎        | 4/32 [02:33<18:12, 39.01s/it]  9%|▉         | 3/32 [02:33<24:59, 51.71s/it]  9%|▉         | 3/32 [02:40<26:10, 54.15s/it] 16%|█▌        | 5/32 [02:42<14:39, 32.57s/it] 16%|█▌        | 5/32 [02:43<14:43, 32.72s/it] 16%|█▌        | 5/32 [02:44<14:37, 32.50s/it] 16%|█▌        | 5/32 [02:45<14:47, 32.87s/it] 16%|█▌        | 5/32 [02:45<14:45, 32.80s/it] 19%|█▉        | 6/32 [03:12<13:42, 31.64s/it] 19%|█▉        | 6/32 [03:14<13:43, 31.66s/it] 16%|█▌        | 5/32 [03:15<17:56, 39.86s/it] 19%|█▉        | 6/32 [03:16<14:18, 33.01s/it] 19%|█▉        | 6/32 [03:16<13:57, 32.21s/it] 19%|█▉        | 6/32 [03:17<14:09, 32.66s/it] 12%|█▎        | 4/32 [03:29<24:58, 53.53s/it] 12%|█▎        | 4/32 [03:39<26:08, 56.01s/it] 22%|██▏       | 7/32 [03:42<12:56, 31.06s/it] 22%|██▏       | 7/32 [03:45<13:06, 31.46s/it] 22%|██▏       | 7/32 [03:48<13:24, 32.19s/it] 22%|██▏       | 7/32 [03:48<13:24, 32.18s/it] 22%|██▏       | 7/32 [03:51<13:57, 33.50s/it] 19%|█▉        | 6/32 [03:55<17:19, 39.98s/it] 25%|██▌       | 8/32 [04:15<12:25, 31.07s/it] 25%|██▌       | 8/32 [04:17<12:53, 32.21s/it] 25%|██▌       | 8/32 [04:21<12:54, 32.25s/it] 16%|█▌        | 5/32 [04:21<23:45, 52.81s/it] 25%|██▌       | 8/32 [04:22<13:04, 32.69s/it] 25%|██▌       | 8/32 [04:21<12:57, 32.39s/it] 16%|█▌        | 5/32 [04:33<24:55, 55.38s/it] 22%|██▏       | 7/32 [04:36<16:51, 40.45s/it] 28%|██▊       | 9/32 [04:46<11:53, 31.04s/it] 28%|██▊       | 9/32 [04:48<12:10, 31.78s/it] 28%|██▊       | 9/32 [04:52<12:14, 31.93s/it] 28%|██▊       | 9/32 [04:52<12:13, 31.87s/it] 28%|██▊       | 9/32 [04:54<12:31, 32.67s/it] 19%|█▉        | 6/32 [05:14<22:55, 52.92s/it] 31%|███▏      | 10/32 [05:16<11:15, 30.71s/it] 31%|███▏      | 10/32 [05:17<11:25, 31.17s/it] 31%|███▏      | 10/32 [05:22<11:30, 31.41s/it] 31%|███▏      | 10/32 [05:22<11:33, 31.51s/it] 19%|█▉        | 6/32 [05:25<23:24, 54.02s/it] 25%|██▌       | 8/32 [05:25<17:15, 43.14s/it] 31%|███▏      | 10/32 [05:31<12:24, 33.86s/it] 34%|███▍      | 11/32 [05:48<10:53, 31.10s/it] 34%|███▍      | 11/32 [05:49<10:59, 31.40s/it] 34%|███▍      | 11/32 [05:53<10:58, 31.35s/it] 34%|███▍      | 11/32 [05:57<11:21, 32.46s/it] 34%|███▍      | 11/32 [06:02<11:36, 33.18s/it] 28%|██▊       | 9/32 [06:04<16:01, 41.82s/it] 22%|██▏       | 7/32 [06:04<21:39, 51.98s/it] 22%|██▏       | 7/32 [06:16<22:08, 53.12s/it] 38%|███▊      | 12/32 [06:18<10:14, 30.74s/it] 38%|███▊      | 12/32 [06:19<10:19, 30.99s/it] 38%|███▊      | 12/32 [06:24<10:25, 31.29s/it] 38%|███▊      | 12/32 [06:28<10:38, 31.92s/it] 38%|███▊      | 12/32 [06:35<11:00, 33.03s/it] 31%|███▏      | 10/32 [06:44<15:09, 41.33s/it] 41%|████      | 13/32 [06:49<09:44, 30.78s/it] 41%|████      | 13/32 [06:51<09:54, 31.30s/it] 25%|██▌       | 8/32 [06:56<20:45, 51.89s/it] 41%|████      | 13/32 [06:56<09:53, 31.25s/it] 41%|████      | 13/32 [06:59<10:04, 31.84s/it] 41%|████      | 13/32 [07:07<10:19, 32.62s/it] 25%|██▌       | 8/32 [07:11<21:26, 53.59s/it] 44%|████▍     | 14/32 [07:22<09:22, 31.23s/it] 44%|████▍     | 14/32 [07:23<09:29, 31.66s/it] 34%|███▍      | 11/32 [07:26<14:27, 41.33s/it] 44%|████▍     | 14/32 [07:27<09:22, 31.27s/it] 44%|████▍     | 14/32 [07:30<09:28, 31.59s/it] 44%|████▍     | 14/32 [07:38<09:38, 32.13s/it] 28%|██▊       | 9/32 [07:48<19:54, 51.92s/it] 47%|████▋     | 15/32 [07:53<08:46, 30.95s/it] 47%|████▋     | 15/32 [07:56<09:03, 32.00s/it] 47%|████▋     | 15/32 [07:59<08:56, 31.53s/it] 28%|██▊       | 9/32 [08:02<20:15, 52.86s/it] 38%|███▊      | 12/32 [08:06<13:39, 40.99s/it] 47%|████▋     | 15/32 [08:05<09:12, 32.51s/it] 47%|████▋     | 15/32 [08:11<09:10, 32.39s/it] 50%|█████     | 16/32 [08:24<08:16, 31.05s/it] 50%|█████     | 16/32 [08:28<08:32, 32.05s/it] 50%|█████     | 16/32 [08:30<08:19, 31.19s/it] 50%|█████     | 16/32 [08:38<08:42, 32.67s/it] 31%|███▏      | 10/32 [08:43<19:22, 52.84s/it] 50%|█████     | 16/32 [08:46<08:50, 33.16s/it] 41%|████      | 13/32 [08:46<12:54, 40.75s/it] 31%|███▏      | 10/32 [08:55<19:23, 52.91s/it] 53%|█████▎    | 17/32 [08:55<07:46, 31.13s/it] 53%|█████▎    | 17/32 [08:59<07:58, 31.91s/it] 53%|█████▎    | 17/32 [09:02<07:51, 31.45s/it] 53%|█████▎    | 17/32 [09:11<08:13, 32.91s/it] 53%|█████▎    | 17/32 [09:16<08:05, 32.39s/it] 56%|█████▋    | 18/32 [09:28<07:23, 31.69s/it] 44%|████▍     | 14/32 [09:30<12:29, 41.64s/it] 56%|█████▋    | 18/32 [09:31<07:26, 31.86s/it] 56%|█████▋    | 18/32 [09:33<07:20, 31.44s/it] 34%|███▍      | 11/32 [09:36<18:31, 52.91s/it] 56%|█████▋    | 18/32 [09:44<07:39, 32.82s/it] 56%|█████▋    | 18/32 [09:47<07:26, 31.91s/it] 34%|███▍      | 11/32 [09:52<18:59, 54.27s/it] 59%|█████▉    | 19/32 [10:01<06:56, 32.07s/it] 59%|█████▉    | 19/32 [10:02<06:49, 31.50s/it] 59%|█████▉    | 19/32 [10:03<06:44, 31.10s/it] 47%|████▋     | 15/32 [10:10<11:40, 41.22s/it] 59%|█████▉    | 19/32 [10:18<06:50, 31.58s/it] 59%|█████▉    | 19/32 [10:19<07:13, 33.34s/it] 62%|██████▎   | 20/32 [10:32<06:12, 31.05s/it] 38%|███▊      | 12/32 [10:32<17:59, 53.96s/it] 62%|██████▎   | 20/32 [10:33<06:25, 32.11s/it] 62%|██████▎   | 20/32 [10:34<06:10, 30.88s/it] 38%|███▊      | 12/32 [10:44<17:47, 53.38s/it]