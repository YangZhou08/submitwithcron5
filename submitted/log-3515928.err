Already on 'yangexp2two'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-24:08:44:00,553 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:44:00,553 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:44:00,554 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:44:00,554 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:44:00,554 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:44:00,554 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:44:00,555 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:44:00,556 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:44:09,978 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:44:09,978 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:44:09,978 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:44:09,978 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:44:09,978 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:44:09,978 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:44:09,978 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:44:09,979 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:44:10,003 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:44:10,003 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:44:10,003 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:44:10,003 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:44:10,003 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:44:10,003 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:44:10,003 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:44:10,003 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:44:10,003 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:44:10,003 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:44:10,003 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:44:10,003 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:44:10,003 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:44:10,003 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:44:10,003 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:44:10,003 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:43, 14.35s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:44, 14.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:44, 14.97s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:44, 14.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:44, 14.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:44, 14.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:47, 15.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:45, 15.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:28<00:28, 14.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:29, 14.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:29, 14.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:30<00:30, 15.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:29, 14.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:29, 14.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:29, 14.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:30<00:30, 15.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:41<00:13, 13.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:43<00:14, 14.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:13, 13.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:13, 13.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:13, 13.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:13, 13.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:13, 13.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:14, 14.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00,  9.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00, 10.89s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:44<00:00,  8.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:44<00:00, 11.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00,  8.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00, 10.94s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00,  8.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00, 10.91s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00,  8.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00, 10.91s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00,  8.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00, 10.94s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00,  8.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00, 10.93s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00,  8.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00, 10.96s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-24:08:45:37,862 INFO     [xhuggingface.py:323] Using 8 devices with data parallelism
2024-07-24:08:45:38,672 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
2024-07-24:08:45:38,672 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
2024-07-24:08:45:38,706 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 20/165 [00:00<00:00, 191.43it/s] 12%|█▏        | 20/165 [00:00<00:00, 190.44it/s] 12%|█▏        | 20/165 [00:00<00:00, 190.27it/s] 24%|██▍       | 40/165 [00:00<00:00, 192.44it/s] 24%|██▍       | 40/165 [00:00<00:00, 191.59it/s] 24%|██▍       | 40/165 [00:00<00:00, 191.45it/s]2024-07-24:08:45:39,011 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/165 [00:00<?, ?it/s] 36%|███▋      | 60/165 [00:00<00:00, 192.86it/s] 36%|███▋      | 60/165 [00:00<00:00, 191.63it/s] 36%|███▋      | 60/165 [00:00<00:00, 190.38it/s] 12%|█▏        | 20/165 [00:00<00:00, 191.17it/s] 48%|████▊     | 80/165 [00:00<00:00, 192.36it/s] 48%|████▊     | 80/165 [00:00<00:00, 191.46it/s] 48%|████▊     | 80/165 [00:00<00:00, 191.27it/s] 24%|██▍       | 40/165 [00:00<00:00, 192.36it/s] 61%|██████    | 100/165 [00:00<00:00, 193.00it/s] 61%|██████    | 100/165 [00:00<00:00, 192.41it/s] 61%|██████    | 100/165 [00:00<00:00, 192.30it/s] 36%|███▋      | 60/165 [00:00<00:00, 192.33it/s] 73%|███████▎  | 120/165 [00:00<00:00, 193.19it/s] 73%|███████▎  | 120/165 [00:00<00:00, 193.17it/s] 73%|███████▎  | 120/165 [00:00<00:00, 192.87it/s]2024-07-24:08:45:39,428 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
 48%|████▊     | 80/165 [00:00<00:00, 193.19it/s]  0%|          | 0/165 [00:00<?, ?it/s] 85%|████████▍ | 140/165 [00:00<00:00, 193.27it/s] 85%|████████▍ | 140/165 [00:00<00:00, 193.35it/s] 85%|████████▍ | 140/165 [00:00<00:00, 193.25it/s]2024-07-24:08:45:39,500 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/164 [00:00<?, ?it/s] 61%|██████    | 100/165 [00:00<00:00, 193.87it/s] 12%|█▏        | 20/165 [00:00<00:00, 192.58it/s] 97%|█████████▋| 160/165 [00:00<00:00, 193.79it/s] 97%|█████████▋| 160/165 [00:00<00:00, 193.48it/s] 97%|█████████▋| 160/165 [00:00<00:00, 193.54it/s]100%|██████████| 165/165 [00:00<00:00, 193.08it/s]
100%|██████████| 165/165 [00:00<00:00, 192.87it/s]
100%|██████████| 165/165 [00:00<00:00, 192.58it/s]
 12%|█▏        | 20/164 [00:00<00:00, 190.54it/s] 73%|███████▎  | 120/165 [00:00<00:00, 194.26it/s] 24%|██▍       | 40/165 [00:00<00:00, 193.80it/s] 24%|██▍       | 40/164 [00:00<00:00, 191.82it/s] 85%|████████▍ | 140/165 [00:00<00:00, 194.51it/s] 36%|███▋      | 60/165 [00:00<00:00, 194.05it/s] 37%|███▋      | 60/164 [00:00<00:00, 192.53it/s] 97%|█████████▋| 160/165 [00:00<00:00, 194.69it/s] 48%|████▊     | 80/165 [00:00<00:00, 195.05it/s]100%|██████████| 165/165 [00:00<00:00, 193.91it/s]
 49%|████▉     | 80/164 [00:00<00:00, 193.35it/s] 61%|██████    | 100/165 [00:00<00:00, 195.59it/s] 61%|██████    | 100/164 [00:00<00:00, 193.86it/s] 73%|███████▎  | 120/165 [00:00<00:00, 196.05it/s] 73%|███████▎  | 120/164 [00:00<00:00, 194.22it/s] 85%|████████▍ | 140/165 [00:00<00:00, 194.41it/s] 85%|████████▌ | 140/164 [00:00<00:00, 194.49it/s] 97%|█████████▋| 160/165 [00:00<00:00, 194.70it/s]100%|██████████| 165/165 [00:00<00:00, 194.74it/s]
 98%|█████████▊| 160/164 [00:00<00:00, 194.40it/s]100%|██████████| 164/164 [00:00<00:00, 193.68it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-24:08:46:00,875 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/165 [00:00<?, ?it/s]  7%|▋         | 12/165 [00:00<00:01, 115.09it/s]2024-07-24:08:46:01,122 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
 15%|█▍        | 24/165 [00:00<00:01, 115.18it/s]  0%|          | 0/165 [00:00<?, ?it/s] 22%|██▏       | 36/165 [00:00<00:01, 116.02it/s]  7%|▋         | 12/165 [00:00<00:01, 114.50it/s] 29%|██▉       | 48/165 [00:00<00:01, 116.65it/s] 15%|█▍        | 24/165 [00:00<00:01, 116.09it/s] 36%|███▋      | 60/165 [00:00<00:00, 117.07it/s] 22%|██▏       | 36/165 [00:00<00:01, 116.59it/s] 44%|████▎     | 72/165 [00:00<00:00, 117.57it/s] 29%|██▉       | 48/165 [00:00<00:01, 116.93it/s] 51%|█████     | 84/165 [00:00<00:00, 117.87it/s] 36%|███▋      | 60/165 [00:00<00:00, 117.25it/s] 58%|█████▊    | 96/165 [00:00<00:00, 112.93it/s] 44%|████▎     | 72/165 [00:00<00:00, 100.79it/s] 65%|██████▌   | 108/165 [00:01<00:00, 74.56it/s] 50%|█████     | 83/165 [00:01<00:01, 58.69it/s]  72%|███████▏  | 118/165 [00:01<00:00, 64.51it/s] 56%|█████▋    | 93/165 [00:01<00:01, 65.54it/s] 79%|███████▉  | 130/165 [00:01<00:00, 75.15it/s] 64%|██████▎   | 105/165 [00:01<00:00, 76.74it/s] 86%|████████▌ | 142/165 [00:01<00:00, 84.84it/s] 71%|███████   | 117/165 [00:01<00:00, 86.51it/s] 93%|█████████▎| 154/165 [00:01<00:00, 93.01it/s] 78%|███████▊  | 129/165 [00:01<00:00, 94.51it/s]100%|██████████| 165/165 [00:01<00:00, 95.72it/s]
 85%|████████▌ | 141/165 [00:01<00:00, 100.54it/s] 93%|█████████▎| 153/165 [00:01<00:00, 105.73it/s]100%|██████████| 165/165 [00:01<00:00, 108.20it/s]100%|██████████| 165/165 [00:01<00:00, 93.51it/s] 
2024-07-24:08:46:17,216 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:46:17,216 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:46:17,216 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:46:17,216 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:46:17,216 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:46:17,216 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:46:17,216 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]2024-07-24:08:46:17,218 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:12<?, ?it/s]
