Already on 'yangexppp'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:05<02:44,  5.68s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:11<02:36,  5.59s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:16<02:32,  5.64s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:22<02:27,  5.66s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:28<02:19,  5.57s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:33<02:11,  5.48s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:38<02:04,  5.43s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:44<02:01,  5.51s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:49<01:56,  5.55s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:55<01:49,  5.46s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [01:00<01:43,  5.42s/it]Loading checkpoint shards:  40%|████      | 12/30 [01:06<01:37,  5.43s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [01:11<01:33,  5.53s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [01:17<01:28,  5.54s/it]Loading checkpoint shards:  50%|█████     | 15/30 [01:22<01:21,  5.45s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [01:27<01:15,  5.39s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [01:33<01:09,  5.35s/it]Loading checkpoint shards:  60%|██████    | 18/30 [01:38<01:05,  5.42s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [01:44<00:59,  5.45s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [01:49<00:54,  5.40s/it]Loading checkpoint shards:  70%|███████   | 21/30 [01:54<00:48,  5.35s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [01:59<00:42,  5.31s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [02:05<00:37,  5.39s/it]Loading checkpoint shards:  80%|████████  | 24/30 [02:11<00:32,  5.48s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [02:16<00:27,  5.42s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [02:21<00:21,  5.37s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [02:26<00:15,  5.33s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [02:32<00:10,  5.42s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [02:38<00:05,  5.47s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:40<00:00,  4.58s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:40<00:00,  5.36s/it]
  0%|          | 0/300 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 1/300 [02:24<12:01:29, 144.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 2/300 [04:44<11:45:10, 141.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 3/300 [07:05<11:39:39, 141.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|▏         | 4/300 [09:22<11:28:42, 139.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 5/300 [11:42<11:27:19, 139.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 6/300 [14:00<11:21:38, 139.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 7/300 [16:20<11:21:06, 139.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 8/300 [18:38<11:16:19, 138.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 9/300 [20:55<11:11:53, 138.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 10/300 [23:12<11:06:29, 137.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  4%|▎         | 11/300 [25:29<11:02:25, 137.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  4%|▍         | 12/300 [27:47<11:00:59, 137.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  4%|▍         | 13/300 [30:05<10:59:51, 137.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▍         | 14/300 [32:28<11:04:16, 139.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▌         | 15/300 [34:45<10:58:56, 138.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▌         | 16/300 [37:07<11:00:35, 139.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  6%|▌         | 17/300 [39:26<10:57:48, 139.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  6%|▌         | 18/300 [41:48<10:59:48, 140.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  6%|▋         | 19/300 [44:10<10:58:42, 140.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 20/300 [46:29<10:55:12, 140.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 21/300 [48:53<10:56:52, 141.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 22/300 [51:13<10:53:24, 141.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  8%|▊         | 23/300 [53:38<10:55:59, 142.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  8%|▊         | 24/300 [56:02<10:56:28, 142.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  8%|▊         | 25/300 [58:21<10:48:28, 141.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▊         | 26/300 [1:00:43<10:47:03, 141.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▉         | 27/300 [1:03:00<10:39:03, 140.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▉         | 28/300 [1:05:19<10:34:40, 140.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|▉         | 29/300 [1:07:39<10:31:53, 139.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 30/300 [1:09:59<10:29:30, 139.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 31/300 [1:12:16<10:24:16, 139.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 11%|█         | 32/300 [1:14:41<10:29:19, 140.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 11%|█         | 33/300 [1:17:00<10:23:58, 140.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 11%|█▏        | 34/300 [1:19:18<10:18:49, 139.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 12%|█▏        | 35/300 [1:21:38<10:16:58, 139.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
