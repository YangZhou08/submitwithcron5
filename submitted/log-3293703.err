Already on 'yangexp2two'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-08:20:25:53,604 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:20:25:53,604 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:20:25:53,604 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:20:25:53,605 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:20:25:53,605 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:20:25:53,615 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:20:25:53,625 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:20:25:53,766 INFO     [main.py:288] Verbosity set to INFO
2024-07-08:20:26:02,963 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:20:26:02,963 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:20:26:02,963 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:20:26:02,963 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:20:26:02,963 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:20:26:02,963 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:20:26:02,963 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:20:26:02,964 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-08:20:26:02,989 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:20:26:02,989 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:20:26:02,989 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:20:26:02,989 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:20:26:02,989 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:20:26:02,989 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:20:26:02,989 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:20:26:02,990 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:20:26:02,990 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:20:26:02,990 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-08:20:26:02,990 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:20:26:02,990 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:20:26:02,990 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:20:26:02,990 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:20:26:02,990 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
2024-07-08:20:26:02,990 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'widthtree': 4, 'griffin': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'filteractiveenabled': True}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:46, 15.44s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:49, 16.55s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:50, 16.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:50, 16.70s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:50, 16.75s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:50, 16.77s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:52, 17.60s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:50, 16.89s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:27<00:26, 13.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:30<00:29, 14.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:28, 14.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:28, 14.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:28, 14.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:29, 14.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:28, 14.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:28, 14.39s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:39<00:12, 12.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:41<00:13, 13.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:41<00:13, 13.37s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:41<00:13, 13.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:41<00:13, 13.18s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:13, 13.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:41<00:13, 13.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.38s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  8.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.66s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  8.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.68s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  8.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.64s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  8.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.69s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  8.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.70s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00,  8.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:43<00:00, 10.91s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  8.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.74s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-08:20:27:35,275 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 184.29it/s] 23%|██▎       | 38/165 [00:00<00:00, 187.44it/s] 35%|███▍      | 57/165 [00:00<00:00, 187.73it/s] 47%|████▋     | 77/165 [00:00<00:00, 189.04it/s]2024-07-08:20:27:35,808 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/165 [00:00<?, ?it/s] 59%|█████▉    | 97/165 [00:00<00:00, 189.93it/s]2024-07-08:20:27:35,890 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 20/165 [00:00<00:00, 191.69it/s] 70%|███████   | 116/165 [00:00<00:00, 189.11it/s]2024-07-08:20:27:35,966 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 20/165 [00:00<00:00, 190.40it/s] 24%|██▍       | 40/165 [00:00<00:00, 193.28it/s] 82%|████████▏ | 136/165 [00:00<00:00, 189.79it/s] 12%|█▏        | 20/165 [00:00<00:00, 191.96it/s]2024-07-08:20:27:36,112 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
 24%|██▍       | 40/165 [00:00<00:00, 192.23it/s]  0%|          | 0/165 [00:00<?, ?it/s] 36%|███▋      | 60/165 [00:00<00:00, 193.93it/s] 95%|█████████▍| 156/165 [00:00<00:00, 190.25it/s] 24%|██▍       | 40/165 [00:00<00:00, 193.46it/s]100%|██████████| 165/165 [00:00<00:00, 189.40it/s]
 36%|███▋      | 60/165 [00:00<00:00, 192.86it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.45it/s] 48%|████▊     | 80/165 [00:00<00:00, 194.79it/s]2024-07-08:20:27:36,291 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
 36%|███▋      | 60/165 [00:00<00:00, 193.81it/s]  0%|          | 0/165 [00:00<?, ?it/s] 48%|████▊     | 80/165 [00:00<00:00, 193.65it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.35it/s] 61%|██████    | 100/165 [00:00<00:00, 195.23it/s] 48%|████▊     | 80/165 [00:00<00:00, 194.56it/s] 12%|█▏        | 20/165 [00:00<00:00, 190.79it/s] 61%|██████    | 100/165 [00:00<00:00, 194.22it/s] 36%|███▌      | 59/165 [00:00<00:00, 192.04it/s] 73%|███████▎  | 120/165 [00:00<00:00, 195.54it/s] 61%|██████    | 100/165 [00:00<00:00, 194.98it/s] 24%|██▍       | 40/165 [00:00<00:00, 192.44it/s] 73%|███████▎  | 120/165 [00:00<00:00, 194.59it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.83it/s] 85%|████████▍ | 140/165 [00:00<00:00, 195.77it/s] 73%|███████▎  | 120/165 [00:00<00:00, 195.26it/s] 36%|███▋      | 60/165 [00:00<00:00, 192.72it/s] 85%|████████▍ | 140/165 [00:00<00:00, 194.82it/s] 60%|██████    | 99/165 [00:00<00:00, 193.38it/s] 97%|█████████▋| 160/165 [00:00<00:00, 195.75it/s]100%|██████████| 165/165 [00:00<00:00, 195.13it/s]
 85%|████████▍ | 140/165 [00:00<00:00, 195.29it/s] 48%|████▊     | 80/165 [00:00<00:00, 193.83it/s] 97%|█████████▋| 160/165 [00:00<00:00, 194.89it/s] 72%|███████▏  | 119/165 [00:00<00:00, 193.81it/s]100%|██████████| 165/165 [00:00<00:00, 194.15it/s]
 97%|█████████▋| 160/165 [00:00<00:00, 195.33it/s] 61%|██████    | 100/165 [00:00<00:00, 194.39it/s]100%|██████████| 165/165 [00:00<00:00, 194.83it/s]
 84%|████████▍ | 139/165 [00:00<00:00, 194.06it/s] 73%|███████▎  | 120/165 [00:00<00:00, 192.50it/s] 96%|█████████▋| 159/165 [00:00<00:00, 193.91it/s]100%|██████████| 165/165 [00:00<00:00, 193.25it/s]
 85%|████████▍ | 140/165 [00:00<00:00, 192.55it/s] 97%|█████████▋| 160/165 [00:00<00:00, 192.99it/s]100%|██████████| 165/165 [00:00<00:00, 192.98it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-08:20:28:02,735 INFO     [xhuggingface.py:323] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-08:20:28:04,305 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/164 [00:00<?, ?it/s]  7%|▋         | 12/164 [00:00<00:01, 114.85it/s]2024-07-08:20:28:04,544 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
 15%|█▍        | 24/164 [00:00<00:01, 115.99it/s]  0%|          | 0/165 [00:00<?, ?it/s] 22%|██▏       | 36/164 [00:00<00:01, 116.57it/s]  7%|▋         | 12/165 [00:00<00:01, 116.17it/s] 29%|██▉       | 48/164 [00:00<00:00, 117.10it/s] 15%|█▍        | 24/165 [00:00<00:01, 116.92it/s] 37%|███▋      | 60/164 [00:00<00:00, 117.34it/s] 22%|██▏       | 36/165 [00:00<00:01, 117.32it/s] 44%|████▍     | 72/164 [00:00<00:00, 117.62it/s] 29%|██▉       | 48/165 [00:00<00:00, 117.68it/s] 51%|█████     | 84/164 [00:00<00:00, 117.83it/s] 36%|███▋      | 60/165 [00:00<00:00, 117.90it/s] 59%|█████▊    | 96/164 [00:00<00:00, 118.08it/s] 44%|████▎     | 72/165 [00:00<00:00, 117.97it/s] 66%|██████▌   | 108/164 [00:00<00:00, 117.93it/s] 51%|█████     | 84/165 [00:00<00:00, 118.26it/s] 73%|███████▎  | 120/164 [00:01<00:00, 118.25it/s] 58%|█████▊    | 96/165 [00:00<00:00, 118.52it/s] 80%|████████  | 132/164 [00:01<00:00, 116.76it/s] 65%|██████▌   | 108/165 [00:00<00:00, 118.82it/s] 88%|████████▊ | 144/164 [00:01<00:00, 117.11it/s] 73%|███████▎  | 120/165 [00:01<00:00, 118.70it/s] 95%|█████████▌| 156/164 [00:01<00:00, 117.55it/s] 80%|████████  | 132/165 [00:01<00:00, 118.65it/s]100%|██████████| 164/164 [00:01<00:00, 117.42it/s]
 90%|████████▉ | 148/165 [00:01<00:00, 129.78it/s]100%|██████████| 165/165 [00:01<00:00, 126.06it/s]
2024-07-08:20:28:20,437 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:20:28:20,437 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:20:28:20,437 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:20:28:20,437 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:20:28:20,437 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:20:28:20,437 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:20:28:20,437 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-08:20:28:20,437 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/165 [00:12<33:48, 12.37s/it]Running generate_until requests:   1%|          | 2/165 [00:20<27:00,  9.94s/it]Running generate_until requests:   2%|▏         | 3/165 [00:39<37:50, 14.01s/it]Running generate_until requests:   2%|▏         | 4/165 [00:49<33:47, 12.59s/it]Running generate_until requests:   3%|▎         | 5/165 [00:56<28:15, 10.60s/it]Running generate_until requests:   4%|▎         | 6/165 [01:08<28:59, 10.94s/it]Running generate_until requests:   4%|▍         | 7/165 [01:14<24:33,  9.33s/it]Running generate_until requests:   5%|▍         | 8/165 [01:22<23:19,  8.91s/it]Running generate_until requests:   5%|▌         | 9/165 [01:27<19:43,  7.59s/it]Running generate_until requests:   6%|▌         | 10/165 [01:34<19:14,  7.45s/it]Running generate_until requests:   7%|▋         | 11/165 [01:43<20:36,  8.03s/it]Running generate_until requests:   7%|▋         | 12/165 [01:53<21:27,  8.41s/it]Running generate_until requests:   8%|▊         | 13/165 [02:03<22:53,  9.04s/it]Running generate_until requests:   8%|▊         | 14/165 [02:13<23:42,  9.42s/it]Running generate_until requests:   9%|▉         | 15/165 [02:20<21:47,  8.72s/it]Running generate_until requests:  10%|▉         | 16/165 [02:25<18:49,  7.58s/it]Running generate_until requests:  10%|█         | 17/165 [02:36<20:54,  8.47s/it]Running generate_until requests:  11%|█         | 18/165 [02:45<21:12,  8.65s/it]Running generate_until requests:  12%|█▏        | 19/165 [02:49<17:54,  7.36s/it]Running generate_until requests:  12%|█▏        | 20/165 [02:53<15:03,  6.23s/it]Running generate_until requests:  13%|█▎        | 21/165 [03:02<16:46,  6.99s/it]Running generate_until requests:  13%|█▎        | 22/165 [03:05<14:16,  5.99s/it]Running generate_until requests:  14%|█▍        | 23/165 [03:08<12:02,  5.09s/it]Running generate_until requests:  15%|█▍        | 24/165 [03:13<11:21,  4.83s/it]Running generate_until requests:  15%|█▌        | 25/165 [03:18<11:27,  4.91s/it]Running generate_until requests:  16%|█▌        | 26/165 [03:22<10:57,  4.73s/it]Running generate_until requests:  16%|█▋        | 27/165 [03:27<11:24,  4.96s/it]Running generate_until requests:  17%|█▋        | 28/165 [03:30<09:55,  4.35s/it]Running generate_until requests:  18%|█▊        | 29/165 [03:36<11:03,  4.88s/it]Running generate_until requests:  18%|█▊        | 30/165 [03:41<10:30,  4.67s/it]Running generate_until requests:  19%|█▉        | 31/165 [03:43<08:47,  3.93s/it]Running generate_until requests:  19%|█▉        | 31/165 [03:47<16:24,  7.35s/it]
