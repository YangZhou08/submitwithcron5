Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:55<00:55, 55.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.04s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.65s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.09s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:14<00:00, 33.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:14<00:00, 37.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 34.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 37.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 34.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 37.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 34.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 37.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 34.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 37.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 34.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 37.66s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 34.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 37.84s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 34.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 37.76s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:32<16:50, 32.59s/it]  3%|▎         | 1/32 [00:32<17:01, 32.95s/it]  3%|▎         | 1/32 [00:35<18:17, 35.42s/it]  3%|▎         | 1/32 [00:36<19:03, 36.90s/it]  3%|▎         | 1/32 [00:36<19:03, 36.89s/it]  3%|▎         | 1/32 [00:37<19:35, 37.93s/it]  3%|▎         | 1/32 [00:48<25:03, 48.50s/it]  3%|▎         | 1/32 [00:51<26:46, 51.81s/it]  6%|▋         | 2/32 [01:01<15:15, 30.51s/it]  6%|▋         | 2/32 [01:04<15:55, 31.86s/it]  6%|▋         | 2/32 [01:05<16:22, 32.75s/it]  6%|▋         | 2/32 [01:07<16:28, 32.94s/it]  6%|▋         | 2/32 [01:09<17:11, 34.37s/it]  6%|▋         | 2/32 [01:11<17:53, 35.80s/it]  9%|▉         | 3/32 [01:35<15:07, 31.30s/it]  6%|▋         | 2/32 [01:35<23:48, 47.63s/it]  9%|▉         | 3/32 [01:35<15:24, 31.88s/it]  9%|▉         | 3/32 [01:36<15:10, 31.41s/it]  9%|▉         | 3/32 [01:37<15:29, 32.06s/it]  6%|▋         | 2/32 [01:39<24:41, 49.39s/it]  9%|▉         | 3/32 [01:40<15:47, 32.68s/it]  9%|▉         | 3/32 [01:44<16:34, 34.28s/it] 12%|█▎        | 4/32 [02:04<14:16, 30.58s/it] 12%|█▎        | 4/32 [02:06<14:42, 31.51s/it] 12%|█▎        | 4/32 [02:10<15:07, 32.41s/it] 12%|█▎        | 4/32 [02:11<15:28, 33.17s/it] 12%|█▎        | 4/32 [02:15<15:40, 33.59s/it] 12%|█▎        | 4/32 [02:21<16:28, 35.30s/it]  9%|▉         | 3/32 [02:28<24:06, 49.88s/it]  9%|▉         | 3/32 [02:31<24:27, 50.60s/it] 16%|█▌        | 5/32 [02:36<14:02, 31.20s/it] 16%|█▌        | 5/32 [02:42<14:31, 32.28s/it] 16%|█▌        | 5/32 [02:43<14:36, 32.45s/it] 16%|█▌        | 5/32 [02:43<15:05, 33.53s/it] 16%|█▌        | 5/32 [02:47<14:57, 33.25s/it] 16%|█▌        | 5/32 [02:52<15:17, 33.97s/it] 19%|█▉        | 6/32 [03:08<13:34, 31.34s/it] 19%|█▉        | 6/32 [03:14<13:58, 32.24s/it] 19%|█▉        | 6/32 [03:15<14:01, 32.37s/it] 12%|█▎        | 4/32 [03:16<22:59, 49.26s/it] 12%|█▎        | 4/32 [03:19<23:03, 49.40s/it] 19%|█▉        | 6/32 [03:19<14:14, 32.88s/it] 19%|█▉        | 6/32 [03:27<16:03, 37.04s/it] 19%|█▉        | 6/32 [03:27<14:45, 34.07s/it] 22%|██▏       | 7/32 [03:40<13:06, 31.45s/it] 22%|██▏       | 7/32 [03:45<13:15, 31.83s/it] 22%|██▏       | 7/32 [03:47<13:28, 32.35s/it] 22%|██▏       | 7/32 [03:53<13:44, 33.00s/it] 22%|██▏       | 7/32 [04:01<14:11, 34.07s/it] 22%|██▏       | 7/32 [04:04<15:27, 37.09s/it] 16%|█▌        | 5/32 [04:04<22:01, 48.96s/it] 25%|██▌       | 8/32 [04:09<12:22, 30.94s/it] 16%|█▌        | 5/32 [04:10<22:36, 50.25s/it] 25%|██▌       | 8/32 [04:15<12:30, 31.29s/it] 25%|██▌       | 8/32 [04:17<12:34, 31.42s/it] 25%|██▌       | 8/32 [04:23<12:49, 32.06s/it] 25%|██▌       | 8/32 [04:30<13:04, 32.70s/it] 28%|██▊       | 9/32 [04:38<11:37, 30.33s/it] 25%|██▌       | 8/32 [04:41<14:51, 37.14s/it] 28%|██▊       | 9/32 [04:46<11:42, 30.56s/it] 28%|██▊       | 9/32 [04:46<11:57, 31.20s/it] 19%|█▉        | 6/32 [04:51<20:53, 48.20s/it] 28%|██▊       | 9/32 [04:52<11:54, 31.09s/it] 28%|██▊       | 9/32 [05:01<12:17, 32.05s/it] 19%|█▉        | 6/32 [05:02<21:59, 50.74s/it] 31%|███▏      | 10/32 [05:08<11:03, 30.16s/it] 31%|███▏      | 10/32 [05:18<11:29, 31.36s/it] 28%|██▊       | 9/32 [05:19<14:22, 37.51s/it] 31%|███▏      | 10/32 [05:21<11:48, 32.19s/it] 31%|███▏      | 10/32 [05:22<11:21, 30.97s/it] 31%|███▏      | 10/32 [05:35<11:59, 32.69s/it] 22%|██▏       | 7/32 [05:38<19:53, 47.73s/it] 34%|███▍      | 11/32 [05:40<10:43, 30.63s/it] 22%|██▏       | 7/32 [05:47<20:18, 48.74s/it] 34%|███▍      | 11/32 [05:47<10:44, 30.70s/it] 34%|███▍      | 11/32 [05:52<11:05, 31.69s/it] 34%|███▍      | 11/32 [05:54<10:55, 31.19s/it] 31%|███▏      | 10/32 [05:57<13:42, 37.41s/it] 34%|███▍      | 11/32 [06:10<11:40, 33.37s/it] 38%|███▊      | 12/32 [06:11<10:12, 30.65s/it] 38%|███▊      | 12/32 [06:19<10:20, 31.05s/it] 38%|███▊      | 12/32 [06:26<10:26, 31.34s/it] 38%|███▊      | 12/32 [06:27<10:52, 32.64s/it] 25%|██▌       | 8/32 [06:27<19:19, 48.30s/it] 25%|██▌       | 8/32 [06:34<19:18, 48.26s/it] 34%|███▍      | 11/32 [06:34<13:03, 37.32s/it] 41%|████      | 13/32 [06:40<09:37, 30.42s/it] 38%|███▊      | 12/32 [06:43<11:07, 33.37s/it] 41%|████      | 13/32 [06:50<09:49, 31.03s/it] 41%|████      | 13/32 [06:56<09:46, 30.87s/it] 41%|████      | 13/32 [06:57<10:07, 31.98s/it] 44%|████▍     | 14/32 [07:11<09:08, 30.45s/it] 38%|███▊      | 12/32 [07:13<12:38, 37.93s/it] 41%|████      | 13/32 [07:16<10:29, 33.11s/it] 28%|██▊       | 9/32 [07:18<18:49, 49.10s/it] 44%|████▍     | 14/32 [07:21<09:14, 30.82s/it] 28%|██▊       | 9/32 [07:21<18:22, 47.95s/it] 44%|████▍     | 14/32 [07:27<09:16, 30.94s/it] 44%|████▍     | 14/32 [07:29<09:33, 31.84s/it] 47%|████▋     | 15/32 [07:40<08:29, 29.97s/it] 44%|████▍     | 14/32 [07:47<09:47, 32.62s/it] 47%|████▋     | 15/32 [07:52<08:44, 30.88s/it] 41%|████      | 13/32 [07:52<12:09, 38.37s/it] 47%|████▋     | 15/32 [07:59<08:54, 31.45s/it] 47%|████▋     | 15/32 [08:00<08:58, 31.70s/it] 31%|███▏      | 10/32 [08:03<17:33, 47.90s/it] 31%|███▏      | 10/32 [08:09<17:31, 47.79s/it] 50%|█████     | 16/32 [08:12<08:11, 30.72s/it] 50%|█████     | 16/32 [08:21<08:06, 30.39s/it] 47%|████▋     | 15/32 [08:22<09:22, 33.10s/it] 44%|████▍     | 14/32 [08:29<11:18, 37.68s/it] 50%|█████     | 16/32 [08:30<08:19, 31.19s/it] 50%|█████     | 16/32 [08:32<08:27, 31.75s/it] 53%|█████▎    | 17/32 [08:42<07:37, 30.48s/it] 34%|███▍      | 11/32 [08:50<16:37, 47.52s/it] 53%|█████▎    | 17/32 [08:54<07:46, 31.10s/it] 50%|█████     | 16/32 [08:56<08:54, 33.43s/it] 34%|███▍      | 11/32 [08:58<16:50, 48.13s/it] 53%|█████▎    | 17/32 [09:01<07:44, 30.95s/it] 53%|█████▎    | 17/32 [09:01<07:48, 31.26s/it] 47%|████▋     | 15/32 [09:03<10:26, 36.86s/it] 56%|█████▋    | 18/32 [09:14<07:10, 30.76s/it] 56%|█████▋    | 18/32 [09:25<07:15, 31.08s/it] 53%|█████▎    | 17/32 [09:28<08:16, 33.13s/it] 56%|█████▋    | 18/32 [09:34<07:19, 31.43s/it] 56%|█████▋    | 18/32 [09:35<07:26, 31.88s/it] 38%|███▊      | 12/32 [09:37<15:45, 47.26s/it] 59%|█████▉    | 19/32 [09:42<06:29, 29.98s/it] 50%|█████     | 16/32 [09:46<10:17, 38.60s/it] 38%|███▊      | 12/32 [09:48<16:14, 48.73s/it] 59%|█████▉    | 19/32 [09:56<06:43, 31.05s/it] 56%|█████▋    | 18/32 [10:03<07:51, 33.69s/it] 59%|█████▉    | 19/32 [10:04<06:46, 31.25s/it] 59%|█████▉    | 19/32 [10:07<06:55, 31.98s/it] 62%|██████▎   | 20/32 [10:12<05:58, 29.90s/it] 41%|████      | 13/32 [10:21<14:38, 46.24s/it] 53%|█████▎    | 17/32 [10:26<09:42, 38.85s/it] 62%|██████▎   | 20/32 [10:26<06:10, 30.88s/it] 62%|██████▎   | 20/32 [10:34<06:08, 30.69s/it] 41%|████      | 13/32 [10:35<15:19, 48.39s/it] 59%|█████▉    | 19/32 [10:35<07:10, 33.11s/it] 62%|██████▎   | 20/32 [10:39<06:25, 32.12s/it] 66%|██████▌   | 21/32 [10:42<05:31, 30.10s/it] 66%|██████▌   | 21/32 [10:59<05:46, 31.54s/it] 66%|██████▌   | 21/32 [11:03<05:32, 30.21s/it] 56%|█████▋    | 18/32 [11:03<08:56, 38.35s/it] 66%|██████▌   | 21/32 [11:10<05:47, 31.61s/it] 44%|████▍     | 14/32 [11:10<14:10, 47.23s/it] 69%|██████▉   | 22/32 [11:10<04:54, 29.43s/it] 62%|██████▎   | 20/32 [11:11<06:47, 33.96s/it] 44%|████▍     | 14/32 [11:20<14:10, 47.25s/it] 69%|██████▉   | 22/32 [11:29<05:08, 30.90s/it] 69%|██████▉   | 22/32 [11:34<05:04, 30.43s/it] 59%|█████▉    | 19/32 [11:39<08:09, 37.68s/it] 69%|██████▉   | 22/32 [11:41<05:15, 31.56s/it] 72%|███████▏  | 23/32 [11:43<04:34, 30.50s/it] 66%|██████▌   | 21/32 [11:45<06:14, 34.01s/it] 47%|████▋     | 15/32 [11:55<13:13, 46.66s/it] 72%|███████▏  | 23/32 [11:59<04:36, 30.68s/it] 72%|███████▏  | 23/32 [12:05<04:35, 30.56s/it] 47%|████▋     | 15/32 [12:10<13:37, 48.11s/it] 72%|███████▏  | 23/32 [12:13<04:43, 31.50s/it] 78%|███████▊  | 25/32 [12:13<02:44, 23.44s/it] 69%|██████▉   | 22/32 [12:16<05:29, 32.98s/it] 62%|██████▎   | 20/32 [12:22<07:50, 39.23s/it] 75%|███████▌  | 24/32 [12:27<04:00, 30.04s/it] 75%|███████▌  | 24/32 [12:35<04:03, 30.49s/it] 75%|███████▌  | 24/32 [12:43<04:09, 31.22s/it] 81%|████████▏ | 26/32 [12:44<02:31, 25.21s/it] 72%|███████▏  | 23/32 [12:47<04:53, 32.56s/it] 50%|█████     | 16/32 [12:49<13:00, 48.77s/it] 78%|███████▊  | 25/32 [12:56<03:26, 29.57s/it] 66%|██████▌   | 21/32 [13:01<07:12, 39.28s/it] 50%|█████     | 16/32 [13:04<13:20, 50.03s/it] 78%|███████▊  | 25/32 [13:06<03:34, 30.60s/it] 78%|███████▊  | 25/32 [13:13<03:35, 30.75s/it] 84%|████████▍ | 27/32 [13:16<02:15, 27.04s/it] 75%|███████▌  | 24/32 [13:19<04:17, 32.22s/it] 81%|████████▏ | 26/32 [13:28<03:01, 30.33s/it] 81%|████████▏ | 26/32 [13:35<03:00, 30.14s/it] 69%|██████▉   | 22/32 [13:38<06:24, 38.42s/it] 53%|█████▎    | 17/32 [13:39<12:15, 49.01s/it] 81%|████████▏ | 26/32 [13:44<03:05, 30.97s/it] 88%|████████▊ | 28/32 [13:47<01:52, 28.12s/it] 78%|███████▊  | 25/32 [13:51<03:45, 32.24s/it] 53%|█████▎    | 17/32 [13:57<12:43, 50.89s/it] 84%|████████▍ | 27/32 [14:00<02:34, 30.81s/it] 84%|████████▍ | 27/32 [14:04<02:29, 29.87s/it] 84%|████████▍ | 27/32 [14:15<02:34, 30.81s/it] 72%|███████▏  | 23/32 [14:17<05:48, 38.68s/it] 91%|█████████ | 29/32 [14:18<01:26, 28.90s/it] 81%|████████▏ | 26/32 [14:21<03:08, 31.45s/it] 56%|█████▋    | 18/32 [14:27<11:22, 48.72s/it] 88%|████████▊ | 28/32 [14:30<02:02, 30.67s/it] 88%|████████▊ | 28/32 [14:34<01:59, 29.89s/it] 88%|████████▊ | 28/32 [14:46<02:04, 31.02s/it] 56%|█████▋    | 18/32 [14:47<11:49, 50.65s/it] 94%|█████████▍| 30/32 [14:48<00:58, 29.29s/it] 84%|████████▍ | 27/32 [14:55<02:41, 32.27s/it] 75%|███████▌  | 24/32 [14:55<05:08, 38.57s/it] 91%|█████████ | 29/32 [15:03<01:33, 31.29s/it] 91%|█████████ | 29/32 [15:07<01:32, 30.69s/it] 59%|█████▉    | 19/32 [15:16<10:35, 48.92s/it] 91%|█████████ | 29/32 [15:18<01:33, 31.14s/it] 97%|█████████▋| 31/32 [15:17<00:29, 29.21s/it] 88%|████████▊ | 28/32 [15:26<02:07, 31.81s/it] 78%|███████▊  | 25/32 [15:34<04:31, 38.81s/it] 94%|█████████▍| 30/32 [15:37<01:04, 32.05s/it] 94%|█████████▍| 30/32 [15:37<01:00, 30.46s/it] 59%|█████▉    | 19/32 [15:40<11:07, 51.38s/it]100%|██████████| 32/32 [15:47<00:00, 29.47s/it]100%|██████████| 32/32 [15:47<00:00, 29.62s/it]
 94%|█████████▍| 30/32 [15:51<01:03, 31.82s/it] 91%|█████████ | 29/32 [16:00<01:37, 32.52s/it] 62%|██████▎   | 20/32 [16:03<09:38, 48.25s/it] 97%|█████████▋| 31/32 [16:07<00:31, 31.48s/it] 97%|█████████▋| 31/32 [16:08<00:30, 30.78s/it] 81%|████████▏ | 26/32 [16:14<03:53, 38.89s/it] 97%|█████████▋| 31/32 [16:22<00:31, 31.70s/it] 62%|██████▎   | 20/32 [16:27<09:58, 49.85s/it] 94%|█████████▍| 30/32 [16:34<01:06, 33.04s/it]100%|██████████| 32/32 [16:38<00:00, 31.30s/it]100%|██████████| 32/32 [16:38<00:00, 31.19s/it]
100%|██████████| 32/32 [16:41<00:00, 31.32s/it]100%|██████████| 32/32 [16:41<00:00, 31.29s/it]
 66%|██████▌   | 21/32 [16:51<08:51, 48.32s/it] 84%|████████▍ | 27/32 [16:54<03:16, 39.31s/it]100%|██████████| 32/32 [16:56<00:00, 32.39s/it]100%|██████████| 32/32 [16:56<00:00, 31.78s/it]
 97%|█████████▋| 31/32 [17:05<00:32, 32.51s/it] 66%|██████▌   | 21/32 [17:13<08:55, 48.71s/it] 88%|████████▊ | 28/32 [17:33<02:36, 39.19s/it]100%|██████████| 32/32 [17:36<00:00, 31.91s/it]100%|██████████| 32/32 [17:36<00:00, 33.01s/it]
 69%|██████▉   | 22/32 [17:40<08:03, 48.32s/it] 69%|██████▉   | 22/32 [18:03<08:11, 49.16s/it] 91%|█████████ | 29/32 [18:10<01:55, 38.48s/it] 72%|███████▏  | 23/32 [18:31<07:21, 49.09s/it] 72%|███████▏  | 23/32 [18:48<07:10, 47.82s/it] 94%|█████████▍| 30/32 [18:48<01:16, 38.36s/it] 75%|███████▌  | 24/32 [19:18<06:30, 48.76s/it] 97%|█████████▋| 31/32 [19:24<00:37, 37.90s/it] 75%|███████▌  | 24/32 [19:39<06:31, 48.89s/it]100%|██████████| 32/32 [20:04<00:00, 38.29s/it]100%|██████████| 32/32 [20:04<00:00, 37.63s/it]
 78%|███████▊  | 25/32 [20:07<05:39, 48.54s/it] 78%|███████▊  | 25/32 [20:27<05:39, 48.47s/it] 81%|████████▏ | 26/32 [20:56<04:52, 48.80s/it] 81%|████████▏ | 26/32 [21:18<04:56, 49.38s/it] 84%|████████▍ | 27/32 [21:43<04:00, 48.19s/it] 84%|████████▍ | 27/32 [22:04<04:01, 48.36s/it] 88%|████████▊ | 28/32 [22:32<03:14, 48.58s/it] 88%|████████▊ | 28/32 [22:53<03:14, 48.51s/it] 94%|█████████▍| 30/32 [23:19<01:13, 36.93s/it] 91%|█████████ | 29/32 [23:40<02:24, 48.18s/it] 97%|█████████▋| 31/32 [24:08<00:40, 40.04s/it] 94%|█████████▍| 30/32 [24:32<01:38, 49.21s/it]100%|██████████| 32/32 [24:55<00:00, 41.81s/it]100%|██████████| 32/32 [24:55<00:00, 46.74s/it]
 97%|█████████▋| 31/32 [25:17<00:47, 47.87s/it][rank6]:[E ProcessGroupNCCL.cpp:523] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600540 milliseconds before timing out.
[rank6]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E ProcessGroupNCCL.cpp:1182] [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600540 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f062fcf1d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f0630e996e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f0630e9cc3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f0630e9d839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f067abb0df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f067bf9f609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f067bd6a353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600540 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f062fcf1d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f0630e996e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f0630e9cc3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f0630e9d839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f067abb0df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f067bf9f609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f067bd6a353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f062fcf1d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f0630bf3b11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f067abb0df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f067bf9f609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f067bd6a353 in /lib/x86_64-linux-gnu/libc.so.6)

[2024-07-09 20:11:13,507] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3766640 closing signal SIGTERM
[2024-07-09 20:11:13,509] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3766641 closing signal SIGTERM
[2024-07-09 20:11:13,509] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3766642 closing signal SIGTERM
[2024-07-09 20:11:13,509] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3766643 closing signal SIGTERM
[2024-07-09 20:11:13,509] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3766644 closing signal SIGTERM
[2024-07-09 20:11:13,510] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3766645 closing signal SIGTERM
[2024-07-09 20:11:13,510] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3766648 closing signal SIGTERM
[2024-07-09 20:11:15,216] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 6 (pid: 3766647) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
main.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-09_20:11:13
  host      : a100-st-p4de24xlarge-148.fair-a100.hpcaas
  rank      : 6 (local_rank: 6)
  exitcode  : -6 (pid: 3766647)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3766647
========================================================
