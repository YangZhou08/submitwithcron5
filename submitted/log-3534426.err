Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-25:03:30:26,424 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:30:26,424 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:30:26,501 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:30:26,535 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:30:26,583 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:30:26,610 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:30:26,624 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:30:26,626 INFO     [main.py:288] Verbosity set to INFO
2024-07-25:03:30:35,862 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:30:35,862 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:30:35,862 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:30:35,862 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:30:35,862 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:30:35,862 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:30:35,862 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:30:35,862 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-25:03:30:35,887 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:30:35,887 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:30:35,887 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:30:35,887 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:30:35,887 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:30:35,887 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:30:35,887 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:30:35,887 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-25:03:30:35,887 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 1, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:30:35,887 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 1, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:30:35,887 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 1, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:30:35,887 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 1, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:30:35,887 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 1, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:30:35,887 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 1, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:30:35,888 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 1, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
2024-07-25:03:30:35,888 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'widthtree': 1, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.05}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.01s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:00, 20.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:59, 19.71s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:58, 19.60s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:58, 19.66s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:58, 19.46s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:58, 19.61s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:58, 19.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:35, 17.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:36, 18.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:36, 18.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:36, 18.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:36, 18.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:36, 18.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:36, 18.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:36, 18.25s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:53<00:17, 17.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:53<00:17, 17.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:53<00:17, 17.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:53<00:17, 17.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:53<00:17, 17.40s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:53<00:17, 17.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:53<00:17, 17.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:53<00:17, 17.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 11.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.68s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 10.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.58s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 10.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.57s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 10.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.56s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 10.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.56s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 10.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.53s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 10.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.59s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 10.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.55s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-25:03:32:12,569 INFO     [xhuggingface.py:336] Using 8 devices with data parallelism
2024-07-25:03:32:13,212 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:13,212 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:13,260 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:13,260 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:13,290 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:13,291 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:13,291 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 7...
2024-07-25:03:32:13,291 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 4...
2024-07-25:03:32:13,294 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:13,294 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/164 [00:00<?, ?it/s]2024-07-25:03:32:13,321 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 5...
2024-07-25:03:32:13,324 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 2...
  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/165 [00:00<?, ?it/s]2024-07-25:03:32:13,398 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:13,398 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
100%|██████████| 164/164 [00:00<00:00, 1696.31it/s]
100%|██████████| 165/165 [00:00<00:00, 1694.64it/s]
2024-07-25:03:32:13,430 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 6...
100%|██████████| 165/165 [00:00<00:00, 1688.41it/s]
100%|██████████| 165/165 [00:00<00:00, 1689.41it/s]
  0%|          | 0/165 [00:00<?, ?it/s]100%|██████████| 165/165 [00:00<00:00, 1687.39it/s]
2024-07-25:03:32:13,629 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:13,629 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:13,660 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/165 [00:00<?, ?it/s]100%|██████████| 165/165 [00:00<00:00, 1688.15it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-25:03:32:42,188 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:42,189 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:42,220 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 3...
  0%|          | 0/165 [00:00<?, ?it/s] 79%|███████▉  | 130/165 [00:00<00:00, 1286.05it/s]2024-07-25:03:32:42,352 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-25:03:32:42,352 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
100%|██████████| 165/165 [00:00<00:00, 1205.65it/s]
2024-07-25:03:32:42,412 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 1...
  0%|          | 0/165 [00:00<?, ?it/s] 41%|████      | 67/165 [00:00<00:00, 659.51it/s] 82%|████████▏ | 135/165 [00:00<00:00, 667.88it/s]100%|██████████| 165/165 [00:00<00:00, 690.04it/s]
2024-07-25:03:32:57,723 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-25:03:32:57,723 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-25:03:32:57,723 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-25:03:32:57,723 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-25:03:32:57,723 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-25:03:32:57,723 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-25:03:32:57,723 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-25:03:32:57,723 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/165 [00:07<20:48,  7.61s/it]Running generate_until requests:   1%|          | 2/165 [00:13<17:39,  6.50s/it]Running generate_until requests:   2%|▏         | 3/165 [00:18<15:50,  5.87s/it]Running generate_until requests:   2%|▏         | 4/165 [00:23<14:55,  5.56s/it]Running generate_until requests:   3%|▎         | 5/165 [00:28<14:26,  5.42s/it]Running generate_until requests:   4%|▎         | 6/165 [00:32<12:57,  4.89s/it]Running generate_until requests:   4%|▍         | 7/165 [00:36<11:58,  4.55s/it]Running generate_until requests:   5%|▍         | 8/165 [00:42<12:51,  4.91s/it]Running generate_until requests:   5%|▌         | 9/165 [00:48<13:55,  5.36s/it]Running generate_until requests:   6%|▌         | 10/165 [00:54<14:36,  5.65s/it]Running generate_until requests:   7%|▋         | 11/165 [00:58<12:37,  4.92s/it]Running generate_until requests:   7%|▋         | 12/165 [01:01<11:15,  4.41s/it]Running generate_until requests:   8%|▊         | 13/165 [01:05<11:13,  4.43s/it]Running generate_until requests:   8%|▊         | 14/165 [01:10<11:10,  4.44s/it]Running generate_until requests:   9%|▉         | 15/165 [01:14<10:38,  4.26s/it]Running generate_until requests:  10%|▉         | 16/165 [01:17<10:15,  4.13s/it]Running generate_until requests:  10%|█         | 17/165 [01:24<11:48,  4.79s/it]Running generate_until requests:  11%|█         | 18/165 [01:29<12:28,  5.09s/it]Running generate_until requests:  12%|█▏        | 19/165 [01:36<13:44,  5.65s/it]Running generate_until requests:  12%|█▏        | 20/165 [01:40<12:20,  5.11s/it]Running generate_until requests:  13%|█▎        | 21/165 [01:45<12:14,  5.10s/it]Running generate_until requests:  13%|█▎        | 22/165 [01:50<12:07,  5.09s/it]Running generate_until requests:  14%|█▍        | 23/165 [01:56<12:01,  5.08s/it]Running generate_until requests:  15%|█▍        | 24/165 [01:58<10:14,  4.36s/it]Running generate_until requests:  15%|█▌        | 25/165 [02:05<11:58,  5.13s/it]Running generate_until requests:  16%|█▌        | 26/165 [02:11<12:16,  5.30s/it]Running generate_until requests:  16%|█▋        | 27/165 [02:15<11:36,  5.05s/it]Running generate_until requests:  17%|█▋        | 28/165 [02:19<10:41,  4.68s/it]Running generate_until requests:  18%|█▊        | 29/165 [02:22<09:11,  4.06s/it]Running generate_until requests:  18%|█▊        | 30/165 [02:26<08:58,  3.99s/it]Running generate_until requests:  19%|█▉        | 31/165 [02:29<08:53,  3.98s/it]Running generate_until requests:  19%|█▉        | 32/165 [02:35<09:33,  4.31s/it]Running generate_until requests:  20%|██        | 33/165 [02:37<08:21,  3.80s/it]Running generate_until requests:  21%|██        | 34/165 [02:39<07:06,  3.25s/it]Running generate_until requests:  21%|██        | 35/165 [02:43<07:25,  3.43s/it]Running generate_until requests:  22%|██▏       | 36/165 [02:46<07:13,  3.36s/it]Running generate_until requests:  22%|██▏       | 37/165 [02:49<07:04,  3.32s/it]Running generate_until requests:  23%|██▎       | 38/165 [02:54<08:08,  3.84s/it]Running generate_until requests:  24%|██▎       | 39/165 [02:57<07:17,  3.47s/it]Running generate_until requests:  24%|██▍       | 40/165 [03:00<06:41,  3.21s/it]Running generate_until requests:  25%|██▍       | 41/165 [03:02<06:15,  3.03s/it]Running generate_until requests:  25%|██▌       | 42/165 [03:07<07:28,  3.64s/it]Running generate_until requests:  26%|██▌       | 43/165 [03:10<06:46,  3.33s/it]Running generate_until requests:  27%|██▋       | 44/165 [03:14<07:23,  3.66s/it]Running generate_until requests:  27%|██▋       | 45/165 [03:18<07:03,  3.53s/it]Running generate_until requests:  28%|██▊       | 46/165 [03:26<09:45,  4.92s/it]Running generate_until requests:  28%|██▊       | 47/165 [03:32<10:32,  5.36s/it]Running generate_until requests:  29%|██▉       | 48/165 [03:36<09:33,  4.90s/it]Running generate_until requests:  30%|██▉       | 49/165 [03:39<08:30,  4.40s/it]Running generate_until requests:  30%|███       | 50/165 [03:44<08:49,  4.60s/it]Running generate_until requests:  31%|███       | 51/165 [03:48<08:18,  4.37s/it]Running generate_until requests:  32%|███▏      | 52/165 [03:51<07:34,  4.02s/it]Running generate_until requests:  32%|███▏      | 53/165 [03:55<07:24,  3.97s/it]Running generate_until requests:  33%|███▎      | 54/165 [03:59<07:18,  3.95s/it]Running generate_until requests:  33%|███▎      | 55/165 [04:03<07:10,  3.91s/it]Running generate_until requests:  34%|███▍      | 56/165 [04:07<07:03,  3.89s/it]Running generate_until requests:  35%|███▍      | 57/165 [04:10<06:38,  3.69s/it]Running generate_until requests:  35%|███▌      | 58/165 [04:14<06:39,  3.73s/it]Running generate_until requests:  36%|███▌      | 59/165 [04:17<06:19,  3.58s/it]Running generate_until requests:  36%|███▋      | 60/165 [04:21<06:43,  3.84s/it]Running generate_until requests:  37%|███▋      | 61/165 [04:24<06:00,  3.47s/it]Running generate_until requests:  38%|███▊      | 62/165 [04:29<06:48,  3.97s/it]Running generate_until requests:  38%|███▊      | 63/165 [04:36<07:56,  4.67s/it]Running generate_until requests:  39%|███▉      | 64/165 [04:40<07:45,  4.61s/it]Running generate_until requests:  39%|███▉      | 65/165 [04:43<06:40,  4.00s/it]Running generate_until requests:  40%|████      | 66/165 [04:45<05:54,  3.58s/it]Running generate_until requests:  41%|████      | 67/165 [04:51<06:53,  4.22s/it]Running generate_until requests:  41%|████      | 68/165 [04:55<06:56,  4.29s/it]Running generate_until requests:  42%|████▏     | 69/165 [04:59<06:39,  4.16s/it]Running generate_until requests:  42%|████▏     | 70/165 [05:03<06:26,  4.06s/it]Running generate_until requests:  43%|████▎     | 71/165 [05:06<05:58,  3.81s/it]Running generate_until requests:  44%|████▎     | 72/165 [05:08<05:03,  3.26s/it]Running generate_until requests:  44%|████▍     | 73/165 [05:13<05:32,  3.62s/it]Running generate_until requests:  45%|████▍     | 74/165 [05:16<05:18,  3.50s/it]Running generate_until requests:  45%|████▌     | 75/165 [05:20<05:40,  3.78s/it]Running generate_until requests:  46%|████▌     | 76/165 [05:25<05:54,  3.98s/it]Running generate_until requests:  47%|████▋     | 77/165 [05:30<06:22,  4.34s/it]Running generate_until requests:  47%|████▋     | 78/165 [05:33<05:48,  4.01s/it]Running generate_until requests:  48%|████▊     | 79/165 [05:36<05:23,  3.77s/it]Running generate_until requests:  48%|████▊     | 80/165 [05:40<05:06,  3.60s/it]Running generate_until requests:  49%|████▉     | 81/165 [05:43<05:08,  3.67s/it]Running generate_until requests:  50%|████▉     | 82/165 [05:45<04:22,  3.16s/it]Running generate_until requests:  50%|█████     | 83/165 [05:49<04:20,  3.18s/it]Running generate_until requests:  51%|█████     | 84/165 [05:52<04:33,  3.37s/it]Running generate_until requests:  52%|█████▏    | 85/165 [05:56<04:26,  3.33s/it]Running generate_until requests:  52%|█████▏    | 86/165 [05:58<03:51,  2.93s/it]Running generate_until requests:  53%|█████▎    | 87/165 [06:02<04:24,  3.40s/it]Running generate_until requests:  53%|█████▎    | 88/165 [06:06<04:31,  3.53s/it]Running generate_until requests:  54%|█████▍    | 89/165 [06:09<04:20,  3.43s/it]Running generate_until requests:  55%|█████▍    | 90/165 [06:12<03:58,  3.18s/it]Running generate_until requests:  55%|█████▌    | 91/165 [06:16<04:10,  3.38s/it]Running generate_until requests:  56%|█████▌    | 92/165 [06:18<03:49,  3.14s/it]Running generate_until requests:  56%|█████▋    | 93/165 [06:23<04:27,  3.72s/it]Running generate_until requests:  57%|█████▋    | 94/165 [06:25<03:47,  3.20s/it]Running generate_until requests:  58%|█████▊    | 95/165 [06:29<03:58,  3.40s/it]Running generate_until requests:  58%|█████▊    | 96/165 [06:32<03:50,  3.35s/it]Running generate_until requests:  59%|█████▉    | 97/165 [06:36<03:45,  3.31s/it]Running generate_until requests:  59%|█████▉    | 98/165 [06:39<03:39,  3.28s/it]Running generate_until requests:  60%|██████    | 99/165 [06:41<03:22,  3.07s/it]Running generate_until requests:  61%|██████    | 100/165 [06:45<03:22,  3.12s/it]Running generate_until requests:  61%|██████    | 101/165 [06:48<03:21,  3.15s/it]Running generate_until requests:  62%|██████▏   | 102/165 [06:53<03:54,  3.72s/it]Running generate_until requests:  62%|██████▏   | 103/165 [06:57<03:54,  3.78s/it]Running generate_until requests:  63%|██████▎   | 104/165 [07:00<03:40,  3.62s/it]Running generate_until requests:  64%|██████▎   | 105/165 [07:02<03:07,  3.13s/it]Running generate_until requests:  64%|██████▍   | 106/165 [07:04<02:44,  2.78s/it]