Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:56<00:56, 56.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:58<00:58, 58.48s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.76s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:58<00:58, 58.02s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:57<00:57, 57.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:12<00:00, 32.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:12<00:00, 36.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 33.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 33.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 32.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 32.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 32.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 32.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 32.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.61s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:36<18:45, 36.30s/it]  3%|▎         | 1/32 [00:36<18:45, 36.30s/it]  3%|▎         | 1/32 [00:36<19:04, 36.93s/it]  3%|▎         | 1/32 [00:37<19:18, 37.38s/it]  3%|▎         | 1/32 [00:37<19:23, 37.52s/it]  3%|▎         | 1/32 [00:39<20:21, 39.39s/it]  3%|▎         | 1/32 [00:53<27:43, 53.65s/it]  3%|▎         | 1/32 [00:58<30:01, 58.10s/it]  6%|▋         | 2/32 [01:10<17:24, 34.83s/it]  6%|▋         | 2/32 [01:10<17:32, 35.07s/it]  6%|▋         | 2/32 [01:11<17:39, 35.33s/it]  6%|▋         | 2/32 [01:11<17:40, 35.35s/it]  6%|▋         | 2/32 [01:13<18:05, 36.19s/it]  6%|▋         | 2/32 [01:14<18:38, 37.28s/it]  9%|▉         | 3/32 [01:43<16:33, 34.27s/it]  6%|▋         | 2/32 [01:44<26:06, 52.21s/it]  9%|▉         | 3/32 [01:45<16:45, 34.66s/it]  9%|▉         | 3/32 [01:45<16:46, 34.71s/it]  9%|▉         | 3/32 [01:46<17:08, 35.47s/it]  9%|▉         | 3/32 [01:46<16:53, 34.94s/it]  9%|▉         | 3/32 [01:48<17:16, 35.75s/it]  6%|▋         | 2/32 [01:48<26:54, 53.82s/it] 12%|█▎        | 4/32 [02:17<15:53, 34.04s/it] 12%|█▎        | 4/32 [02:19<16:02, 34.36s/it] 12%|█▎        | 4/32 [02:19<16:02, 34.37s/it] 12%|█▎        | 4/32 [02:20<16:14, 34.81s/it] 12%|█▎        | 4/32 [02:20<16:04, 34.44s/it] 12%|█▎        | 4/32 [02:21<16:18, 34.94s/it]  9%|▉         | 3/32 [02:36<25:00, 51.76s/it]  9%|▉         | 3/32 [02:41<25:41, 53.16s/it] 16%|█▌        | 5/32 [02:50<15:14, 33.87s/it] 16%|█▌        | 5/32 [02:52<15:22, 34.18s/it] 16%|█▌        | 5/32 [02:53<15:24, 34.25s/it] 16%|█▌        | 5/32 [02:54<15:28, 34.38s/it] 16%|█▌        | 5/32 [02:54<15:22, 34.17s/it] 16%|█▌        | 5/32 [02:55<15:32, 34.53s/it] 19%|█▉        | 6/32 [03:24<14:39, 33.81s/it] 19%|█▉        | 6/32 [03:26<14:47, 34.14s/it] 12%|█▎        | 4/32 [03:27<24:01, 51.49s/it] 19%|█▉        | 6/32 [03:27<14:42, 33.94s/it] 19%|█▉        | 6/32 [03:27<14:49, 34.21s/it] 19%|█▉        | 6/32 [03:28<14:56, 34.49s/it] 19%|█▉        | 6/32 [03:29<14:53, 34.37s/it] 12%|█▎        | 4/32 [03:32<24:23, 52.28s/it] 22%|██▏       | 7/32 [03:58<14:04, 33.79s/it] 22%|██▏       | 7/32 [04:00<14:10, 34.04s/it] 22%|██▏       | 7/32 [04:01<14:04, 33.78s/it] 22%|██▏       | 7/32 [04:01<14:11, 34.05s/it] 22%|██▏       | 7/32 [04:03<14:25, 34.63s/it] 22%|██▏       | 7/32 [04:03<14:14, 34.17s/it] 16%|█▌        | 5/32 [04:19<23:22, 51.95s/it] 16%|█▌        | 5/32 [04:24<23:29, 52.20s/it] 25%|██▌       | 8/32 [04:32<13:32, 33.84s/it] 25%|██▌       | 8/32 [04:34<13:37, 34.05s/it] 25%|██▌       | 8/32 [04:35<13:34, 33.96s/it] 25%|██▌       | 8/32 [04:36<13:41, 34.24s/it] 25%|██▌       | 8/32 [04:37<13:39, 34.14s/it] 25%|██▌       | 8/32 [04:39<14:02, 35.09s/it] 28%|██▊       | 9/32 [05:06<12:58, 33.85s/it] 28%|██▊       | 9/32 [05:09<13:03, 34.08s/it] 28%|██▊       | 9/32 [05:10<13:03, 34.07s/it] 28%|██▊       | 9/32 [05:10<13:08, 34.29s/it] 19%|█▉        | 6/32 [05:11<22:23, 51.68s/it] 28%|██▊       | 9/32 [05:12<13:13, 34.51s/it] 28%|██▊       | 9/32 [05:12<13:18, 34.71s/it] 19%|█▉        | 6/32 [05:14<22:23, 51.67s/it] 31%|███▏      | 10/32 [05:39<12:21, 33.71s/it] 31%|███▏      | 10/32 [05:42<12:27, 34.00s/it] 31%|███▏      | 10/32 [05:43<12:26, 33.94s/it] 31%|███▏      | 10/32 [05:43<12:29, 34.06s/it] 31%|███▏      | 10/32 [05:46<12:33, 34.26s/it] 31%|███▏      | 10/32 [05:48<12:51, 35.09s/it] 22%|██▏       | 7/32 [06:02<21:26, 51.45s/it] 22%|██▏       | 7/32 [06:07<21:35, 51.83s/it] 34%|███▍      | 11/32 [06:13<11:47, 33.69s/it] 34%|███▍      | 11/32 [06:16<11:53, 33.96s/it] 34%|███▍      | 11/32 [06:17<11:51, 33.88s/it] 34%|███▍      | 11/32 [06:17<11:54, 34.02s/it] 34%|███▍      | 11/32 [06:20<11:56, 34.13s/it] 34%|███▍      | 11/32 [06:22<12:10, 34.77s/it] 38%|███▊      | 12/32 [06:46<11:13, 33.67s/it] 38%|███▊      | 12/32 [06:51<11:16, 33.83s/it] 38%|███▊      | 12/32 [06:51<11:17, 33.89s/it] 38%|███▊      | 12/32 [06:51<11:25, 34.28s/it] 25%|██▌       | 8/32 [06:53<20:31, 51.31s/it] 38%|███▊      | 12/32 [06:54<11:20, 34.04s/it] 38%|███▊      | 12/32 [06:56<11:29, 34.46s/it] 25%|██▌       | 8/32 [06:59<20:48, 52.01s/it] 41%|████      | 13/32 [07:20<10:39, 33.65s/it] 41%|████      | 13/32 [07:24<10:41, 33.78s/it] 41%|████      | 13/32 [07:25<10:44, 33.91s/it] 41%|████      | 13/32 [07:25<10:49, 34.21s/it] 41%|████      | 13/32 [07:28<10:47, 34.08s/it] 41%|████      | 13/32 [07:30<10:52, 34.36s/it] 28%|██▊       | 9/32 [07:49<20:13, 52.78s/it] 28%|██▊       | 9/32 [07:51<19:58, 52.10s/it] 44%|████▍     | 14/32 [07:54<10:06, 33.67s/it] 44%|████▍     | 14/32 [07:58<10:09, 33.84s/it] 44%|████▍     | 14/32 [07:59<10:10, 33.90s/it] 44%|████▍     | 14/32 [08:00<10:15, 34.22s/it] 44%|████▍     | 14/32 [08:03<10:17, 34.29s/it] 44%|████▍     | 14/32 [08:06<10:28, 34.89s/it] 47%|████▋     | 15/32 [08:29<09:38, 34.02s/it] 47%|████▋     | 15/32 [08:32<09:34, 33.80s/it] 47%|████▋     | 15/32 [08:33<09:39, 34.12s/it] 47%|████▋     | 15/32 [08:35<09:45, 34.46s/it] 47%|████▋     | 15/32 [08:37<09:41, 34.22s/it] 47%|████▋     | 15/32 [08:40<09:48, 34.62s/it] 31%|███▏      | 10/32 [08:40<19:15, 52.52s/it] 31%|███▏      | 10/32 [08:42<18:55, 51.60s/it] 50%|█████     | 16/32 [09:04<09:12, 34.53s/it] 50%|█████     | 16/32 [09:06<09:00, 33.79s/it] 50%|█████     | 16/32 [09:07<09:04, 34.01s/it] 50%|█████     | 16/32 [09:09<09:09, 34.32s/it] 50%|█████     | 16/32 [09:11<09:06, 34.14s/it] 50%|█████     | 16/32 [09:16<09:16, 34.78s/it] 34%|███▍      | 11/32 [09:32<18:15, 52.15s/it] 34%|███▍      | 11/32 [09:32<17:56, 51.24s/it] 53%|█████▎    | 17/32 [09:38<08:36, 34.40s/it] 53%|█████▎    | 17/32 [09:40<08:26, 33.77s/it] 53%|█████▎    | 17/32 [09:41<08:28, 33.93s/it] 53%|█████▎    | 17/32 [09:42<08:33, 34.21s/it] 53%|█████▎    | 17/32 [09:45<08:31, 34.11s/it] 53%|█████▎    | 17/32 [09:49<08:37, 34.49s/it] 56%|█████▋    | 18/32 [10:12<07:59, 34.23s/it] 56%|█████▋    | 18/32 [10:14<07:53, 33.85s/it] 56%|█████▋    | 18/32 [10:16<08:01, 34.41s/it] 56%|█████▋    | 18/32 [10:17<07:58, 34.19s/it] 56%|█████▋    | 18/32 [10:20<08:01, 34.36s/it] 38%|███▊      | 12/32 [10:23<17:00, 51.04s/it] 56%|█████▋    | 18/32 [10:24<08:01, 34.38s/it] 38%|███▊      | 12/32 [10:28<17:44, 53.25s/it] 59%|█████▉    | 19/32 [10:48<07:30, 34.65s/it] 59%|█████▉    | 19/32 [10:48<07:19, 33.78s/it] 59%|█████▉    | 19/32 [10:50<07:25, 34.27s/it] 59%|█████▉    | 19/32 [10:51<07:26, 34.36s/it] 59%|█████▉    | 19/32 [10:55<07:29, 34.60s/it] 59%|█████▉    | 19/32 [10:59<07:29, 34.57s/it] 41%|████      | 13/32 [11:13<16:05, 50.84s/it] 41%|████      | 13/32 [11:19<16:40, 52.68s/it] 62%|██████▎   | 20/32 [11:21<06:44, 33.74s/it] 62%|██████▎   | 20/32 [11:22<06:55, 34.65s/it] 62%|██████▎   | 20/32 [11:24<06:49, 34.15s/it] 62%|██████▎   | 20/32 [11:25<06:50, 34.24s/it] 62%|██████▎   | 20/32 [11:29<06:52, 34.41s/it] 62%|██████▎   | 20/32 [11:35<07:02, 35.25s/it] 66%|██████▌   | 21/32 [11:55<06:11, 33.73s/it] 66%|██████▌   | 21/32 [11:56<06:18, 34.37s/it] 66%|██████▌   | 21/32 [11:57<06:14, 34.04s/it] 66%|██████▌   | 21/32 [12:00<06:16, 34.22s/it] 66%|██████▌   | 21/32 [12:03<06:16, 34.20s/it] 44%|████▍     | 14/32 [12:08<15:38, 52.11s/it] 66%|██████▌   | 21/32 [12:11<06:29, 35.43s/it] 44%|████▍     | 14/32 [12:12<15:49, 52.76s/it] 69%|██████▉   | 22/32 [12:29<05:37, 33.77s/it] 69%|██████▉   | 22/32 [12:30<05:41, 34.14s/it] 69%|██████▉   | 22/32 [12:31<05:39, 33.96s/it] 69%|██████▉   | 22/32 [12:33<05:41, 34.13s/it] 69%|██████▉   | 22/32 [12:36<05:40, 34.10s/it] 69%|██████▉   | 22/32 [12:46<05:52, 35.28s/it] 47%|████▋     | 15/32 [12:59<14:38, 51.69s/it] 72%|███████▏  | 23/32 [13:03<05:03, 33.75s/it] 47%|████▋     | 15/32 [13:03<14:48, 52.24s/it] 72%|███████▏  | 23/32 [13:03<05:05, 33.97s/it] 72%|███████▏  | 23/32 [13:05<05:04, 33.84s/it] 72%|███████▏  | 23/32 [13:07<05:06, 34.09s/it] 72%|███████▏  | 23/32 [13:10<05:06, 34.06s/it] 72%|███████▏  | 23/32 [13:20<05:13, 34.86s/it] 75%|███████▌  | 24/32 [13:37<04:31, 33.88s/it] 78%|███████▊  | 25/32 [13:38<03:01, 25.99s/it] 75%|███████▌  | 24/32 [13:39<04:35, 34.44s/it] 75%|███████▌  | 24/32 [13:41<04:32, 34.05s/it] 75%|███████▌  | 24/32 [13:44<04:31, 33.99s/it] 50%|█████     | 16/32 [13:49<13:41, 51.34s/it] 75%|███████▌  | 24/32 [13:54<04:36, 34.56s/it] 50%|█████     | 16/32 [13:56<13:58, 52.43s/it] 78%|███████▊  | 25/32 [14:11<03:56, 33.80s/it] 81%|████████▏ | 26/32 [14:12<02:47, 27.94s/it] 78%|███████▊  | 25/32 [14:15<03:58, 34.07s/it] 78%|███████▊  | 25/32 [14:18<03:57, 33.91s/it] 78%|███████▊  | 25/32 [14:20<04:16, 36.59s/it] 78%|███████▊  | 25/32 [14:28<04:00, 34.38s/it] 53%|█████▎    | 17/32 [14:42<12:53, 51.60s/it] 81%|████████▏ | 26/32 [14:45<03:23, 33.88s/it] 84%|████████▍ | 27/32 [14:46<02:27, 29.41s/it] 53%|█████▎    | 17/32 [14:47<13:00, 52.05s/it] 81%|████████▏ | 26/32 [14:49<03:24, 34.03s/it] 81%|████████▏ | 26/32 [14:52<03:23, 33.87s/it] 81%|████████▏ | 26/32 [15:02<03:47, 37.97s/it] 81%|████████▏ | 26/32 [15:03<03:27, 34.52s/it] 88%|████████▊ | 28/32 [15:20<02:02, 30.65s/it] 84%|████████▍ | 27/32 [15:23<02:55, 35.12s/it] 84%|████████▍ | 27/32 [15:25<02:51, 34.37s/it] 84%|████████▍ | 27/32 [15:26<02:49, 33.94s/it] 56%|█████▋    | 18/32 [15:32<11:58, 51.35s/it] 84%|████████▍ | 27/32 [15:37<02:52, 34.40s/it] 56%|█████▋    | 18/32 [15:38<12:04, 51.78s/it] 84%|████████▍ | 27/32 [15:43<03:15, 39.04s/it] 91%|█████████ | 29/32 [15:55<01:35, 31.88s/it] 88%|████████▊ | 28/32 [15:56<02:18, 34.64s/it] 88%|████████▊ | 28/32 [15:59<02:17, 34.27s/it] 88%|████████▊ | 28/32 [16:00<02:15, 33.98s/it] 88%|████████▊ | 28/32 [16:12<02:18, 34.56s/it] 59%|█████▉    | 19/32 [16:23<11:03, 51.07s/it] 88%|████████▊ | 28/32 [16:24<02:38, 39.68s/it] 94%|█████████▍| 30/32 [16:29<01:05, 32.52s/it] 94%|█████████▍| 30/32 [16:30<00:52, 26.39s/it] 59%|█████▉    | 19/32 [16:30<11:12, 51.70s/it] 91%|█████████ | 29/32 [16:32<01:42, 34.14s/it] 91%|█████████ | 29/32 [16:34<01:41, 33.98s/it] 91%|█████████ | 29/32 [16:46<01:43, 34.36s/it] 97%|█████████▋| 31/32 [17:03<00:28, 28.17s/it] 97%|█████████▋| 31/32 [17:05<00:33, 33.45s/it] 91%|█████████ | 29/32 [17:06<02:00, 40.15s/it] 94%|█████████▍| 30/32 [17:07<01:08, 34.39s/it] 94%|█████████▍| 30/32 [17:09<01:08, 34.30s/it] 62%|██████▎   | 20/32 [17:13<10:10, 50.90s/it] 94%|█████████▍| 30/32 [17:20<01:08, 34.25s/it] 62%|██████▎   | 20/32 [17:21<10:18, 51.51s/it]100%|██████████| 32/32 [17:38<00:00, 33.53s/it]100%|██████████| 32/32 [17:38<00:00, 33.09s/it]
100%|██████████| 32/32 [17:40<00:00, 30.38s/it]100%|██████████| 32/32 [17:40<00:00, 33.14s/it]
 97%|█████████▋| 31/32 [17:41<00:34, 34.27s/it] 97%|█████████▋| 31/32 [17:43<00:34, 34.15s/it] 94%|█████████▍| 30/32 [17:47<01:20, 40.44s/it] 97%|█████████▋| 31/32 [17:56<00:35, 35.01s/it] 66%|██████▌   | 21/32 [18:04<09:18, 50.75s/it] 66%|██████▌   | 21/32 [18:12<09:24, 51.33s/it]100%|██████████| 32/32 [18:15<00:00, 34.08s/it]100%|██████████| 32/32 [18:15<00:00, 34.24s/it]
100%|██████████| 32/32 [18:21<00:00, 35.35s/it]100%|██████████| 32/32 [18:21<00:00, 34.42s/it]
 97%|█████████▋| 31/32 [18:28<00:40, 40.62s/it]100%|██████████| 32/32 [18:30<00:00, 34.67s/it]100%|██████████| 32/32 [18:30<00:00, 34.71s/it]
 69%|██████▉   | 22/32 [18:55<08:27, 50.78s/it] 69%|██████▉   | 22/32 [19:03<08:32, 51.22s/it]100%|██████████| 32/32 [19:09<00:00, 40.69s/it]100%|██████████| 32/32 [19:09<00:00, 35.91s/it]
 72%|███████▏  | 23/32 [19:45<07:35, 50.65s/it] 72%|███████▏  | 23/32 [19:54<07:40, 51.22s/it] 75%|███████▌  | 24/32 [20:38<06:52, 51.52s/it] 75%|███████▌  | 24/32 [20:45<06:49, 51.18s/it] 78%|███████▊  | 25/32 [21:29<05:58, 51.19s/it] 78%|███████▊  | 25/32 [21:38<06:01, 51.67s/it] 81%|████████▏ | 26/32 [22:20<05:06, 51.07s/it] 81%|████████▏ | 26/32 [22:29<05:09, 51.52s/it] 84%|████████▍ | 27/32 [23:10<04:14, 50.84s/it] 84%|████████▍ | 27/32 [23:20<04:16, 51.34s/it] 88%|████████▊ | 28/32 [24:00<03:22, 50.66s/it] 88%|████████▊ | 28/32 [24:11<03:25, 51.30s/it] 91%|█████████ | 29/32 [24:51<02:31, 50.56s/it] 91%|█████████ | 29/32 [25:02<02:33, 51.31s/it] 94%|█████████▍| 30/32 [25:41<01:41, 50.53s/it] 94%|█████████▍| 30/32 [25:53<01:42, 51.23s/it] 97%|█████████▋| 31/32 [26:33<00:50, 50.95s/it] 97%|█████████▋| 31/32 [26:48<00:52, 52.12s/it]100%|██████████| 32/32 [27:23<00:00, 50.79s/it]100%|██████████| 32/32 [27:23<00:00, 51.37s/it]
100%|██████████| 32/32 [27:36<00:00, 50.94s/it]100%|██████████| 32/32 [27:36<00:00, 51.76s/it]
