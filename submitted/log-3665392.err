Already on 'yangexppp'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:05<02:42,  5.59s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:11<02:36,  5.58s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:16<02:33,  5.69s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:22<02:29,  5.76s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:28<02:21,  5.67s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:33<02:13,  5.57s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:39<02:06,  5.50s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:44<02:02,  5.56s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:50<01:58,  5.62s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:55<01:51,  5.56s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [01:01<01:45,  5.53s/it]Loading checkpoint shards:  40%|████      | 12/30 [01:06<01:38,  5.50s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [01:12<01:35,  5.63s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [01:18<01:31,  5.71s/it]Loading checkpoint shards:  50%|█████     | 15/30 [01:24<01:25,  5.68s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [01:29<01:17,  5.56s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [01:34<01:11,  5.47s/it]Loading checkpoint shards:  60%|██████    | 18/30 [01:40<01:05,  5.49s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [01:45<01:00,  5.50s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [01:51<00:54,  5.45s/it]Loading checkpoint shards:  70%|███████   | 21/30 [01:56<00:48,  5.38s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [02:01<00:42,  5.35s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [02:07<00:37,  5.40s/it]Loading checkpoint shards:  80%|████████  | 24/30 [02:12<00:32,  5.45s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [02:17<00:26,  5.36s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [02:23<00:21,  5.36s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [02:28<00:15,  5.31s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [02:34<00:10,  5.39s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [02:39<00:05,  5.42s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:42<00:00,  4.53s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:42<00:00,  5.40s/it]
  0%|          | 0/300 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 1/300 [02:27<12:13:10, 147.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 2/300 [04:53<12:09:34, 146.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 3/300 [07:24<12:15:14, 148.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|▏         | 4/300 [09:58<12:22:47, 150.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 5/300 [12:31<12:25:43, 151.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 6/300 [15:05<12:26:35, 152.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 7/300 [17:41<12:29:25, 153.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 8/300 [20:14<12:27:03, 153.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 9/300 [22:47<12:23:27, 153.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 10/300 [25:19<12:18:26, 152.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  4%|▎         | 11/300 [27:51<12:15:09, 152.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  4%|▍         | 12/300 [30:24<12:13:38, 152.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  4%|▍         | 13/300 [32:58<12:12:52, 153.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▍         | 14/300 [35:35<12:14:36, 154.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▌         | 15/300 [38:09<12:12:12, 154.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▌         | 16/300 [40:46<12:14:33, 155.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  6%|▌         | 17/300 [43:22<12:12:13, 155.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  6%|▌         | 18/300 [46:01<12:15:43, 156.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  6%|▋         | 19/300 [48:39<12:14:12, 156.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 20/300 [51:12<12:06:59, 155.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 21/300 [53:49<12:05:28, 156.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 22/300 [56:24<12:02:18, 155.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  8%|▊         | 23/300 [59:02<12:02:21, 156.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  8%|▊         | 24/300 [1:01:34<11:54:09, 155.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  8%|▊         | 25/300 [1:04:07<11:48:20, 154.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
