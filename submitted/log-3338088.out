Your branch is up to date with 'origin/addinggriffin'.
Already up to date.
Already up to date.
/fsx-storygen/beidic/anaconda3/envs/griffin/bin/python
/fsx-storygen/beidic/anaconda3/envs/griffin/bin/python
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /data/home/beidic/.cache/huggingface/token
Login successful
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=16, spr=0.5, thr=0.01, widthtree=8, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
NCCL_TIMEOUT 1800
NCCL_TIMEOUT 1800
We now use eos_token as pad token
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=16, spr=0.5, thr=0.01, widthtree=8, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
beam width is 8
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=16, spr=0.5, thr=0.01, widthtree=8, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=16, spr=0.5, thr=0.01, widthtree=8, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=16, spr=0.5, thr=0.01, widthtree=8, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=16, spr=0.5, thr=0.01, widthtree=8, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=16, spr=0.5, thr=0.01, widthtree=8, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
We now use eos_token as pad token
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=True, cats=False, check=True, kernel_size=16, spr=0.5, thr=0.01, widthtree=8, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
We now use eos_token as pad token
We now use eos_token as pad token
We now use eos_token as pad token
We now use eos_token as pad token
We now use eos_token as pad token
We now use eos_token as pad token
beam width is 8
beam width is 8
beam width is 8
beam width is 8
beam width is 8
beam width is 8
beam width is 8
tasks ['aqua']
tasks ['aqua']tasks ['aqua']

tasks ['aqua']
tasks ['aqua']
tasks ['aqua']
tasks ['aqua']tasks ['aqua']

Answer a expected e
Answer d expected c
Answer b expected e
Answer e expected b
Answer c expected a
Answer c expected d
Answer a expected b
Answer e expected c
Answer b expected b
Answer a expected e
Answer e expected b
Answer c expected c
Answer b expected b
Answer b expected d
Answer e expected d
Answer d expected d
Answer b expected a
Answer b expected d
Answer e expected b
Answer b expected a
Answer  expected b
Skipping the batch
Answer e expected e
Answer  expected a
Answer a expected a
Answer a expected c
Skipping the batch
Answer a expected a
Answer c expected d
index 6 start communication
Answer b expected c
Answer e expected d
Answer a expected a
index 1 start communication
index 5 start communication
Answer a expected b
index 4 start communication
index 2 start communication
Answer e expected c
index 0 start communication
index 3 start communication
index 7 start communication
Here are the statistics for inference
Here are the statistics for inference
Here are the statistics for inference
Here are the statistics for inference
+--------+----------------+---------------------------+-----------------------------+---------------+-------------+---------+--------------------------------+------------------+----------------------------------+-----------------------+----------------------+
| Task   |   Num Sentence |   Total Generation Length |   Average Generation Length |   Total Steps |   Num Steps |     AAL |   Total Roll Back Length Error |   Error Instance |   Average Roll Back Length Error |   Effective Tree Size |   Drafting Tree Size |
+========+================+===========================+=============================+===============+=============+=========+================================+==================+==================================+=======================+======================+
| aqua   |            254 |                     24260 |                     95.5118 |         20016 |        1345 | 14.8818 |                           1504 |              151 |                          9.96026 |               66.6335 |               7.5625 |
+--------+----------------+---------------------------+-----------------------------+---------------+-------------+---------+--------------------------------+------------------+----------------------------------+-----------------------+----------------------+
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device='cuda:0', limit=None, griffin=True, cats=False, check=True, kernel_size=16, spr=0.5, thr=0.01, widthtree=8, patternstrict=False, shotfive=True, shottwo=False, filteractiveenabled=False)
+--------+---------+-----------+--------------+
| Task   |   Total |   Correct |   Solve Rate |
+========+=========+===========+==============+
| aqua   |     254 |        64 |     0.251969 |
+--------+---------+-----------+--------------+
Here are the statistics for inference
Here are the statistics for inference
Here are the statistics for inference
Here are the statistics for inference
