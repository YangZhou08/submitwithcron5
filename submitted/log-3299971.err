Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.65s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:53<00:53, 53.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:53<00:53, 53.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:53<00:53, 53.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:53<00:53, 53.83s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:54<00:54, 54.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:54<00:54, 54.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:54<00:54, 54.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 28.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.77s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:06<00:00, 29.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.79s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:06<00:00, 33.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.73s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 31.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 34.59s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 31.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 34.74s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:42<22:08, 42.86s/it]  3%|▎         | 1/32 [00:50<25:59, 50.31s/it]  3%|▎         | 1/32 [00:51<26:49, 51.91s/it]  3%|▎         | 1/32 [00:54<27:55, 54.04s/it]  3%|▎         | 1/32 [00:58<30:28, 58.98s/it]  3%|▎         | 1/32 [01:05<33:35, 65.02s/it]  3%|▎         | 1/32 [01:06<34:16, 66.33s/it]  3%|▎         | 1/32 [01:10<36:21, 70.38s/it]  6%|▋         | 2/32 [01:31<23:16, 46.55s/it]  6%|▋         | 2/32 [01:38<24:20, 48.68s/it]  6%|▋         | 2/32 [01:42<25:51, 51.70s/it]  6%|▋         | 2/32 [01:43<25:32, 51.09s/it]  6%|▋         | 2/32 [01:45<25:43, 51.45s/it]  6%|▋         | 2/32 [02:07<31:47, 63.59s/it]  6%|▋         | 2/32 [02:13<33:37, 67.26s/it]  6%|▋         | 2/32 [02:24<36:10, 72.35s/it]  9%|▉         | 3/32 [02:24<22:10, 45.87s/it]  9%|▉         | 3/32 [02:25<23:08, 47.89s/it]  9%|▉         | 3/32 [02:26<22:59, 47.58s/it]  9%|▉         | 3/32 [02:26<24:21, 50.41s/it]  9%|▉         | 3/32 [02:32<24:29, 50.67s/it]  9%|▉         | 3/32 [03:07<29:51, 61.77s/it] 12%|█▎        | 4/32 [03:11<21:48, 46.74s/it] 12%|█▎        | 4/32 [03:12<21:47, 46.71s/it] 12%|█▎        | 4/32 [03:13<22:45, 48.78s/it] 12%|█▎        | 4/32 [03:14<22:31, 48.26s/it] 12%|█▎        | 4/32 [03:28<24:36, 52.72s/it]  9%|▉         | 3/32 [03:28<34:05, 70.53s/it]  9%|▉         | 3/32 [03:39<35:40, 73.81s/it] 16%|█▌        | 5/32 [03:58<21:19, 47.40s/it] 16%|█▌        | 5/32 [04:00<21:23, 47.53s/it] 16%|█▌        | 5/32 [04:00<21:24, 47.58s/it] 16%|█▌        | 5/32 [04:08<22:28, 49.93s/it] 12%|█▎        | 4/32 [04:13<29:31, 63.27s/it] 16%|█▌        | 5/32 [04:18<23:23, 51.97s/it] 12%|█▎        | 4/32 [04:42<33:38, 72.08s/it] 12%|█▎        | 4/32 [04:44<32:50, 70.38s/it] 19%|█▉        | 6/32 [04:47<20:35, 47.54s/it] 19%|█▉        | 6/32 [04:48<20:54, 48.24s/it] 19%|█▉        | 6/32 [04:50<20:52, 48.16s/it] 19%|█▉        | 6/32 [04:54<21:05, 48.68s/it] 16%|█▌        | 5/32 [05:07<26:56, 59.88s/it] 19%|█▉        | 6/32 [05:08<22:10, 51.18s/it] 22%|██▏       | 7/32 [05:36<20:05, 48.23s/it] 22%|██▏       | 7/32 [05:38<20:03, 48.16s/it] 22%|██▏       | 7/32 [05:43<20:18, 48.74s/it] 22%|██▏       | 7/32 [05:46<21:18, 51.13s/it] 22%|██▏       | 7/32 [05:54<20:38, 49.55s/it] 16%|█▌        | 5/32 [05:56<32:41, 72.66s/it] 16%|█▌        | 5/32 [05:59<32:22, 71.93s/it] 19%|█▉        | 6/32 [06:09<26:18, 60.73s/it] 25%|██▌       | 8/32 [06:25<19:24, 48.51s/it] 25%|██▌       | 8/32 [06:28<19:32, 48.84s/it] 25%|██▌       | 8/32 [06:29<19:09, 47.91s/it] 25%|██▌       | 8/32 [06:34<18:33, 46.38s/it] 25%|██▌       | 8/32 [06:38<20:36, 51.54s/it] 22%|██▏       | 7/32 [07:08<25:07, 60.32s/it] 28%|██▊       | 9/32 [07:10<18:13, 47.52s/it] 19%|█▉        | 6/32 [07:12<32:01, 73.90s/it] 28%|██▊       | 9/32 [07:16<18:37, 48.57s/it] 28%|██▊       | 9/32 [07:18<18:34, 48.44s/it] 19%|█▉        | 6/32 [07:25<33:15, 76.73s/it] 28%|██▊       | 9/32 [07:28<18:43, 48.84s/it] 28%|██▊       | 9/32 [07:31<19:54, 51.93s/it] 31%|███▏      | 10/32 [08:00<17:19, 47.23s/it] 31%|███▏      | 10/32 [08:07<18:24, 50.22s/it] 25%|██▌       | 8/32 [08:09<24:07, 60.30s/it] 31%|███▏      | 10/32 [08:11<17:37, 48.07s/it] 31%|███▏      | 10/32 [08:10<18:08, 49.48s/it] 22%|██▏       | 7/32 [08:26<30:48, 73.94s/it] 31%|███▏      | 10/32 [08:30<19:19, 52.72s/it] 22%|██▏       | 7/32 [08:41<31:52, 76.52s/it] 34%|███▍      | 11/32 [08:46<16:23, 46.84s/it] 34%|███▍      | 11/32 [08:55<17:22, 49.64s/it] 28%|██▊       | 9/32 [08:57<21:43, 56.67s/it] 34%|███▍      | 11/32 [09:02<17:34, 50.22s/it] 34%|███▍      | 11/32 [09:03<17:19, 49.51s/it] 34%|███▍      | 11/32 [09:15<17:38, 50.42s/it] 25%|██▌       | 8/32 [09:37<29:09, 72.91s/it] 38%|███▊      | 12/32 [09:39<16:10, 48.54s/it] 38%|███▊      | 12/32 [09:48<16:52, 50.61s/it] 38%|███▊      | 12/32 [09:48<16:03, 48.16s/it] 38%|███▊      | 12/32 [09:51<16:38, 49.90s/it] 31%|███▏      | 10/32 [09:53<20:39, 56.34s/it] 25%|██▌       | 8/32 [09:53<30:02, 75.11s/it] 38%|███▊      | 12/32 [10:08<17:02, 51.14s/it] 41%|████      | 13/32 [10:27<15:19, 48.38s/it] 41%|████      | 13/32 [10:37<15:55, 50.31s/it] 41%|████      | 13/32 [10:40<15:36, 49.27s/it] 41%|████      | 13/32 [10:41<15:46, 49.81s/it] 28%|██▊       | 9/32 [10:50<27:55, 72.85s/it] 34%|███▍      | 11/32 [10:57<20:34, 58.78s/it] 28%|██▊       | 9/32 [11:00<27:46, 72.46s/it] 41%|████      | 13/32 [11:02<16:30, 52.12s/it] 44%|████▍     | 14/32 [11:18<14:44, 49.14s/it] 44%|████▍     | 14/32 [11:23<14:38, 48.79s/it] 44%|████▍     | 14/32 [11:25<14:27, 48.22s/it] 44%|████▍     | 14/32 [11:27<14:31, 48.43s/it] 44%|████▍     | 14/32 [11:47<14:59, 49.99s/it] 38%|███▊      | 12/32 [11:59<19:52, 59.60s/it] 31%|███▏      | 10/32 [12:04<26:53, 73.36s/it] 47%|████▋     | 15/32 [12:06<13:49, 48.81s/it] 47%|████▋     | 15/32 [12:07<13:28, 47.54s/it] 47%|████▋     | 15/32 [12:07<13:06, 46.27s/it] 31%|███▏      | 10/32 [12:12<26:30, 72.30s/it] 47%|████▋     | 15/32 [12:13<13:30, 47.66s/it] 47%|████▋     | 15/32 [12:40<14:25, 50.93s/it] 50%|█████     | 16/32 [12:56<12:46, 47.91s/it] 50%|█████     | 16/32 [12:57<13:12, 49.53s/it] 50%|█████     | 16/32 [12:58<12:41, 47.61s/it] 41%|████      | 13/32 [13:03<19:18, 60.99s/it] 34%|███▍      | 11/32 [13:08<24:40, 70.48s/it] 50%|█████     | 16/32 [13:13<13:44, 51.53s/it] 34%|███▍      | 11/32 [13:20<24:51, 71.02s/it] 50%|█████     | 16/32 [13:30<13:30, 50.65s/it] 53%|█████▎    | 17/32 [13:42<11:48, 47.22s/it] 53%|█████▎    | 17/32 [13:46<11:56, 47.78s/it] 53%|█████▎    | 17/32 [13:54<12:58, 51.92s/it] 53%|█████▎    | 17/32 [14:00<12:30, 50.03s/it] 44%|████▍     | 14/32 [14:03<18:12, 60.71s/it] 38%|███▊      | 12/32 [14:12<22:51, 68.58s/it] 53%|█████▎    | 17/32 [14:20<12:34, 50.31s/it] 56%|█████▋    | 18/32 [14:27<10:51, 46.53s/it] 56%|█████▋    | 18/32 [14:34<11:11, 47.94s/it] 38%|███▊      | 12/32 [14:37<24:18, 72.93s/it] 56%|█████▋    | 18/32 [14:48<11:32, 49.46s/it] 56%|█████▋    | 18/32 [14:49<12:17, 52.69s/it] 47%|████▋     | 15/32 [14:57<16:38, 58.72s/it] 56%|█████▋    | 18/32 [15:05<11:23, 48.85s/it] 59%|█████▉    | 19/32 [15:14<10:08, 46.83s/it] 41%|████      | 13/32 [15:18<21:26, 67.70s/it] 59%|█████▉    | 19/32 [15:24<10:30, 48.48s/it] 59%|█████▉    | 19/32 [15:29<10:34, 48.82s/it] 59%|█████▉    | 19/32 [15:37<10:40, 49.25s/it] 59%|█████▉    | 19/32 [15:52<10:26, 48.22s/it] 50%|█████     | 16/32 [15:53<15:25, 57.84s/it] 41%|████      | 13/32 [15:53<23:19, 73.68s/it] 62%|██████▎   | 20/32 [15:59<09:14, 46.17s/it] 62%|██████▎   | 20/32 [16:15<09:34, 47.91s/it] 62%|██████▎   | 20/32 [16:20<10:07, 50.59s/it] 44%|████▍     | 14/32 [16:22<20:00, 66.68s/it] 62%|██████▎   | 20/32 [16:24<09:43, 48.64s/it] 66%|██████▌   | 21/32 [16:42<08:20, 45.46s/it] 62%|██████▎   | 20/32 [16:44<09:54, 49.52s/it] 53%|█████▎    | 17/32 [16:48<14:16, 57.10s/it] 66%|██████▌   | 21/32 [17:01<08:42, 47.48s/it] 44%|████▍     | 14/32 [17:05<21:57, 73.17s/it] 66%|██████▌   | 21/32 [17:07<09:05, 49.56s/it] 66%|██████▌   | 21/32 [17:20<09:18, 50.77s/it] 66%|██████▌   | 21/32 [17:31<08:54, 48.55s/it] 69%|██████▉   | 22/32 [17:31<07:44, 46.46s/it] 47%|████▋     | 15/32 [17:36<19:28, 68.76s/it] 56%|█████▋    | 18/32 [17:45<13:17, 56.99s/it] 69%|██████▉   | 22/32 [17:50<07:59, 47.94s/it] 69%|██████▉   | 22/32 [17:56<08:15, 49.55s/it] 69%|██████▉   | 22/32 [18:09<08:23, 50.38s/it] 47%|████▋     | 15/32 [18:09<20:00, 70.61s/it] 72%|███████▏  | 23/32 [18:18<06:58, 46.45s/it] 69%|██████▉   | 22/32 [18:18<08:02, 48.28s/it] 72%|███████▏  | 23/32 [18:39<07:15, 48.35s/it] 59%|█████▉    | 19/32 [18:45<12:31, 57.81s/it] 50%|█████     | 16/32 [18:45<18:22, 68.91s/it] 72%|███████▏  | 23/32 [18:49<07:33, 50.44s/it] 72%|███████▏  | 23/32 [18:56<07:25, 49.48s/it] 75%|███████▌  | 24/32 [19:03<06:08, 46.08s/it] 72%|███████▏  | 23/32 [19:04<07:08, 47.58s/it] 50%|█████     | 16/32 [19:28<19:29, 73.09s/it] 75%|███████▌  | 24/32 [19:29<06:29, 48.75s/it] 78%|███████▊  | 25/32 [19:39<04:31, 38.83s/it] 62%|██████▎   | 20/32 [19:40<11:25, 57.12s/it] 75%|███████▌  | 24/32 [19:40<06:22, 47.80s/it] 75%|███████▌  | 24/32 [19:42<05:58, 44.76s/it] 53%|█████▎    | 17/32 [19:56<17:22, 69.48s/it] 78%|███████▊  | 25/32 [19:57<05:38, 48.35s/it] 78%|███████▊  | 25/32 [20:15<05:35, 47.90s/it] 81%|████████▏ | 26/32 [20:27<04:05, 40.96s/it] 78%|███████▊  | 25/32 [20:28<05:35, 47.90s/it] 78%|███████▊  | 25/32 [20:37<05:34, 47.79s/it] 53%|█████▎    | 17/32 [20:38<18:04, 72.27s/it] 66%|██████▌   | 21/32 [20:45<10:52, 59.34s/it] 81%|████████▏ | 26/32 [20:49<04:58, 49.68s/it] 81%|████████▏ | 26/32 [21:01<04:43, 47.33s/it] 56%|█████▋    | 18/32 [21:10<16:32, 70.90s/it] 84%|████████▍ | 27/32 [21:15<03:34, 42.87s/it] 81%|████████▏ | 26/32 [21:23<04:59, 49.95s/it] 81%|████████▏ | 26/32 [21:28<04:51, 48.52s/it] 84%|████████▍ | 27/32 [21:38<04:06, 49.34s/it] 84%|████████▍ | 27/32 [21:44<03:49, 45.98s/it] 69%|██████▉   | 22/32 [21:45<09:55, 59.50s/it] 56%|█████▋    | 18/32 [21:51<16:51, 72.25s/it] 88%|████████▊ | 28/32 [22:02<02:55, 43.88s/it] 84%|████████▍ | 27/32 [22:09<04:03, 48.64s/it] 84%|████████▍ | 27/32 [22:17<04:03, 48.67s/it] 59%|█████▉    | 19/32 [22:23<15:27, 71.37s/it] 88%|████████▊ | 28/32 [22:33<03:08, 47.08s/it] 88%|████████▊ | 28/32 [22:36<03:27, 51.82s/it] 72%|███████▏  | 23/32 [22:45<08:56, 59.61s/it] 91%|█████████ | 29/32 [22:57<02:21, 47.17s/it] 88%|████████▊ | 28/32 [23:02<03:19, 49.94s/it] 88%|████████▊ | 28/32 [23:06<03:16, 49.05s/it] 59%|█████▉    | 19/32 [23:10<16:06, 74.33s/it] 91%|█████████ | 29/32 [23:23<02:23, 47.78s/it] 94%|█████████▍| 30/32 [23:24<01:18, 39.06s/it] 75%|███████▌  | 24/32 [23:35<07:34, 56.76s/it] 62%|██████▎   | 20/32 [23:39<14:33, 72.78s/it] 94%|█████████▍| 30/32 [23:46<01:35, 47.65s/it] 91%|█████████ | 29/32 [23:52<02:24, 48.04s/it] 91%|█████████ | 29/32 [23:55<02:33, 51.07s/it] 97%|█████████▋| 31/32 [24:12<00:41, 41.42s/it] 94%|█████████▍| 30/32 [24:14<01:37, 48.90s/it] 62%|██████▎   | 20/32 [24:23<14:48, 74.03s/it] 78%|███████▊  | 25/32 [24:32<06:38, 56.89s/it] 97%|█████████▋| 31/32 [24:32<00:47, 47.22s/it] 94%|█████████▍| 30/32 [24:40<01:36, 48.09s/it] 94%|█████████▍| 30/32 [24:46<01:41, 50.85s/it]100%|██████████| 32/32 [24:57<00:00, 42.32s/it]100%|██████████| 32/32 [24:57<00:00, 46.81s/it]
 66%|██████▌   | 21/32 [24:58<13:43, 74.88s/it] 97%|█████████▋| 31/32 [25:06<00:49, 49.89s/it]100%|██████████| 32/32 [25:21<00:00, 47.71s/it]100%|██████████| 32/32 [25:21<00:00, 47.55s/it]
 97%|█████████▋| 31/32 [25:26<00:47, 47.32s/it] 97%|█████████▋| 31/32 [25:35<00:50, 50.24s/it] 81%|████████▏ | 26/32 [25:36<05:53, 58.97s/it] 66%|██████▌   | 21/32 [25:42<13:50, 75.48s/it]100%|██████████| 32/32 [25:46<00:00, 46.70s/it]100%|██████████| 32/32 [25:46<00:00, 48.32s/it]
 69%|██████▉   | 22/32 [26:07<12:11, 73.10s/it]100%|██████████| 32/32 [26:15<00:00, 47.93s/it]100%|██████████| 32/32 [26:15<00:00, 49.24s/it]
 84%|████████▍ | 27/32 [26:33<04:51, 58.36s/it]100%|██████████| 32/32 [26:33<00:00, 52.60s/it]100%|██████████| 32/32 [26:33<00:00, 49.78s/it]
 69%|██████▉   | 22/32 [26:48<12:05, 72.56s/it] 72%|███████▏  | 23/32 [27:20<10:55, 72.88s/it] 88%|████████▊ | 28/32 [27:32<03:55, 58.77s/it] 72%|███████▏  | 23/32 [28:01<10:55, 72.82s/it] 75%|███████▌  | 24/32 [28:17<09:04, 68.04s/it] 91%|█████████ | 29/32 [28:20<02:45, 55.32s/it] 75%|███████▌  | 24/32 [29:06<09:23, 70.45s/it] 94%|█████████▍| 30/32 [29:15<01:50, 55.24s/it] 78%|███████▊  | 25/32 [29:28<08:03, 69.02s/it] 97%|█████████▋| 31/32 [30:03<00:53, 53.13s/it] 78%|███████▊  | 25/32 [30:14<08:08, 69.80s/it] 81%|████████▏ | 26/32 [30:42<07:04, 70.68s/it]100%|██████████| 32/32 [30:46<00:00, 50.05s/it]100%|██████████| 32/32 [30:46<00:00, 57.69s/it]
 81%|████████▏ | 26/32 [31:29<07:08, 71.38s/it] 84%|████████▍ | 27/32 [31:57<05:58, 71.75s/it]