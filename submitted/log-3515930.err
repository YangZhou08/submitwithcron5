Already on 'yangexp2two'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-24:08:49:18,603 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:49:18,725 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:49:18,764 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:49:18,831 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:49:18,836 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:49:18,858 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:49:18,870 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:49:18,876 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:49:25,496 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:49:25,496 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:49:25,503 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:49:25,503 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 2, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:49:25,504 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:49:25,504 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 2, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:49:25,634 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:49:25,640 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:49:25,640 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 2, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:49:25,651 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:49:25,657 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:49:25,658 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 2, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:49:25,698 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:49:25,704 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:49:25,704 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 2, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:49:25,756 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:49:25,763 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:49:25,763 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 2, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:49:25,899 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:49:25,905 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:49:25,905 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 2, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:49:26,168 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:49:26,174 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:49:26,174 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 2, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.44s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.39s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.51s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.45s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:06,  3.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.40s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.79s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.83s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.77s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.81s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.78s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.79s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.90s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.01s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-24:08:50:19,005 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/165 [00:00<?, ?it/s]2024-07-24:08:50:19,098 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.81it/s]2024-07-24:08:50:19,179 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.53it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.38it/s]2024-07-24:08:50:19,295 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
 12%|█▏        | 19/165 [00:00<00:00, 188.37it/s]  0%|          | 0/165 [00:00<?, ?it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.30it/s] 36%|███▌      | 59/165 [00:00<00:00, 191.77it/s] 23%|██▎       | 38/165 [00:00<00:00, 188.84it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.86it/s]2024-07-24:08:50:19,423 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 36%|███▌      | 59/165 [00:00<00:00, 191.78it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.38it/s]  0%|          | 0/165 [00:00<?, ?it/s] 35%|███▌      | 58/165 [00:00<00:00, 189.42it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.15it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.57it/s] 60%|██████    | 99/165 [00:00<00:00, 193.05it/s] 12%|█▏        | 20/165 [00:00<00:00, 191.93it/s] 47%|████▋     | 78/165 [00:00<00:00, 190.24it/s] 36%|███▌      | 59/165 [00:00<00:00, 191.65it/s] 60%|██████    | 99/165 [00:00<00:00, 193.08it/s] 72%|███████▏  | 119/165 [00:00<00:00, 193.51it/s] 24%|██▍       | 40/165 [00:00<00:00, 193.35it/s] 59%|█████▉    | 98/165 [00:00<00:00, 191.12it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.69it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 72%|███████▏  | 119/165 [00:00<00:00, 193.50it/s]2024-07-24:08:50:19,739 INFO     [xhuggingface.py:323] Using 8 devices with data parallelism
 84%|████████▍ | 139/165 [00:00<00:00, 193.86it/s] 36%|███▋      | 60/165 [00:00<00:00, 193.78it/s]2024-07-24:08:50:19,777 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/164 [00:00<?, ?it/s] 72%|███████▏  | 118/165 [00:00<00:00, 191.55it/s] 60%|██████    | 99/165 [00:00<00:00, 193.33it/s] 84%|████████▍ | 139/165 [00:00<00:00, 193.79it/s] 96%|█████████▋| 159/165 [00:00<00:00, 193.79it/s] 48%|████▊     | 80/165 [00:00<00:00, 194.63it/s]100%|██████████| 165/165 [00:00<00:00, 193.10it/s]
 12%|█▏        | 20/164 [00:00<00:00, 190.91it/s] 84%|████████▎ | 138/165 [00:00<00:00, 191.86it/s] 72%|███████▏  | 119/165 [00:00<00:00, 193.73it/s] 96%|█████████▋| 159/165 [00:00<00:00, 193.82it/s] 61%|██████    | 100/165 [00:00<00:00, 195.19it/s]100%|██████████| 165/165 [00:00<00:00, 193.09it/s]
 24%|██▍       | 40/164 [00:00<00:00, 192.20it/s] 96%|█████████▌| 158/165 [00:00<00:00, 191.89it/s] 84%|████████▍ | 139/165 [00:00<00:00, 194.06it/s] 73%|███████▎  | 120/165 [00:00<00:00, 195.63it/s]100%|██████████| 165/165 [00:00<00:00, 191.12it/s]
 37%|███▋      | 60/164 [00:00<00:00, 192.49it/s] 96%|█████████▋| 159/165 [00:00<00:00, 194.09it/s] 85%|████████▍ | 140/165 [00:00<00:00, 195.16it/s]100%|██████████| 165/165 [00:00<00:00, 193.29it/s]
 49%|████▉     | 80/164 [00:00<00:00, 193.24it/s] 97%|█████████▋| 160/165 [00:00<00:00, 195.33it/s]100%|██████████| 165/165 [00:00<00:00, 194.88it/s]
 61%|██████    | 100/164 [00:00<00:00, 193.80it/s] 73%|███████▎  | 120/164 [00:00<00:00, 194.18it/s] 85%|████████▌ | 140/164 [00:00<00:00, 194.29it/s] 98%|█████████▊| 160/164 [00:00<00:00, 192.43it/s]100%|██████████| 164/164 [00:00<00:00, 192.95it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-24:08:50:20,907 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 187.30it/s] 23%|██▎       | 38/165 [00:00<00:00, 188.60it/s] 35%|███▍      | 57/165 [00:00<00:00, 189.03it/s] 47%|████▋     | 77/165 [00:00<00:00, 189.75it/s] 59%|█████▉    | 97/165 [00:00<00:00, 190.37it/s] 71%|███████   | 117/165 [00:00<00:00, 188.40it/s] 83%|████████▎ | 137/165 [00:00<00:00, 189.00it/s] 95%|█████████▍| 156/165 [00:00<00:00, 189.20it/s]100%|██████████| 165/165 [00:00<00:00, 189.16it/s]
2024-07-24:08:50:22,035 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 19/165 [00:00<00:00, 188.43it/s] 24%|██▎       | 39/165 [00:00<00:00, 189.82it/s] 35%|███▌      | 58/165 [00:00<00:00, 189.73it/s] 47%|████▋     | 77/165 [00:00<00:00, 183.09it/s] 59%|█████▉    | 97/165 [00:00<00:00, 185.85it/s] 70%|███████   | 116/165 [00:00<00:00, 160.35it/s] 81%|████████  | 133/165 [00:00<00:00, 145.23it/s] 93%|█████████▎| 153/165 [00:00<00:00, 158.03it/s]100%|██████████| 165/165 [00:00<00:00, 168.17it/s]
2024-07-24:08:50:36,881 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:50:36,881 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:50:36,881 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:50:36,881 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:50:36,881 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:50:36,881 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:50:36,881 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:50:36,882 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/165 [00:12<35:08, 12.86s/it]Running generate_until requests:   1%|          | 2/165 [00:19<25:15,  9.30s/it]Running generate_until requests:   2%|▏         | 3/165 [00:31<28:30, 10.56s/it]Running generate_until requests:   2%|▏         | 4/165 [00:39<25:01,  9.33s/it]Running generate_until requests:   3%|▎         | 5/165 [00:43<19:57,  7.48s/it]Running generate_until requests:   4%|▎         | 6/165 [00:56<25:06,  9.47s/it]Running generate_until requests:   4%|▍         | 7/165 [01:00<20:19,  7.72s/it]Running generate_until requests:   4%|▍         | 7/165 [01:05<24:27,  9.29s/it]
