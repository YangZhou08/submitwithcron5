Your branch is up to date with 'origin/addinggriffin'.
Updating a3ab251..c553cb6
Fast-forward
 llama12addingtree.py |  8 +++++---
 main.py              | 15 +++++++++++++++
 2 files changed, 20 insertions(+), 3 deletions(-)
/fsx-storygen/beidic/anaconda3/envs/griffin/bin/python
/fsx-storygen/beidic/anaconda3/envs/griffin/bin/python
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /data/home/beidic/.cache/huggingface/token
Login successful
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=False, cats=True, check=True, kernel_size=16, spr=0.5, thr=0.1, widthtree=1, patternstrict=True, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=False, cats=True, check=True, kernel_size=16, spr=0.5, thr=0.1, widthtree=1, patternstrict=True, shotfive=True, shottwo=False, filteractiveenabled=False)
We now use eos_token as pad token
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=False, cats=True, check=True, kernel_size=16, spr=0.5, thr=0.1, widthtree=1, patternstrict=True, shotfive=True, shottwo=False, filteractiveenabled=False)
We now use eos_token as pad token
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=False, cats=True, check=True, kernel_size=16, spr=0.5, thr=0.1, widthtree=1, patternstrict=True, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=False, cats=True, check=True, kernel_size=16, spr=0.5, thr=0.1, widthtree=1, patternstrict=True, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=False, cats=True, check=True, kernel_size=16, spr=0.5, thr=0.1, widthtree=1, patternstrict=True, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=False, cats=True, check=True, kernel_size=16, spr=0.5, thr=0.1, widthtree=1, patternstrict=True, shotfive=True, shottwo=False, filteractiveenabled=False)
is_distributed True
Namespace(tasks='aqua', model='meta-llama/Llama-2-7b-chat-hf', device=None, limit=None, griffin=False, cats=True, check=True, kernel_size=16, spr=0.5, thr=0.1, widthtree=1, patternstrict=True, shotfive=True, shottwo=False, filteractiveenabled=False)
We now use eos_token as pad token
We now use eos_token as pad tokenWe now use eos_token as pad token

We now use eos_token as pad token
We now use eos_token as pad token
We now use eos_token as pad token
beam width is 8
beam width is 8
beam width is 8
beam width is 8
beam width is 8
beam width is 8
beam width is 8
beam width is 8
tasks ['aqua']tasks ['aqua']tasks ['aqua']tasks ['aqua']
tasks ['aqua']


tasks ['aqua']

tasks ['aqua']
tasks ['aqua']
beamwidth is 1
beamwidth is 1beamwidth is 1

beamwidth is 1
beamwidth is 1
beamwidth is 1
beamwidth is 1
beamwidth is 1
Answer c expected e
Answer e expected c
Answer e expected e
Answer  expected b
Answer c expected a
Answer a expected d
Answer b expected b
Answer c expected c
Answer c expected b
Answer e expected e
Answer e expected b
Answer c expected c
Answer b expected b
Skipping the batch
Answer b expected d
Answer e expected d
Answer d expected d
