Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:44<00:44, 44.44s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.53s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:46<00:46, 46.14s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.78s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:44<00:44, 44.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:59<00:00, 27.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:59<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 27.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 27.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 27.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.38s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:59<00:00, 27.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:59<00:00, 29.73s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 27.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 27.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.50s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 27.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.44s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:38<19:50, 38.39s/it]  3%|▎         | 1/32 [00:39<20:12, 39.11s/it]  3%|▎         | 1/32 [00:40<20:42, 40.08s/it]  3%|▎         | 1/32 [00:40<20:52, 40.39s/it]  3%|▎         | 1/32 [00:41<21:28, 41.56s/it]  3%|▎         | 1/32 [00:46<23:48, 46.08s/it]  3%|▎         | 1/32 [00:56<29:22, 56.86s/it]  3%|▎         | 1/32 [01:01<31:39, 61.28s/it]  6%|▋         | 2/32 [01:16<19:01, 38.04s/it]  6%|▋         | 2/32 [01:16<19:09, 38.33s/it]  6%|▋         | 2/32 [01:16<19:10, 38.34s/it]  6%|▋         | 2/32 [01:20<19:59, 39.99s/it]  6%|▋         | 2/32 [01:20<20:04, 40.16s/it]  6%|▋         | 2/32 [01:29<22:15, 44.51s/it]  6%|▋         | 2/32 [01:51<27:51, 55.73s/it]  9%|▉         | 3/32 [01:53<18:01, 37.29s/it]  9%|▉         | 3/32 [01:53<18:07, 37.50s/it]  9%|▉         | 3/32 [01:53<18:10, 37.62s/it]  9%|▉         | 3/32 [01:56<18:36, 38.52s/it]  6%|▋         | 2/32 [01:57<29:02, 58.10s/it]  9%|▉         | 3/32 [01:57<18:41, 38.68s/it]  9%|▉         | 3/32 [02:14<21:36, 44.71s/it] 12%|█▎        | 4/32 [02:29<17:12, 36.86s/it] 12%|█▎        | 4/32 [02:29<17:14, 36.95s/it] 12%|█▎        | 4/32 [02:30<17:24, 37.31s/it] 12%|█▎        | 4/32 [02:33<17:34, 37.65s/it] 12%|█▎        | 4/32 [02:34<17:48, 38.14s/it]  9%|▉         | 3/32 [02:44<26:15, 54.32s/it]  9%|▉         | 3/32 [02:50<26:59, 55.83s/it] 12%|█▎        | 4/32 [02:58<20:45, 44.47s/it] 16%|█▌        | 5/32 [03:05<16:27, 36.59s/it] 16%|█▌        | 5/32 [03:06<16:41, 37.10s/it] 16%|█▌        | 5/32 [03:07<16:43, 37.18s/it] 16%|█▌        | 5/32 [03:10<16:52, 37.51s/it] 16%|█▌        | 5/32 [03:11<16:56, 37.66s/it] 12%|█▎        | 4/32 [03:40<25:41, 55.04s/it] 19%|█▉        | 6/32 [03:41<15:49, 36.50s/it] 16%|█▌        | 5/32 [03:42<19:51, 44.15s/it] 12%|█▎        | 4/32 [03:43<25:36, 54.86s/it] 19%|█▉        | 6/32 [03:44<16:07, 37.21s/it] 19%|█▉        | 6/32 [03:44<16:08, 37.23s/it] 19%|█▉        | 6/32 [03:46<16:06, 37.16s/it] 19%|█▉        | 6/32 [03:48<16:10, 37.31s/it] 22%|██▏       | 7/32 [04:18<15:20, 36.81s/it] 22%|██▏       | 7/32 [04:20<15:25, 37.04s/it] 22%|██▏       | 7/32 [04:22<15:33, 37.36s/it] 22%|██▏       | 7/32 [04:23<15:25, 37.00s/it] 22%|██▏       | 7/32 [04:24<15:24, 36.98s/it] 19%|█▉        | 6/32 [04:25<19:03, 43.98s/it] 16%|█▌        | 5/32 [04:35<24:43, 54.93s/it] 16%|█▌        | 5/32 [04:38<24:45, 55.03s/it] 25%|██▌       | 8/32 [04:55<14:40, 36.67s/it] 25%|██▌       | 8/32 [05:00<14:43, 36.82s/it] 25%|██▌       | 8/32 [05:01<14:48, 37.03s/it] 25%|██▌       | 8/32 [05:02<15:22, 38.44s/it] 25%|██▌       | 8/32 [05:02<15:21, 38.38s/it] 22%|██▏       | 7/32 [05:09<18:16, 43.84s/it] 19%|█▉        | 6/32 [05:29<23:41, 54.67s/it] 19%|█▉        | 6/32 [05:31<23:28, 54.17s/it] 28%|██▊       | 9/32 [05:32<14:06, 36.81s/it] 28%|██▊       | 9/32 [05:36<14:04, 36.72s/it] 28%|██▊       | 9/32 [05:38<14:29, 37.79s/it] 28%|██▊       | 9/32 [05:39<14:30, 37.83s/it] 28%|██▊       | 9/32 [05:41<14:32, 37.94s/it] 25%|██▌       | 8/32 [05:54<17:40, 44.17s/it] 31%|███▏      | 10/32 [06:08<13:25, 36.59s/it] 31%|███▏      | 10/32 [06:13<13:27, 36.70s/it] 31%|███▏      | 10/32 [06:14<13:38, 37.22s/it] 31%|███▏      | 10/32 [06:16<13:42, 37.40s/it] 31%|███▏      | 10/32 [06:21<14:04, 38.39s/it] 22%|██▏       | 7/32 [06:25<22:29, 53.98s/it] 22%|██▏       | 7/32 [06:28<23:25, 56.20s/it] 28%|██▊       | 9/32 [06:35<16:33, 43.19s/it] 34%|███▍      | 11/32 [06:46<12:54, 36.87s/it] 34%|███▍      | 11/32 [06:50<12:55, 36.91s/it] 34%|███▍      | 11/32 [06:52<13:01, 37.24s/it] 34%|███▍      | 11/32 [06:56<13:34, 38.77s/it] 34%|███▍      | 11/32 [07:03<13:53, 39.71s/it] 31%|███▏      | 10/32 [07:11<15:00, 40.95s/it] 25%|██▌       | 8/32 [07:20<21:48, 54.52s/it] 38%|███▊      | 12/32 [07:23<12:19, 36.96s/it] 25%|██▌       | 8/32 [07:23<22:17, 55.74s/it] 38%|███▊      | 12/32 [07:30<12:26, 37.34s/it] 38%|███▊      | 12/32 [07:32<12:47, 38.37s/it] 38%|███▊      | 12/32 [07:32<12:40, 38.03s/it] 38%|███▊      | 12/32 [07:41<13:01, 39.07s/it] 34%|███▍      | 11/32 [07:48<13:54, 39.75s/it] 41%|████      | 13/32 [08:05<12:11, 38.52s/it] 41%|████      | 13/32 [08:07<11:50, 37.39s/it] 41%|████      | 13/32 [08:08<11:57, 37.75s/it] 41%|████      | 13/32 [08:09<11:56, 37.69s/it] 28%|██▊       | 9/32 [08:14<20:46, 54.19s/it] 28%|██▊       | 9/32 [08:17<21:11, 55.26s/it] 41%|████      | 13/32 [08:20<12:22, 39.08s/it] 38%|███▊      | 12/32 [08:24<12:53, 38.67s/it] 44%|████▍     | 14/32 [08:41<11:20, 37.82s/it] 44%|████▍     | 14/32 [08:45<11:12, 37.35s/it] 44%|████▍     | 14/32 [08:46<11:12, 37.36s/it] 44%|████▍     | 14/32 [08:52<11:50, 39.49s/it] 44%|████▍     | 14/32 [08:56<11:27, 38.19s/it] 41%|████      | 13/32 [09:00<12:01, 37.95s/it] 31%|███▏      | 10/32 [09:08<19:50, 54.09s/it] 31%|███▏      | 10/32 [09:11<20:02, 54.67s/it] 47%|████▋     | 15/32 [09:18<10:38, 37.54s/it] 47%|████▋     | 15/32 [09:21<10:30, 37.07s/it] 47%|████▋     | 15/32 [09:22<10:30, 37.09s/it] 47%|████▋     | 15/32 [09:33<10:41, 37.72s/it] 47%|████▋     | 15/32 [09:37<11:40, 41.22s/it] 44%|████▍     | 14/32 [09:39<11:25, 38.10s/it] 50%|█████     | 16/32 [09:55<09:59, 37.44s/it] 50%|█████     | 16/32 [09:58<09:49, 36.87s/it] 50%|█████     | 16/32 [09:59<09:52, 37.06s/it] 34%|███▍      | 11/32 [10:04<18:58, 54.20s/it] 34%|███▍      | 11/32 [10:04<19:13, 54.91s/it] 50%|█████     | 16/32 [10:10<09:58, 37.40s/it] 47%|████▋     | 15/32 [10:15<10:38, 37.57s/it] 50%|█████     | 16/32 [10:25<11:33, 43.36s/it] 53%|█████▎    | 17/32 [10:33<09:21, 37.44s/it] 53%|█████▎    | 17/32 [10:35<09:13, 36.92s/it] 53%|█████▎    | 17/32 [10:36<09:14, 36.97s/it] 53%|█████▎    | 17/32 [10:46<09:18, 37.25s/it] 50%|█████     | 16/32 [10:53<10:00, 37.56s/it] 38%|███▊      | 12/32 [11:00<18:22, 55.11s/it] 38%|███▊      | 12/32 [11:01<18:24, 55.20s/it] 53%|█████▎    | 17/32 [11:09<10:52, 43.49s/it] 56%|█████▋    | 18/32 [11:10<08:43, 37.42s/it] 56%|█████▋    | 18/32 [11:12<08:40, 37.15s/it] 56%|█████▋    | 18/32 [11:14<08:42, 37.29s/it] 56%|█████▋    | 18/32 [11:28<09:01, 38.65s/it] 53%|█████▎    | 17/32 [11:33<09:36, 38.41s/it] 59%|█████▉    | 19/32 [11:46<08:01, 37.06s/it] 56%|█████▋    | 18/32 [11:47<09:43, 41.64s/it] 59%|█████▉    | 19/32 [11:49<07:59, 36.89s/it] 59%|█████▉    | 19/32 [11:51<08:01, 37.02s/it] 41%|████      | 13/32 [11:53<17:17, 54.61s/it] 41%|████      | 13/32 [11:55<17:17, 54.62s/it] 59%|█████▉    | 19/32 [12:05<08:15, 38.14s/it] 56%|█████▋    | 18/32 [12:10<08:50, 37.92s/it] 62%|██████▎   | 20/32 [12:23<07:23, 36.94s/it] 62%|██████▎   | 20/32 [12:25<07:21, 36.76s/it] 59%|█████▉    | 19/32 [12:25<08:50, 40.83s/it] 62%|██████▎   | 20/32 [12:27<07:21, 36.77s/it] 62%|██████▎   | 20/32 [12:42<07:31, 37.66s/it] 59%|█████▉    | 19/32 [12:46<08:05, 37.33s/it] 44%|████▍     | 14/32 [12:47<16:19, 54.41s/it] 44%|████▍     | 14/32 [12:48<16:17, 54.28s/it] 66%|██████▌   | 21/32 [13:00<06:48, 37.12s/it] 66%|██████▌   | 21/32 [13:02<06:44, 36.74s/it] 62%|██████▎   | 20/32 [13:02<07:54, 39.54s/it] 66%|██████▌   | 21/32 [13:03<06:43, 36.66s/it] 66%|██████▌   | 21/32 [13:18<06:49, 37.23s/it] 62%|██████▎   | 20/32 [13:26<07:40, 38.35s/it] 69%|██████▉   | 22/32 [13:37<06:08, 36.85s/it] 66%|██████▌   | 21/32 [13:38<07:04, 38.55s/it] 69%|██████▉   | 22/32 [13:39<06:08, 36.90s/it] 69%|██████▉   | 22/32 [13:40<06:06, 36.66s/it] 47%|████▋     | 15/32 [13:41<15:18, 54.00s/it] 47%|████▋     | 15/32 [13:43<15:28, 54.65s/it] 69%|██████▉   | 22/32 [13:59<06:23, 38.35s/it] 66%|██████▌   | 21/32 [14:04<06:58, 38.05s/it] 72%|███████▏  | 23/32 [14:13<05:29, 36.62s/it] 69%|██████▉   | 22/32 [14:15<06:19, 37.96s/it] 72%|███████▏  | 23/32 [14:16<05:31, 36.78s/it] 72%|███████▏  | 23/32 [14:16<05:28, 36.55s/it] 72%|███████▏  | 23/32 [14:36<05:40, 37.82s/it] 50%|█████     | 16/32 [14:36<14:27, 54.21s/it] 50%|█████     | 16/32 [14:36<14:30, 54.43s/it] 69%|██████▉   | 22/32 [14:41<06:18, 37.81s/it] 78%|███████▊  | 25/32 [14:49<03:16, 28.05s/it] 72%|███████▏  | 23/32 [14:52<05:38, 37.59s/it] 75%|███████▌  | 24/32 [14:52<04:52, 36.62s/it] 75%|███████▌  | 24/32 [14:55<04:57, 37.17s/it] 75%|███████▌  | 24/32 [15:13<05:01, 37.71s/it] 72%|███████▏  | 23/32 [15:24<05:54, 39.39s/it] 81%|████████▏ | 26/32 [15:26<03:02, 30.40s/it] 75%|███████▌  | 24/32 [15:28<04:57, 37.21s/it] 78%|███████▊  | 25/32 [15:32<04:19, 37.04s/it] 53%|█████▎    | 17/32 [15:32<13:40, 54.71s/it] 53%|█████▎    | 17/32 [15:32<13:42, 54.80s/it] 78%|███████▊  | 25/32 [15:33<04:25, 37.89s/it] 78%|███████▊  | 25/32 [15:49<04:20, 37.26s/it] 84%|████████▍ | 27/32 [16:03<02:39, 31.92s/it] 78%|███████▊  | 25/32 [16:05<04:19, 37.11s/it] 81%|████████▏ | 26/32 [16:08<03:41, 36.89s/it] 75%|███████▌  | 24/32 [16:09<05:29, 41.13s/it] 81%|████████▏ | 26/32 [16:13<03:51, 38.60s/it] 81%|████████▏ | 26/32 [16:26<03:42, 37.02s/it] 56%|█████▋    | 18/32 [16:27<12:47, 54.84s/it] 56%|█████▋    | 18/32 [16:30<13:02, 55.90s/it] 88%|████████▊ | 28/32 [16:40<02:13, 33.42s/it] 81%|████████▏ | 26/32 [16:42<03:42, 37.09s/it] 84%|████████▍ | 27/32 [16:45<03:04, 36.82s/it] 84%|████████▍ | 27/32 [16:50<03:09, 37.97s/it] 78%|███████▊  | 25/32 [16:54<04:55, 42.27s/it] 84%|████████▍ | 27/32 [17:02<03:04, 36.88s/it] 84%|████████▍ | 27/32 [17:18<03:04, 36.88s/it] 91%|█████████ | 29/32 [17:19<01:44, 34.89s/it] 88%|████████▊ | 28/32 [17:21<02:27, 36.79s/it] 59%|█████▉    | 19/32 [17:25<12:06, 55.87s/it] 88%|████████▊ | 28/32 [17:26<02:29, 37.42s/it] 59%|█████▉    | 19/32 [17:27<12:11, 56.24s/it] 81%|████████▏ | 26/32 [17:37<04:15, 42.59s/it] 88%|████████▊ | 28/32 [17:40<02:28, 37.24s/it] 88%|████████▊ | 28/32 [17:55<02:27, 36.85s/it] 94%|█████████▍| 30/32 [17:57<01:11, 35.84s/it] 91%|█████████ | 29/32 [17:59<01:51, 37.06s/it] 94%|█████████▍| 30/32 [18:02<00:57, 28.55s/it] 91%|█████████ | 29/32 [18:17<01:51, 37.09s/it] 62%|██████▎   | 20/32 [18:19<11:00, 55.06s/it] 84%|████████▍ | 27/32 [18:21<03:34, 42.80s/it] 62%|██████▎   | 20/32 [18:23<11:12, 56.04s/it] 91%|█████████ | 29/32 [18:33<01:51, 37.14s/it] 97%|█████████▋| 31/32 [18:33<00:35, 35.94s/it] 94%|█████████▍| 30/32 [18:36<01:13, 36.99s/it] 97%|█████████▋| 31/32 [18:39<00:30, 30.73s/it] 94%|█████████▍| 30/32 [18:54<01:14, 37.04s/it] 88%|████████▊ | 28/32 [19:04<02:51, 42.93s/it] 94%|█████████▍| 30/32 [19:10<01:14, 37.24s/it] 66%|██████▌   | 21/32 [19:12<10:00, 54.57s/it] 97%|█████████▋| 31/32 [19:13<00:36, 36.91s/it]100%|██████████| 32/32 [19:13<00:00, 37.04s/it]100%|██████████| 32/32 [19:13<00:00, 36.04s/it]
100%|██████████| 32/32 [19:16<00:00, 32.16s/it]100%|██████████| 32/32 [19:16<00:00, 36.13s/it]
 66%|██████▌   | 21/32 [19:18<10:13, 55.81s/it] 97%|█████████▋| 31/32 [19:31<00:36, 36.89s/it] 97%|█████████▋| 31/32 [19:46<00:36, 36.94s/it] 91%|█████████ | 29/32 [19:47<02:08, 42.97s/it]100%|██████████| 32/32 [19:49<00:00, 36.76s/it]100%|██████████| 32/32 [19:49<00:00, 37.17s/it]
100%|██████████| 32/32 [20:07<00:00, 36.79s/it]100%|██████████| 32/32 [20:07<00:00, 37.74s/it]
 69%|██████▉   | 22/32 [20:08<09:11, 55.12s/it] 69%|██████▉   | 22/32 [20:12<09:10, 55.07s/it]100%|██████████| 32/32 [20:23<00:00, 36.78s/it]100%|██████████| 32/32 [20:23<00:00, 38.23s/it]
 94%|█████████▍| 30/32 [20:30<01:26, 43.02s/it] 72%|███████▏  | 23/32 [21:02<08:11, 54.56s/it] 72%|███████▏  | 23/32 [21:06<08:12, 54.71s/it] 97%|█████████▋| 31/32 [21:13<00:43, 43.00s/it]100%|██████████| 32/32 [21:56<00:00, 43.06s/it]100%|██████████| 32/32 [21:56<00:00, 41.15s/it]
 75%|███████▌  | 24/32 [21:57<07:17, 54.71s/it] 75%|███████▌  | 24/32 [22:00<07:17, 54.64s/it] 78%|███████▊  | 25/32 [22:53<06:26, 55.22s/it] 78%|███████▊  | 25/32 [22:56<06:24, 54.91s/it] 81%|████████▏ | 26/32 [23:49<05:27, 54.55s/it] 81%|████████▏ | 26/32 [23:49<05:33, 55.55s/it] 84%|████████▍ | 27/32 [24:43<04:32, 54.43s/it] 84%|████████▍ | 27/32 [24:44<04:36, 55.29s/it] 88%|████████▊ | 28/32 [25:37<03:36, 54.14s/it] 88%|████████▊ | 28/32 [25:38<03:38, 54.70s/it] 91%|█████████ | 29/32 [26:30<02:42, 54.18s/it] 91%|█████████ | 29/32 [26:33<02:43, 54.58s/it] 94%|█████████▍| 30/32 [27:23<01:47, 53.82s/it] 94%|█████████▍| 30/32 [27:26<01:48, 54.17s/it] 97%|█████████▋| 31/32 [28:17<00:53, 53.59s/it] 97%|█████████▋| 31/32 [28:19<00:53, 53.99s/it]100%|██████████| 32/32 [29:09<00:00, 53.33s/it]100%|██████████| 32/32 [29:09<00:00, 54.68s/it]
100%|██████████| 32/32 [29:12<00:00, 53.73s/it]100%|██████████| 32/32 [29:12<00:00, 54.78s/it]
[rank6]:[E ProcessGroupNCCL.cpp:523] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600140 milliseconds before timing out.
[rank6]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E ProcessGroupNCCL.cpp:1182] [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600140 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f491cd4cd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f491def46e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f491def7c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f491def8839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f4967c0bdf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f4968ffa609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f4968dc5353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600140 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f491cd4cd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f491def46e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f491def7c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f491def8839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f4967c0bdf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f4968ffa609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f4968dc5353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f491cd4cd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f491dc4eb11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f4967c0bdf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f4968ffa609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f4968dc5353 in /lib/x86_64-linux-gnu/libc.so.6)

[2024-07-08 22:22:58,061] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 6 (pid: 1970222) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
main.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-08_22:22:58
  host      : a100-st-p4de24xlarge-74.fair-a100.hpcaas
  rank      : 6 (local_rank: 6)
  exitcode  : -6 (pid: 1970222)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 1970222
========================================================
