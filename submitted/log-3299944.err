Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:06<01:06, 66.28s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.11s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.29s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 39.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 43.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.59s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.61s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 39.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 43.49s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 39.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.60s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
  0%|          | 0/32 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:22<11:48, 22.87s/it]  3%|▎         | 1/32 [00:22<11:50, 22.92s/it]  3%|▎         | 1/32 [00:22<11:49, 22.89s/it]  3%|▎         | 1/32 [00:23<11:56, 23.12s/it]  3%|▎         | 1/32 [00:23<11:59, 23.20s/it]  3%|▎         | 1/32 [00:23<12:19, 23.87s/it]  3%|▎         | 1/32 [00:34<17:52, 34.58s/it]  3%|▎         | 1/32 [00:34<17:57, 34.76s/it]  6%|▋         | 2/32 [00:44<10:58, 21.95s/it]  6%|▋         | 2/32 [00:44<11:06, 22.23s/it]  6%|▋         | 2/32 [00:44<11:05, 22.17s/it]  6%|▋         | 2/32 [00:45<11:18, 22.62s/it]  6%|▋         | 2/32 [00:45<11:17, 22.59s/it]  6%|▋         | 2/32 [00:47<11:52, 23.76s/it]  9%|▉         | 3/32 [01:05<10:27, 21.63s/it]  9%|▉         | 3/32 [01:06<10:35, 21.91s/it]  9%|▉         | 3/32 [01:07<10:44, 22.24s/it]  9%|▉         | 3/32 [01:07<10:49, 22.38s/it]  9%|▉         | 3/32 [01:08<10:54, 22.56s/it]  9%|▉         | 3/32 [01:08<11:08, 23.06s/it]  6%|▋         | 2/32 [01:11<18:01, 36.05s/it]  6%|▋         | 2/32 [01:15<19:05, 38.18s/it] 12%|█▎        | 4/32 [01:26<10:02, 21.50s/it] 12%|█▎        | 4/32 [01:27<10:09, 21.76s/it] 12%|█▎        | 4/32 [01:29<10:15, 21.98s/it] 12%|█▎        | 4/32 [01:29<10:23, 22.27s/it] 12%|█▎        | 4/32 [01:30<10:29, 22.47s/it] 12%|█▎        | 4/32 [01:30<10:39, 22.84s/it] 16%|█▌        | 5/32 [01:48<09:38, 21.43s/it] 16%|█▌        | 5/32 [01:49<09:45, 21.67s/it]  9%|▉         | 3/32 [01:49<17:52, 36.99s/it] 16%|█▌        | 5/32 [01:50<09:45, 21.67s/it] 16%|█▌        | 5/32 [01:51<09:53, 21.98s/it] 16%|█▌        | 5/32 [01:51<09:50, 21.88s/it] 16%|█▌        | 5/32 [01:51<09:59, 22.21s/it]  9%|▉         | 3/32 [01:53<18:27, 38.19s/it] 19%|█▉        | 6/32 [02:09<09:21, 21.59s/it] 19%|█▉        | 6/32 [02:10<09:22, 21.63s/it] 19%|█▉        | 6/32 [02:11<09:18, 21.49s/it] 19%|█▉        | 6/32 [02:12<09:20, 21.55s/it] 19%|█▉        | 6/32 [02:12<09:27, 21.81s/it] 19%|█▉        | 6/32 [02:13<09:27, 21.83s/it] 12%|█▎        | 4/32 [02:28<17:39, 37.85s/it] 12%|█▎        | 4/32 [02:31<17:49, 38.19s/it] 22%|██▏       | 7/32 [02:31<09:02, 21.72s/it] 22%|██▏       | 7/32 [02:33<08:55, 21.42s/it] 22%|██▏       | 7/32 [02:33<09:05, 21.83s/it] 22%|██▏       | 7/32 [02:33<08:54, 21.37s/it] 22%|██▏       | 7/32 [02:34<09:03, 21.74s/it] 22%|██▏       | 7/32 [02:34<09:00, 21.63s/it] 25%|██▌       | 8/32 [02:53<08:42, 21.76s/it] 25%|██▌       | 8/32 [02:54<08:31, 21.31s/it] 25%|██▌       | 8/32 [02:54<08:33, 21.41s/it] 25%|██▌       | 8/32 [02:55<08:34, 21.44s/it] 25%|██▌       | 8/32 [02:55<08:38, 21.62s/it] 25%|██▌       | 8/32 [02:57<09:01, 22.56s/it] 16%|█▌        | 5/32 [03:09<17:26, 38.76s/it] 16%|█▌        | 5/32 [03:13<17:44, 39.41s/it] 28%|██▊       | 9/32 [03:15<08:17, 21.61s/it] 28%|██▊       | 9/32 [03:15<08:13, 21.46s/it] 28%|██▊       | 9/32 [03:16<08:11, 21.35s/it] 28%|██▊       | 9/32 [03:16<08:16, 21.58s/it] 28%|██▊       | 9/32 [03:16<08:18, 21.66s/it] 28%|██▊       | 9/32 [03:20<08:46, 22.88s/it] 31%|███▏      | 10/32 [03:36<07:53, 21.53s/it] 31%|███▏      | 10/32 [03:37<07:50, 21.40s/it] 31%|███▏      | 10/32 [03:37<07:49, 21.32s/it] 31%|███▏      | 10/32 [03:37<07:51, 21.45s/it] 31%|███▏      | 10/32 [03:41<08:12, 22.41s/it] 31%|███▏      | 10/32 [03:43<08:18, 22.67s/it] 19%|█▉        | 6/32 [03:49<17:04, 39.40s/it] 19%|█▉        | 6/32 [03:51<16:55, 39.07s/it] 34%|███▍      | 11/32 [03:57<07:29, 21.42s/it] 34%|███▍      | 11/32 [03:58<07:26, 21.24s/it] 34%|███▍      | 11/32 [03:58<07:31, 21.50s/it] 34%|███▍      | 11/32 [03:59<07:30, 21.46s/it] 34%|███▍      | 11/32 [04:04<07:48, 22.32s/it] 34%|███▍      | 11/32 [04:04<07:57, 22.73s/it] 38%|███▊      | 12/32 [04:20<07:15, 21.80s/it] 38%|███▊      | 12/32 [04:20<07:08, 21.42s/it] 38%|███▊      | 12/32 [04:20<07:12, 21.60s/it] 38%|███▊      | 12/32 [04:21<07:14, 21.70s/it] 38%|███▊      | 12/32 [04:26<07:27, 22.36s/it] 38%|███▊      | 12/32 [04:26<07:25, 22.29s/it] 22%|██▏       | 7/32 [04:27<16:06, 38.65s/it] 22%|██▏       | 7/32 [04:28<16:00, 38.40s/it] 41%|████      | 13/32 [04:41<06:47, 21.47s/it] 41%|████      | 13/32 [04:42<06:55, 21.85s/it] 41%|████      | 13/32 [04:43<06:53, 21.74s/it] 41%|████      | 13/32 [04:43<06:55, 21.88s/it] 41%|████      | 13/32 [04:47<06:59, 22.10s/it] 41%|████      | 13/32 [04:48<06:59, 22.08s/it] 44%|████▍     | 14/32 [05:03<06:28, 21.57s/it] 25%|██▌       | 8/32 [05:04<15:16, 38.19s/it] 44%|████▍     | 14/32 [05:04<06:37, 22.07s/it] 44%|████▍     | 14/32 [05:04<06:31, 21.76s/it] 44%|████▍     | 14/32 [05:04<06:28, 21.58s/it] 25%|██▌       | 8/32 [05:05<15:10, 37.96s/it] 44%|████▍     | 14/32 [05:09<06:34, 21.91s/it] 44%|████▍     | 14/32 [05:10<06:41, 22.32s/it] 47%|████▋     | 15/32 [05:24<06:04, 21.44s/it] 47%|████▋     | 15/32 [05:26<06:06, 21.57s/it] 47%|████▋     | 15/32 [05:26<06:11, 21.83s/it] 47%|████▋     | 15/32 [05:27<06:10, 21.77s/it] 47%|████▋     | 15/32 [05:32<06:15, 22.11s/it] 47%|████▋     | 15/32 [05:32<06:14, 22.01s/it] 28%|██▊       | 9/32 [05:42<14:26, 37.67s/it] 28%|██▊       | 9/32 [05:44<14:55, 38.93s/it] 50%|█████     | 16/32 [05:45<05:41, 21.36s/it] 50%|█████     | 16/32 [05:49<05:52, 22.04s/it] 50%|█████     | 16/32 [05:49<05:53, 22.10s/it] 50%|█████     | 16/32 [05:51<06:05, 22.85s/it] 50%|█████     | 16/32 [05:53<05:50, 21.92s/it] 50%|█████     | 16/32 [05:54<05:52, 22.05s/it] 53%|█████▎    | 17/32 [06:10<05:26, 21.76s/it] 53%|█████▎    | 17/32 [06:11<05:39, 22.67s/it] 53%|█████▎    | 17/32 [06:12<05:32, 22.13s/it] 53%|█████▎    | 17/32 [06:13<05:38, 22.56s/it] 53%|█████▎    | 17/32 [06:17<05:35, 22.39s/it] 53%|█████▎    | 17/32 [06:17<05:37, 22.49s/it] 31%|███▏      | 10/32 [06:19<13:43, 37.45s/it] 31%|███▏      | 10/32 [06:24<14:19, 39.07s/it] 56%|█████▋    | 18/32 [06:31<05:02, 21.59s/it] 56%|█████▋    | 18/32 [06:33<05:07, 21.97s/it] 56%|█████▋    | 18/32 [06:35<05:13, 22.40s/it] 56%|█████▋    | 18/32 [06:39<05:13, 22.36s/it] 56%|█████▋    | 18/32 [06:39<05:11, 22.24s/it] 56%|█████▋    | 18/32 [06:40<05:41, 24.39s/it] 59%|█████▉    | 19/32 [06:52<04:38, 21.44s/it] 59%|█████▉    | 19/32 [06:54<04:41, 21.63s/it] 34%|███▍      | 11/32 [06:56<13:04, 37.35s/it] 59%|█████▉    | 19/32 [06:57<04:49, 22.24s/it] 59%|█████▉    | 19/32 [07:00<04:47, 22.10s/it] 34%|███▍      | 11/32 [07:01<13:28, 38.50s/it] 59%|█████▉    | 19/32 [07:01<04:47, 22.13s/it] 59%|█████▉    | 19/32 [07:05<05:22, 24.83s/it] 62%|██████▎   | 20/32 [07:13<04:16, 21.38s/it] 62%|██████▎   | 20/32 [07:16<04:21, 21.82s/it] 62%|██████▎   | 20/32 [07:18<04:23, 21.99s/it] 62%|██████▎   | 20/32 [07:22<04:22, 21.90s/it] 62%|██████▎   | 20/32 [07:23<04:25, 22.14s/it] 62%|██████▎   | 20/32 [07:27<04:44, 23.74s/it] 38%|███▊      | 12/32 [07:34<12:25, 37.28s/it] 66%|██████▌   | 21/32 [07:35<03:54, 21.34s/it] 66%|██████▌   | 21/32 [07:37<03:57, 21.58s/it] 66%|██████▌   | 21/32 [07:39<03:59, 21.81s/it] 38%|███▊      | 12/32 [07:40<12:55, 38.78s/it] 66%|██████▌   | 21/32 [07:44<04:01, 21.96s/it] 66%|██████▌   | 21/32 [07:45<04:03, 22.14s/it] 66%|██████▌   | 21/32 [07:48<04:14, 23.12s/it] 69%|██████▉   | 22/32 [07:56<03:32, 21.25s/it] 69%|██████▉   | 22/32 [07:58<03:33, 21.35s/it] 69%|██████▉   | 22/32 [08:01<03:38, 21.88s/it] 69%|██████▉   | 22/32 [08:06<03:38, 21.83s/it] 69%|██████▉   | 22/32 [08:07<03:40, 22.08s/it] 69%|██████▉   | 22/32 [08:10<03:45, 22.59s/it] 41%|████      | 13/32 [08:10<11:46, 37.17s/it] 72%|███████▏  | 23/32 [08:17<03:10, 21.21s/it] 41%|████      | 13/32 [08:17<12:06, 38.26s/it] 72%|███████▏  | 23/32 [08:19<03:11, 21.22s/it] 72%|███████▏  | 23/32 [08:25<03:20, 22.31s/it] 72%|███████▏  | 23/32 [08:27<03:15, 21.73s/it] 72%|███████▏  | 23/32 [08:28<03:16, 21.84s/it] 72%|███████▏  | 23/32 [08:37<03:36, 24.05s/it] 75%|███████▌  | 24/32 [08:39<02:51, 21.40s/it] 75%|███████▌  | 24/32 [08:41<02:50, 21.33s/it] 75%|███████▌  | 24/32 [08:47<02:59, 22.42s/it] 44%|████▍     | 14/32 [08:49<11:14, 37.47s/it] 75%|███████▌  | 24/32 [08:49<02:53, 21.67s/it] 75%|███████▌  | 24/32 [08:51<02:58, 22.27s/it] 44%|████▍     | 14/32 [08:54<11:22, 37.90s/it] 78%|███████▊  | 25/32 [09:02<02:33, 21.91s/it] 78%|███████▊  | 25/32 [09:02<02:28, 21.20s/it] 78%|███████▊  | 25/32 [09:05<02:16, 19.46s/it] 78%|███████▊  | 25/32 [09:09<02:34, 22.09s/it] 78%|███████▊  | 25/32 [09:10<02:31, 21.63s/it] 78%|███████▊  | 25/32 [09:13<02:33, 21.97s/it] 81%|████████▏ | 26/32 [09:23<02:10, 21.67s/it] 81%|████████▏ | 26/32 [09:23<02:06, 21.14s/it] 47%|████▋     | 15/32 [09:26<10:34, 37.32s/it] 81%|████████▏ | 26/32 [09:30<02:11, 21.88s/it] 81%|████████▏ | 26/32 [09:33<02:08, 21.43s/it] 81%|████████▏ | 26/32 [09:34<02:13, 22.22s/it] 47%|████▋     | 15/32 [09:34<10:52, 38.39s/it] 81%|████████▏ | 26/32 [09:36<02:14, 22.38s/it] 84%|████████▍ | 27/32 [09:45<01:49, 21.88s/it] 84%|████████▍ | 27/32 [09:47<01:51, 22.25s/it] 84%|████████▍ | 27/32 [09:51<01:48, 21.68s/it] 84%|████████▍ | 27/32 [09:55<01:49, 21.98s/it] 84%|████████▍ | 27/32 [09:58<01:51, 22.23s/it] 84%|████████▍ | 27/32 [10:02<01:57, 23.43s/it] 50%|█████     | 16/32 [10:02<09:55, 37.20s/it] 88%|████████▊ | 28/32 [10:06<01:26, 21.67s/it] 88%|████████▊ | 28/32 [10:09<01:28, 22.05s/it] 88%|████████▊ | 28/32 [10:14<01:27, 21.96s/it] 50%|█████     | 16/32 [10:17<10:36, 39.75s/it] 88%|████████▊ | 28/32 [10:17<01:27, 21.89s/it] 88%|████████▊ | 28/32 [10:19<01:27, 21.97s/it] 91%|█████████ | 29/32 [10:28<01:05, 21.71s/it] 88%|████████▊ | 28/32 [10:29<01:37, 24.50s/it] 94%|█████████▍| 30/32 [10:31<00:33, 16.86s/it] 91%|█████████ | 29/32 [10:36<01:05, 21.98s/it] 53%|█████▎    | 17/32 [10:39<09:16, 37.10s/it] 91%|█████████ | 29/32 [10:41<01:07, 22.60s/it] 91%|█████████ | 29/32 [10:42<01:06, 22.16s/it] 94%|█████████▍| 30/32 [10:49<00:43, 21.53s/it] 97%|█████████▋| 31/32 [10:53<00:18, 18.18s/it] 53%|█████▎    | 17/32 [10:55<09:49, 39.27s/it] 94%|█████████▍| 30/32 [10:58<00:43, 21.95s/it] 91%|█████████ | 29/32 [10:58<01:17, 25.75s/it] 94%|█████████▍| 30/32 [11:03<00:44, 22.27s/it] 94%|█████████▍| 30/32 [11:03<00:43, 21.90s/it] 97%|█████████▋| 31/32 [11:10<00:21, 21.41s/it]100%|██████████| 32/32 [11:14<00:00, 18.89s/it]100%|██████████| 32/32 [11:14<00:00, 21.07s/it]
 56%|█████▋    | 18/32 [11:19<08:48, 37.75s/it] 97%|█████████▋| 31/32 [11:19<00:21, 21.74s/it] 97%|█████████▋| 31/32 [11:24<00:22, 22.02s/it] 97%|█████████▋| 31/32 [11:24<00:21, 21.71s/it] 94%|█████████▍| 30/32 [11:27<00:53, 26.70s/it]100%|██████████| 32/32 [11:32<00:00, 21.34s/it]100%|██████████| 32/32 [11:32<00:00, 21.63s/it]
 56%|█████▋    | 18/32 [11:33<09:05, 38.96s/it]100%|██████████| 32/32 [11:42<00:00, 21.98s/it]100%|██████████| 32/32 [11:42<00:00, 21.94s/it]
100%|██████████| 32/32 [11:46<00:00, 21.55s/it]100%|██████████| 32/32 [11:46<00:00, 22.06s/it]
100%|██████████| 32/32 [11:48<00:00, 22.47s/it]100%|██████████| 32/32 [11:48<00:00, 22.13s/it]
 97%|█████████▋| 31/32 [11:54<00:26, 26.84s/it] 59%|█████▉    | 19/32 [11:55<08:07, 37.50s/it] 59%|█████▉    | 19/32 [12:10<08:17, 38.28s/it]100%|██████████| 32/32 [12:22<00:00, 26.93s/it]100%|██████████| 32/32 [12:22<00:00, 23.19s/it]
 62%|██████▎   | 20/32 [12:33<07:31, 37.65s/it] 62%|██████▎   | 20/32 [12:48<07:39, 38.26s/it] 66%|██████▌   | 21/32 [13:10<06:51, 37.39s/it] 66%|██████▌   | 21/32 [13:26<07:00, 38.24s/it] 69%|██████▉   | 22/32 [13:47<06:12, 37.29s/it] 69%|██████▉   | 22/32 [14:05<06:22, 38.27s/it] 72%|███████▏  | 23/32 [14:25<05:37, 37.46s/it] 72%|███████▏  | 23/32 [14:43<05:44, 38.23s/it] 75%|███████▌  | 24/32 [15:05<05:04, 38.04s/it] 75%|███████▌  | 24/32 [15:19<05:01, 37.74s/it] 78%|███████▊  | 25/32 [15:42<04:25, 37.98s/it] 78%|███████▊  | 25/32 [15:58<04:25, 37.90s/it] 81%|████████▏ | 26/32 [16:23<03:52, 38.71s/it] 81%|████████▏ | 26/32 [16:35<03:45, 37.65s/it] 84%|████████▍ | 27/32 [17:01<03:12, 38.45s/it] 84%|████████▍ | 27/32 [17:14<03:10, 38.05s/it] 88%|████████▊ | 28/32 [17:38<02:31, 37.99s/it] 88%|████████▊ | 28/32 [17:53<02:33, 38.40s/it] 91%|█████████ | 29/32 [18:18<01:56, 38.73s/it] 91%|█████████ | 29/32 [18:31<01:55, 38.37s/it] 94%|█████████▍| 30/32 [18:55<01:16, 38.15s/it] 94%|█████████▍| 30/32 [19:10<01:17, 38.53s/it] 97%|█████████▋| 31/32 [19:33<00:38, 38.14s/it] 97%|█████████▋| 31/32 [19:47<00:37, 37.99s/it]100%|██████████| 32/32 [20:10<00:00, 37.73s/it]100%|██████████| 32/32 [20:10<00:00, 37.82s/it]
100%|██████████| 32/32 [20:21<00:00, 36.68s/it]100%|██████████| 32/32 [20:21<00:00, 38.16s/it]
