Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:02<01:02, 62.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:04<01:04, 64.53s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:05<01:05, 65.47s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:04<01:04, 64.54s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:04<01:04, 64.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:04<01:04, 64.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:04<01:04, 64.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:05<01:05, 65.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 33.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 38.67s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:18<00:00, 34.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:18<00:00, 39.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 34.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 34.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 38.71s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 38.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 34.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 38.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 34.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 38.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 34.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 38.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 34.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 38.84s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:26<13:56, 26.98s/it]  3%|▎         | 1/32 [00:31<16:03, 31.09s/it]  3%|▎         | 1/32 [00:32<16:55, 32.76s/it]  3%|▎         | 1/32 [00:32<17:01, 32.94s/it]  3%|▎         | 1/32 [00:34<17:34, 34.00s/it]  3%|▎         | 1/32 [00:34<17:57, 34.76s/it]  3%|▎         | 1/32 [00:35<18:17, 35.41s/it]  3%|▎         | 1/32 [00:36<18:56, 36.65s/it]  6%|▋         | 2/32 [00:58<14:47, 29.58s/it]  6%|▋         | 2/32 [01:02<15:44, 31.47s/it]  6%|▋         | 2/32 [01:06<16:36, 33.21s/it]  6%|▋         | 2/32 [01:07<16:53, 33.77s/it]  6%|▋         | 2/32 [01:08<17:22, 34.77s/it]  6%|▋         | 2/32 [01:10<17:31, 35.04s/it]  9%|▉         | 3/32 [01:21<12:54, 26.72s/it]  9%|▉         | 3/32 [01:27<13:47, 28.55s/it]  6%|▋         | 2/32 [01:32<24:00, 48.02s/it]  6%|▋         | 2/32 [01:33<24:37, 49.24s/it]  9%|▉         | 3/32 [01:37<15:29, 32.06s/it]  9%|▉         | 3/32 [01:39<15:57, 33.02s/it]  9%|▉         | 3/32 [01:41<15:58, 33.04s/it]  9%|▉         | 3/32 [01:41<16:22, 33.88s/it] 12%|█▎        | 4/32 [01:54<13:34, 29.09s/it] 12%|█▎        | 4/32 [02:00<14:01, 30.05s/it]  9%|▉         | 3/32 [02:06<20:05, 41.56s/it] 12%|█▎        | 4/32 [02:08<14:47, 31.69s/it] 12%|█▎        | 4/32 [02:13<15:32, 33.30s/it] 12%|█▎        | 4/32 [02:15<15:36, 33.45s/it] 12%|█▎        | 4/32 [02:18<16:19, 34.97s/it] 16%|█▌        | 5/32 [02:26<13:33, 30.14s/it] 16%|█▌        | 5/32 [02:27<13:08, 29.22s/it]  9%|▉         | 3/32 [02:33<26:09, 54.11s/it] 16%|█▌        | 5/32 [02:37<13:48, 30.69s/it] 16%|█▌        | 5/32 [02:40<13:52, 30.82s/it] 16%|█▌        | 5/32 [02:49<15:05, 33.55s/it] 16%|█▌        | 5/32 [02:52<15:33, 34.56s/it] 19%|█▉        | 6/32 [02:57<12:42, 29.34s/it] 19%|█▉        | 6/32 [02:58<13:17, 30.67s/it] 12%|█▎        | 4/32 [03:00<21:49, 46.77s/it] 19%|█▉        | 6/32 [03:11<13:28, 31.08s/it] 19%|█▉        | 6/32 [03:18<14:54, 34.41s/it] 19%|█▉        | 6/32 [03:21<14:22, 33.18s/it] 22%|██▏       | 7/32 [03:26<12:25, 29.84s/it] 19%|█▉        | 6/32 [03:26<14:58, 34.55s/it] 22%|██▏       | 7/32 [03:29<12:31, 30.04s/it] 12%|█▎        | 4/32 [03:35<26:43, 57.28s/it] 22%|██▏       | 7/32 [03:43<13:05, 31.43s/it] 22%|██▏       | 7/32 [03:49<13:09, 31.59s/it] 25%|██▌       | 8/32 [03:54<11:42, 29.29s/it] 22%|██▏       | 7/32 [03:59<14:08, 33.94s/it] 22%|██▏       | 7/32 [03:59<15:11, 36.46s/it] 25%|██▌       | 8/32 [04:02<12:23, 30.98s/it] 16%|█▌        | 5/32 [04:05<23:56, 53.22s/it] 25%|██▌       | 8/32 [04:15<12:39, 31.63s/it] 25%|██▌       | 8/32 [04:20<12:31, 31.33s/it] 28%|██▊       | 9/32 [04:22<11:08, 29.07s/it] 25%|██▌       | 8/32 [04:27<12:49, 32.08s/it] 16%|█▌        | 5/32 [04:29<25:11, 55.97s/it] 28%|██▊       | 9/32 [04:37<12:24, 32.38s/it] 25%|██▌       | 8/32 [04:40<15:07, 37.82s/it] 31%|███▏      | 10/32 [04:52<10:41, 29.15s/it] 28%|██▊       | 9/32 [04:53<11:36, 30.26s/it] 28%|██▊       | 9/32 [04:57<12:38, 32.98s/it] 19%|█▉        | 6/32 [05:11<24:56, 57.55s/it] 31%|███▏      | 10/32 [05:13<12:16, 33.47s/it] 19%|█▉        | 6/32 [05:18<23:09, 53.44s/it] 28%|██▊       | 9/32 [05:19<14:39, 38.23s/it] 28%|██▊       | 9/32 [05:20<16:01, 41.81s/it] 34%|███▍      | 11/32 [05:22<10:19, 29.51s/it] 31%|███▏      | 10/32 [05:26<11:22, 31.00s/it] 31%|███▏      | 10/32 [05:29<12:01, 32.78s/it] 34%|███▍      | 11/32 [05:44<11:29, 32.85s/it] 31%|███▏      | 10/32 [05:51<13:21, 36.43s/it] 34%|███▍      | 11/32 [05:53<10:24, 29.72s/it] 31%|███▏      | 10/32 [05:54<14:29, 39.51s/it] 38%|███▊      | 12/32 [05:57<10:23, 31.17s/it] 22%|██▏       | 7/32 [05:57<20:21, 48.87s/it] 34%|███▍      | 11/32 [05:58<11:06, 31.75s/it] 22%|██▏       | 7/32 [06:07<23:44, 56.97s/it] 38%|███▊      | 12/32 [06:17<10:54, 32.74s/it] 34%|███▍      | 11/32 [06:24<12:47, 36.53s/it] 38%|███▊      | 12/32 [06:26<10:15, 30.79s/it] 34%|███▍      | 11/32 [06:26<12:33, 35.88s/it] 38%|███▊      | 12/32 [06:29<10:27, 31.40s/it] 41%|████      | 13/32 [06:30<10:04, 31.82s/it] 41%|████      | 13/32 [06:48<10:11, 32.17s/it] 25%|██▌       | 8/32 [06:53<20:24, 51.00s/it] 38%|███▊      | 12/32 [06:56<11:44, 35.20s/it] 41%|████      | 13/32 [06:57<09:45, 30.82s/it] 41%|████      | 13/32 [06:58<09:45, 30.80s/it] 44%|████▍     | 14/32 [07:02<09:31, 31.74s/it] 38%|███▊      | 12/32 [07:07<12:26, 37.34s/it] 25%|██▌       | 8/32 [07:10<23:33, 58.91s/it] 44%|████▍     | 14/32 [07:18<09:28, 31.57s/it] 44%|████▍     | 14/32 [07:25<09:00, 30.03s/it] 41%|████      | 13/32 [07:28<10:51, 34.29s/it] 44%|████▍     | 14/32 [07:29<09:10, 30.61s/it] 47%|████▋     | 15/32 [07:34<09:02, 31.89s/it] 41%|████      | 13/32 [07:43<11:43, 37.03s/it] 28%|██▊       | 9/32 [07:49<20:09, 52.61s/it] 47%|████▋     | 15/32 [07:52<09:11, 32.45s/it] 47%|████▋     | 15/32 [07:57<08:27, 29.84s/it] 47%|████▋     | 15/32 [07:58<08:46, 30.97s/it] 44%|████▍     | 14/32 [07:59<09:58, 33.23s/it] 50%|█████     | 16/32 [07:59<07:55, 29.72s/it] 28%|██▊       | 9/32 [08:15<23:21, 60.95s/it] 44%|████▍     | 14/32 [08:21<11:09, 37.22s/it] 50%|█████     | 16/32 [08:22<08:23, 31.47s/it] 47%|████▋     | 15/32 [08:25<08:48, 31.10s/it] 50%|█████     | 16/32 [08:29<08:14, 30.90s/it] 50%|█████     | 16/32 [08:32<08:24, 31.51s/it] 53%|█████▎    | 17/32 [08:34<07:52, 31.48s/it] 31%|███▏      | 10/32 [08:47<19:52, 54.22s/it] 53%|█████▎    | 17/32 [08:48<07:29, 29.94s/it] 50%|█████     | 16/32 [08:52<07:57, 29.83s/it] 47%|████▋     | 15/32 [08:56<10:23, 36.69s/it] 53%|█████▎    | 17/32 [09:01<07:49, 31.33s/it] 53%|█████▎    | 17/32 [09:01<07:41, 30.78s/it] 56%|█████▋    | 18/32 [09:05<07:17, 31.26s/it] 31%|███▏      | 10/32 [09:10<21:38, 59.04s/it] 56%|█████▋    | 18/32 [09:17<06:55, 29.66s/it] 53%|█████▎    | 17/32 [09:22<07:29, 29.93s/it] 56%|█████▋    | 18/32 [09:32<07:15, 31.10s/it] 59%|█████▉    | 19/32 [09:33<06:31, 30.08s/it] 56%|█████▋    | 18/32 [09:33<07:13, 30.97s/it] 50%|█████     | 16/32 [09:43<10:35, 39.70s/it] 59%|█████▉    | 19/32 [09:46<06:23, 29.47s/it] 34%|███▍      | 11/32 [09:50<19:55, 56.92s/it] 56%|█████▋    | 18/32 [09:50<06:50, 29.33s/it] 59%|█████▉    | 19/32 [09:58<06:21, 29.35s/it] 59%|█████▉    | 19/32 [10:02<06:39, 30.72s/it] 62%|██████▎   | 20/32 [10:04<06:05, 30.48s/it] 34%|███▍      | 11/32 [10:05<20:11, 57.71s/it] 62%|██████▎   | 20/32 [10:16<05:54, 29.51s/it] 59%|█████▉    | 19/32 [10:21<06:29, 29.93s/it] 53%|█████▎    | 17/32 [10:23<09:59, 39.99s/it] 38%|███▊      | 12/32 [10:27<16:59, 50.96s/it] 62%|██████▎   | 20/32 [10:31<06:02, 30.20s/it] 62%|██████▎   | 20/32 [10:32<06:08, 30.70s/it] 66%|██████▌   | 21/32 [10:37<05:44, 31.34s/it] 66%|██████▌   | 21/32 [10:49<05:37, 30.72s/it] 62%|██████▎   | 20/32 [10:49<05:51, 29.33s/it] 38%|███▊      | 12/32 [10:58<18:49, 56.48s/it] 66%|██████▌   | 21/32 [11:01<05:32, 30.27s/it] 56%|█████▋    | 18/32 [11:01<09:09, 39.28s/it] 66%|██████▌   | 21/32 [11:04<05:42, 31.18s/it] 69%|██████▉   | 22/32 [11:13<05:25, 32.59s/it] 69%|██████▉   | 22/32 [11:20<05:06, 30.69s/it] 41%|████      | 13/32 [11:25<16:50, 53.19s/it] 66%|██████▌   | 21/32 [11:27<05:49, 31.79s/it] 69%|██████▉   | 22/32 [11:30<05:00, 30.00s/it] 69%|██████▉   | 22/32 [11:32<05:00, 30.01s/it] 59%|█████▉    | 19/32 [11:37<08:15, 38.12s/it] 72%|███████▏  | 23/32 [11:43<04:47, 31.95s/it] 72%|███████▏  | 23/32 [11:48<04:28, 29.82s/it] 72%|███████▏  | 23/32 [11:57<04:20, 28.94s/it] 72%|███████▏  | 23/32 [11:58<04:21, 29.01s/it] 69%|██████▉   | 22/32 [11:59<05:20, 32.01s/it] 41%|████      | 13/32 [12:01<18:26, 58.23s/it] 62%|██████▎   | 20/32 [12:11<07:24, 37.01s/it] 75%|███████▌  | 24/32 [12:14<04:11, 31.46s/it] 44%|████▍     | 14/32 [12:17<15:49, 52.74s/it] 78%|███████▊  | 25/32 [12:18<02:41, 23.00s/it] 75%|███████▌  | 24/32 [12:25<03:46, 28.27s/it] 72%|███████▏  | 23/32 [12:33<04:51, 32.39s/it] 75%|███████▌  | 24/32 [12:34<04:11, 31.47s/it] 66%|██████▌   | 21/32 [12:42<06:27, 35.19s/it] 78%|███████▊  | 25/32 [12:42<03:34, 30.61s/it] 81%|████████▏ | 26/32 [12:46<02:25, 24.21s/it] 44%|████▍     | 14/32 [12:57<17:16, 57.56s/it] 78%|███████▊  | 25/32 [12:58<03:27, 29.64s/it] 47%|████▋     | 15/32 [13:03<14:23, 50.82s/it] 78%|███████▊  | 25/32 [13:04<03:36, 30.99s/it] 69%|██████▉   | 22/32 [13:08<05:24, 32.49s/it] 75%|███████▌  | 24/32 [13:12<04:35, 34.49s/it] 81%|████████▏ | 26/32 [13:14<03:06, 31.04s/it] 84%|████████▍ | 27/32 [13:21<02:15, 27.09s/it] 81%|████████▏ | 26/32 [13:28<02:59, 29.92s/it] 81%|████████▏ | 26/32 [13:34<03:04, 30.68s/it] 72%|███████▏  | 23/32 [13:40<04:49, 32.22s/it] 78%|███████▊  | 25/32 [13:42<03:51, 33.09s/it] 47%|████▋     | 15/32 [13:43<15:21, 54.21s/it] 84%|████████▍ | 27/32 [13:43<02:32, 30.46s/it] 88%|████████▊ | 28/32 [13:51<01:51, 27.93s/it] 84%|████████▍ | 27/32 [13:58<02:29, 29.93s/it] 50%|█████     | 16/32 [14:08<14:37, 54.82s/it] 84%|████████▍ | 27/32 [14:09<02:39, 31.96s/it] 75%|███████▌  | 24/32 [14:10<04:13, 31.64s/it] 81%|████████▏ | 26/32 [14:14<03:16, 32.68s/it] 88%|████████▊ | 28/32 [14:23<02:13, 33.28s/it] 91%|█████████ | 29/32 [14:27<01:30, 30.13s/it] 88%|████████▊ | 28/32 [14:32<02:04, 31.01s/it] 50%|█████     | 16/32 [14:40<14:40, 55.02s/it] 88%|████████▊ | 28/32 [14:41<02:08, 32.05s/it] 84%|████████▍ | 27/32 [14:41<02:36, 31.21s/it] 78%|███████▊  | 25/32 [14:42<03:42, 31.85s/it] 53%|█████▎    | 17/32 [14:53<12:59, 51.99s/it] 94%|█████████▍| 30/32 [14:54<00:58, 29.36s/it] 91%|█████████ | 29/32 [15:01<01:31, 30.53s/it] 91%|█████████ | 29/32 [15:09<01:51, 37.03s/it] 91%|█████████ | 29/32 [15:09<01:32, 30.85s/it] 88%|████████▊ | 28/32 [15:17<02:10, 32.54s/it] 81%|████████▏ | 26/32 [15:19<03:19, 33.29s/it] 97%|█████████▋| 31/32 [15:22<00:28, 28.91s/it] 94%|█████████▍| 30/32 [15:30<01:00, 30.07s/it] 53%|█████▎    | 17/32 [15:34<13:40, 54.68s/it] 94%|█████████▍| 30/32 [15:42<01:02, 31.42s/it] 84%|████████▍ | 27/32 [15:43<02:32, 30.49s/it] 91%|█████████ | 29/32 [15:50<01:37, 32.65s/it] 94%|█████████▍| 30/32 [15:51<01:17, 38.57s/it]100%|██████████| 32/32 [15:56<00:00, 30.27s/it]100%|██████████| 32/32 [15:56<00:00, 29.88s/it]
 56%|█████▋    | 18/32 [15:56<12:54, 55.31s/it] 97%|█████████▋| 31/32 [15:57<00:29, 29.01s/it] 88%|████████▊ | 28/32 [16:14<02:02, 30.61s/it] 97%|█████████▋| 31/32 [16:15<00:31, 31.78s/it] 94%|█████████▍| 30/32 [16:17<01:01, 30.95s/it] 56%|█████▋    | 18/32 [16:27<12:37, 54.11s/it]100%|██████████| 32/32 [16:28<00:00, 29.82s/it]100%|██████████| 32/32 [16:28<00:00, 30.90s/it]
 97%|█████████▋| 31/32 [16:33<00:39, 39.61s/it] 91%|█████████ | 29/32 [16:45<01:32, 30.89s/it] 97%|█████████▋| 31/32 [16:49<00:31, 31.17s/it]100%|██████████| 32/32 [16:49<00:00, 32.44s/it]100%|██████████| 32/32 [16:49<00:00, 31.53s/it]
 59%|█████▉    | 19/32 [16:53<12:04, 55.73s/it]100%|██████████| 32/32 [17:15<00:00, 29.89s/it]100%|██████████| 32/32 [17:15<00:00, 32.37s/it]
100%|██████████| 32/32 [17:16<00:00, 40.49s/it]100%|██████████| 32/32 [17:16<00:00, 32.38s/it]
 59%|█████▉    | 19/32 [17:16<11:26, 52.77s/it] 94%|█████████▍| 30/32 [17:17<01:02, 31.03s/it] 62%|██████▎   | 20/32 [17:50<11:15, 56.33s/it] 97%|█████████▋| 31/32 [17:52<00:32, 32.17s/it] 62%|██████▎   | 20/32 [18:06<10:21, 51.79s/it]100%|██████████| 32/32 [18:21<00:00, 31.37s/it]100%|██████████| 32/32 [18:21<00:00, 34.42s/it]
 66%|██████▌   | 21/32 [18:45<10:13, 55.81s/it] 66%|██████▌   | 21/32 [19:00<09:36, 52.40s/it] 69%|██████▉   | 22/32 [19:40<08:08, 48.82s/it] 69%|██████▉   | 22/32 [19:48<09:39, 57.91s/it] 72%|███████▏  | 23/32 [20:31<08:01, 53.48s/it] 72%|███████▏  | 23/32 [20:37<07:41, 51.24s/it]