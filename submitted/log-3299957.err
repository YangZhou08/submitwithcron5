Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:52, 17.54s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:56, 18.76s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.22s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.14s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.18s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:32<00:31, 15.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:33, 16.99s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:33, 16.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:33, 16.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:45<00:14, 14.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:47<00:15, 15.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:15, 15.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:47<00:15, 15.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:47<00:15, 15.33s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:47<00:15, 15.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:47<00:15, 15.33s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:47<00:15, 15.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00,  9.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.34s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00,  9.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.41s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00,  9.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 10.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.28s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00,  9.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.27s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 10.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 10.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.41s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:24<12:48, 24.80s/it]  3%|▎         | 1/32 [00:25<13:25, 25.97s/it]  3%|▎         | 1/32 [00:29<15:13, 29.45s/it]  3%|▎         | 1/32 [00:33<17:16, 33.44s/it]  3%|▎         | 1/32 [00:34<17:50, 34.53s/it]  3%|▎         | 1/32 [00:36<18:55, 36.63s/it]  3%|▎         | 1/32 [00:38<19:45, 38.24s/it]  3%|▎         | 1/32 [00:43<22:39, 43.85s/it]  6%|▋         | 2/32 [00:47<11:41, 23.38s/it]  6%|▋         | 2/32 [00:50<12:44, 25.47s/it]  6%|▋         | 2/32 [00:54<13:09, 26.33s/it]  6%|▋         | 2/32 [00:55<13:48, 27.60s/it]  6%|▋         | 2/32 [00:56<13:28, 26.95s/it]  6%|▋         | 2/32 [01:05<16:09, 32.32s/it]  9%|▉         | 3/32 [01:07<10:36, 21.96s/it]  9%|▉         | 3/32 [01:11<11:17, 23.38s/it]  9%|▉         | 3/32 [01:18<12:20, 25.55s/it]  9%|▉         | 3/32 [01:19<12:07, 25.08s/it]  9%|▉         | 3/32 [01:20<12:33, 25.98s/it]  6%|▋         | 2/32 [01:20<20:23, 40.78s/it] 12%|█▎        | 4/32 [01:29<10:14, 21.96s/it]  9%|▉         | 3/32 [01:30<13:59, 28.96s/it]  6%|▋         | 2/32 [01:31<23:02, 46.10s/it] 12%|█▎        | 4/32 [01:38<11:35, 24.85s/it] 12%|█▎        | 4/32 [01:41<11:15, 24.14s/it] 12%|█▎        | 4/32 [01:45<11:59, 25.70s/it] 12%|█▎        | 4/32 [01:53<12:22, 26.52s/it] 12%|█▎        | 4/32 [01:57<14:19, 30.70s/it] 16%|█▌        | 5/32 [01:59<11:04, 24.63s/it] 16%|█▌        | 5/32 [02:01<10:50, 24.11s/it] 16%|█▌        | 5/32 [02:08<11:18, 25.12s/it] 16%|█▌        | 5/32 [02:13<11:54, 26.46s/it]  9%|▉         | 3/32 [02:17<23:14, 48.09s/it]  9%|▉         | 3/32 [02:18<22:25, 46.38s/it] 16%|█▌        | 5/32 [02:20<11:59, 26.64s/it] 16%|█▌        | 5/32 [02:21<12:46, 28.40s/it] 19%|█▉        | 6/32 [02:24<10:49, 24.98s/it] 19%|█▉        | 6/32 [02:26<10:32, 24.31s/it] 19%|█▉        | 6/32 [02:31<10:29, 24.23s/it] 19%|█▉        | 6/32 [02:34<10:41, 24.68s/it] 19%|█▉        | 6/32 [02:42<10:52, 25.10s/it] 22%|██▏       | 7/32 [02:45<09:51, 23.68s/it] 22%|██▏       | 7/32 [02:49<10:01, 24.07s/it] 19%|█▉        | 6/32 [02:51<12:26, 28.72s/it] 22%|██▏       | 7/32 [02:54<10:02, 24.08s/it] 12%|█▎        | 4/32 [02:55<19:55, 42.71s/it] 22%|██▏       | 7/32 [02:57<09:58, 23.93s/it] 22%|██▏       | 7/32 [03:07<10:22, 24.88s/it] 25%|██▌       | 8/32 [03:07<09:16, 23.17s/it] 25%|██▌       | 8/32 [03:14<09:41, 24.23s/it] 12%|█▎        | 4/32 [03:15<24:16, 52.01s/it] 22%|██▏       | 7/32 [03:16<11:27, 27.50s/it] 25%|██▌       | 8/32 [03:17<09:04, 22.68s/it] 25%|██▌       | 8/32 [03:19<09:44, 24.35s/it] 16%|█▌        | 5/32 [03:32<18:17, 40.66s/it] 28%|██▊       | 9/32 [03:32<09:06, 23.75s/it] 25%|██▌       | 8/32 [03:38<10:50, 27.12s/it] 25%|██▌       | 8/32 [03:40<10:36, 26.51s/it] 28%|██▊       | 9/32 [03:40<09:33, 24.92s/it] 28%|██▊       | 9/32 [03:41<09:01, 23.55s/it] 28%|██▊       | 9/32 [03:52<10:14, 26.73s/it] 31%|███▏      | 10/32 [03:54<08:26, 23.02s/it] 16%|█▌        | 5/32 [03:55<21:21, 47.48s/it] 28%|██▊       | 9/32 [04:02<09:57, 25.99s/it] 31%|███▏      | 10/32 [04:05<08:38, 23.56s/it] 31%|███▏      | 10/32 [04:06<09:14, 25.20s/it] 28%|██▊       | 9/32 [04:07<10:16, 26.81s/it] 19%|█▉        | 6/32 [04:16<18:12, 42.01s/it] 34%|███▍      | 11/32 [04:21<08:27, 24.18s/it] 31%|███▏      | 10/32 [04:22<10:11, 27.80s/it] 34%|███▍      | 11/32 [04:27<08:10, 23.35s/it] 31%|███▏      | 10/32 [04:27<09:28, 25.82s/it] 34%|███▍      | 11/32 [04:28<08:29, 24.25s/it] 31%|███▏      | 10/32 [04:34<09:45, 26.61s/it] 19%|█▉        | 6/32 [04:34<19:22, 44.73s/it] 34%|███▍      | 11/32 [04:45<09:09, 26.18s/it] 38%|███▊      | 12/32 [04:52<08:03, 24.19s/it] 22%|██▏       | 7/32 [04:52<16:40, 40.03s/it] 38%|███▊      | 12/32 [04:54<09:00, 27.04s/it] 34%|███▍      | 11/32 [04:58<09:31, 27.20s/it] 34%|███▍      | 11/32 [04:58<09:04, 25.91s/it] 38%|███▊      | 12/32 [05:00<08:40, 26.03s/it] 22%|██▏       | 7/32 [05:10<17:27, 41.90s/it] 38%|███▊      | 12/32 [05:13<08:53, 26.67s/it] 41%|████      | 13/32 [05:16<07:34, 23.94s/it] 41%|████      | 13/32 [05:17<08:08, 25.71s/it] 41%|████      | 13/32 [05:24<08:07, 25.63s/it] 38%|███▊      | 12/32 [05:24<09:00, 27.05s/it] 38%|███▊      | 12/32 [05:26<08:51, 26.55s/it] 25%|██▌       | 8/32 [05:35<16:19, 40.81s/it] 41%|████      | 13/32 [05:37<08:12, 25.94s/it] 44%|████▍     | 14/32 [05:40<07:26, 24.80s/it] 44%|████▍     | 14/32 [05:42<07:22, 24.56s/it] 41%|████      | 13/32 [05:49<08:01, 25.36s/it] 44%|████▍     | 14/32 [05:50<07:43, 25.76s/it] 25%|██▌       | 8/32 [05:52<16:42, 41.78s/it] 41%|████      | 13/32 [05:53<08:45, 27.64s/it] 44%|████▍     | 14/32 [06:00<07:30, 25.04s/it] 47%|████▋     | 15/32 [06:04<06:48, 24.01s/it] 47%|████▋     | 15/32 [06:06<07:11, 25.40s/it] 44%|████▍     | 14/32 [06:12<07:27, 24.86s/it] 47%|████▋     | 15/32 [06:14<07:06, 25.10s/it] 28%|██▊       | 9/32 [06:15<15:34, 40.63s/it] 47%|████▋     | 15/32 [06:22<06:49, 24.06s/it] 50%|█████     | 16/32 [06:25<06:06, 22.88s/it] 44%|████▍     | 14/32 [06:28<08:55, 29.73s/it] 28%|██▊       | 9/32 [06:29<15:27, 40.34s/it] 50%|█████     | 16/32 [06:31<06:44, 25.31s/it] 47%|████▋     | 15/32 [06:35<06:54, 24.35s/it] 50%|█████     | 16/32 [06:41<06:49, 25.58s/it] 53%|█████▎    | 17/32 [06:47<05:40, 22.68s/it] 31%|███▏      | 10/32 [06:52<14:29, 39.53s/it] 50%|█████     | 16/32 [06:53<07:01, 26.36s/it] 50%|█████     | 16/32 [06:59<06:26, 24.15s/it] 47%|████▋     | 15/32 [07:01<08:42, 30.71s/it] 53%|█████▎    | 17/32 [07:02<06:43, 26.87s/it] 53%|█████▎    | 17/32 [07:04<06:13, 24.93s/it] 56%|█████▋    | 18/32 [07:06<05:04, 21.72s/it] 31%|███▏      | 10/32 [07:13<15:11, 41.42s/it] 53%|█████▎    | 17/32 [07:17<06:21, 25.42s/it] 53%|█████▎    | 17/32 [07:22<05:54, 23.66s/it] 50%|█████     | 16/32 [07:28<07:51, 29.46s/it] 56%|█████▋    | 18/32 [07:29<05:50, 25.05s/it] 56%|█████▋    | 18/32 [07:31<06:27, 27.66s/it] 59%|█████▉    | 19/32 [07:32<04:57, 22.87s/it] 56%|█████▋    | 18/32 [07:40<05:45, 24.66s/it] 34%|███▍      | 11/32 [07:41<14:51, 42.44s/it] 56%|█████▋    | 18/32 [07:44<05:24, 23.19s/it] 59%|█████▉    | 19/32 [07:52<05:32, 25.60s/it] 62%|██████▎   | 20/32 [07:53<04:29, 22.47s/it] 59%|█████▉    | 19/32 [07:55<05:29, 25.32s/it] 59%|█████▉    | 19/32 [07:59<05:00, 23.09s/it] 34%|███▍      | 11/32 [08:01<15:12, 43.47s/it] 53%|█████▎    | 17/32 [08:01<07:41, 30.76s/it] 59%|█████▉    | 19/32 [08:06<04:58, 22.97s/it] 62%|██████▎   | 20/32 [08:12<04:45, 23.81s/it] 38%|███▊      | 12/32 [08:16<13:22, 40.10s/it] 62%|██████▎   | 20/32 [08:18<04:54, 24.58s/it] 66%|██████▌   | 21/32 [08:22<04:26, 24.19s/it] 62%|██████▎   | 20/32 [08:22<04:35, 22.96s/it] 62%|██████▎   | 20/32 [08:29<04:33, 22.81s/it] 56%|█████▋    | 18/32 [08:31<07:06, 30.45s/it] 66%|██████▌   | 21/32 [08:33<04:14, 23.12s/it] 38%|███▊      | 12/32 [08:40<14:04, 42.22s/it] 66%|██████▌   | 21/32 [08:46<04:17, 23.37s/it] 66%|██████▌   | 21/32 [08:49<04:51, 26.46s/it] 69%|██████▉   | 22/32 [08:50<04:14, 25.42s/it] 66%|██████▌   | 21/32 [08:54<04:20, 23.65s/it] 59%|█████▉    | 19/32 [08:58<06:23, 29.51s/it] 41%|████      | 13/32 [09:01<13:07, 41.45s/it] 69%|██████▉   | 22/32 [09:01<04:06, 24.61s/it] 69%|██████▉   | 22/32 [09:12<04:01, 24.19s/it] 72%|███████▏  | 23/32 [09:15<03:48, 25.35s/it] 69%|██████▉   | 22/32 [09:17<04:28, 26.88s/it] 69%|██████▉   | 22/32 [09:19<03:58, 23.85s/it] 41%|████      | 13/32 [09:19<13:04, 41.30s/it] 72%|███████▏  | 23/32 [09:27<03:42, 24.74s/it] 72%|███████▏  | 23/32 [09:32<03:26, 22.95s/it] 62%|██████▎   | 20/32 [09:36<06:24, 32.02s/it] 72%|███████▏  | 23/32 [09:38<03:46, 25.15s/it] 72%|███████▏  | 23/32 [09:39<03:24, 22.72s/it] 75%|███████▌  | 24/32 [09:42<03:25, 25.71s/it] 44%|████▍     | 14/32 [09:43<12:31, 41.75s/it] 75%|███████▌  | 24/32 [09:52<03:20, 25.05s/it] 78%|███████▊  | 25/32 [09:54<02:00, 17.28s/it] 75%|███████▌  | 24/32 [09:58<03:09, 23.71s/it] 44%|████▍     | 14/32 [10:03<12:37, 42.07s/it] 66%|██████▌   | 21/32 [10:06<05:44, 31.35s/it] 78%|███████▊  | 25/32 [10:08<03:00, 25.79s/it] 75%|███████▌  | 24/32 [10:08<03:18, 24.76s/it] 81%|████████▏ | 26/32 [10:18<01:54, 19.06s/it] 78%|███████▊  | 25/32 [10:19<02:58, 25.44s/it] 78%|███████▊  | 25/32 [10:21<02:44, 23.48s/it] 47%|████▋     | 15/32 [10:27<12:03, 42.58s/it] 81%|████████▏ | 26/32 [10:31<02:30, 25.07s/it] 84%|████████▍ | 27/32 [10:39<01:38, 19.68s/it] 81%|████████▏ | 26/32 [10:41<02:26, 24.45s/it] 78%|███████▊  | 25/32 [10:44<03:16, 28.12s/it] 69%|██████▉   | 22/32 [10:45<05:35, 33.51s/it] 47%|████▋     | 15/32 [10:46<11:59, 42.30s/it] 81%|████████▏ | 26/32 [10:48<02:26, 24.42s/it] 84%|████████▍ | 27/32 [11:00<02:11, 26.27s/it] 84%|████████▍ | 27/32 [11:01<01:56, 23.33s/it] 88%|████████▊ | 28/32 [11:05<01:25, 21.26s/it] 50%|█████     | 16/32 [11:08<11:10, 41.91s/it] 81%|████████▏ | 26/32 [11:10<02:44, 27.49s/it] 84%|████████▍ | 27/32 [11:11<02:00, 24.15s/it] 72%|███████▏  | 23/32 [11:17<04:59, 33.32s/it] 88%|████████▊ | 28/32 [11:24<01:42, 25.58s/it] 88%|████████▊ | 28/32 [11:26<01:34, 23.66s/it] 91%|█████████ | 29/32 [11:30<01:07, 22.42s/it] 88%|████████▊ | 28/32 [11:34<01:34, 23.57s/it] 84%|████████▍ | 27/32 [11:37<02:16, 27.24s/it] 91%|█████████ | 29/32 [11:49<01:10, 23.37s/it] 50%|█████     | 16/32 [11:49<12:58, 48.64s/it] 53%|█████▎    | 17/32 [11:49<10:25, 41.73s/it] 91%|█████████ | 29/32 [11:51<01:18, 26.05s/it] 75%|███████▌  | 24/32 [11:52<04:29, 33.68s/it] 91%|█████████ | 29/32 [11:58<01:11, 23.72s/it] 94%|█████████▍| 30/32 [11:58<00:48, 24.02s/it] 88%|████████▊ | 28/32 [12:00<01:44, 26.12s/it] 94%|█████████▍| 30/32 [12:14<00:47, 23.89s/it] 94%|█████████▍| 30/32 [12:17<00:52, 26.01s/it] 78%|███████▊  | 25/32 [12:19<03:40, 31.57s/it] 97%|█████████▋| 31/32 [12:23<00:24, 24.33s/it] 91%|█████████ | 29/32 [12:25<01:17, 25.78s/it] 94%|█████████▍| 30/32 [12:27<00:50, 25.36s/it] 56%|█████▋    | 18/32 [12:29<09:38, 41.30s/it] 53%|█████▎    | 17/32 [12:35<11:57, 47.85s/it] 97%|█████████▋| 31/32 [12:38<00:24, 24.10s/it] 97%|█████████▋| 31/32 [12:41<00:25, 25.26s/it]100%|██████████| 32/32 [12:46<00:00, 23.82s/it]100%|██████████| 32/32 [12:46<00:00, 23.95s/it]
 97%|█████████▋| 31/32 [12:52<00:25, 25.25s/it] 94%|█████████▍| 30/32 [12:53<00:52, 26.44s/it] 81%|████████▏ | 26/32 [13:01<03:29, 34.86s/it]100%|██████████| 32/32 [13:05<00:00, 24.86s/it]100%|██████████| 32/32 [13:05<00:00, 24.53s/it]
100%|██████████| 32/32 [13:09<00:00, 26.03s/it]100%|██████████| 32/32 [13:09<00:00, 24.67s/it]
 59%|█████▉    | 19/32 [13:10<08:53, 41.01s/it]100%|██████████| 32/32 [13:16<00:00, 24.97s/it]100%|██████████| 32/32 [13:16<00:00, 24.90s/it]
 97%|█████████▋| 31/32 [13:17<00:25, 25.56s/it] 56%|█████▋    | 18/32 [13:19<10:52, 46.61s/it]100%|██████████| 32/32 [13:43<00:00, 25.68s/it]100%|██████████| 32/32 [13:43<00:00, 25.73s/it]
 84%|████████▍ | 27/32 [13:44<03:06, 37.34s/it] 62%|██████▎   | 20/32 [13:50<08:09, 40.77s/it] 59%|█████▉    | 19/32 [14:02<09:50, 45.43s/it] 88%|████████▊ | 28/32 [14:15<02:21, 35.26s/it] 66%|██████▌   | 21/32 [14:30<07:26, 40.58s/it] 91%|█████████ | 29/32 [14:43<01:39, 33.21s/it] 62%|██████▎   | 20/32 [14:47<09:03, 45.27s/it] 94%|█████████▍| 30/32 [15:05<00:59, 29.90s/it] 69%|██████▉   | 22/32 [15:09<06:41, 40.17s/it] 97%|█████████▋| 31/32 [15:27<00:27, 27.58s/it] 66%|██████▌   | 21/32 [15:29<08:09, 44.47s/it] 72%|███████▏  | 23/32 [15:50<06:01, 40.19s/it]100%|██████████| 32/32 [15:52<00:00, 26.57s/it]100%|██████████| 32/32 [15:52<00:00, 29.76s/it]
 69%|██████▉   | 22/32 [16:17<07:35, 45.58s/it] 75%|███████▌  | 24/32 [16:34<05:31, 41.49s/it] 72%|███████▏  | 23/32 [16:58<06:36, 44.07s/it] 78%|███████▊  | 25/32 [17:20<04:59, 42.73s/it] 75%|███████▌  | 24/32 [17:40<05:46, 43.34s/it] 81%|████████▏ | 26/32 [18:02<04:15, 42.64s/it] 78%|███████▊  | 25/32 [18:21<04:59, 42.79s/it] 84%|████████▍ | 27/32 [18:50<03:40, 44.19s/it] 81%|████████▏ | 26/32 [19:22<04:49, 48.30s/it] 88%|████████▊ | 28/32 [19:26<02:46, 41.68s/it] 84%|████████▍ | 27/32 [20:07<03:56, 47.30s/it] 91%|█████████ | 29/32 [20:07<02:04, 41.58s/it] 88%|████████▊ | 28/32 [20:50<03:04, 46.03s/it] 94%|█████████▍| 30/32 [20:54<01:26, 43.22s/it] 97%|█████████▋| 31/32 [21:36<00:42, 42.69s/it] 94%|█████████▍| 30/32 [21:37<01:11, 35.67s/it]100%|██████████| 32/32 [22:28<00:00, 45.54s/it]100%|██████████| 32/32 [22:28<00:00, 42.14s/it]
 97%|█████████▋| 31/32 [22:30<00:39, 39.94s/it][rank6]:[E ProcessGroupNCCL.cpp:523] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
[rank6]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E ProcessGroupNCCL.cpp:1182] [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fcb29fc5d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7fcb2b16d6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7fcb2b170c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fcb2b171839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7fcb74e84df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7fcb76273609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7fcb7603e353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fcb29fc5d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7fcb2b16d6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7fcb2b170c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fcb2b171839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7fcb74e84df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7fcb76273609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7fcb7603e353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fcb29fc5d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7fcb2aec7b11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7fcb74e84df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7fcb76273609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7fcb7603e353 in /lib/x86_64-linux-gnu/libc.so.6)

100%|██████████| 32/32 [23:02<00:00, 37.88s/it]100%|██████████| 32/32 [23:02<00:00, 43.21s/it]
[2024-07-09 16:06:56,843] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1328063 closing signal SIGTERM
[2024-07-09 16:06:56,845] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1328064 closing signal SIGTERM
[2024-07-09 16:06:56,845] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1328065 closing signal SIGTERM
[2024-07-09 16:06:56,845] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1328066 closing signal SIGTERM
[2024-07-09 16:06:56,845] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1328067 closing signal SIGTERM
[2024-07-09 16:06:56,846] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1328068 closing signal SIGTERM
[2024-07-09 16:06:56,846] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1328070 closing signal SIGTERM
[2024-07-09 16:06:58,339] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 6 (pid: 1328069) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
main.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-09_16:06:56
  host      : a100-st-p4de24xlarge-170.fair-a100.hpcaas
  rank      : 6 (local_rank: 6)
  exitcode  : -6 (pid: 1328069)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 1328069
========================================================
