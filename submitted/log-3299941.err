Already on 'addinggriffin'
From github.com:YangZhou08/CommonSenseReasoning
   a3ab251..c553cb6  addinggriffin -> origin/addinggriffin
error: Unable to create '/fsx-storygen/beidic/yang/CommonSenseReasoning/.git/index.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:25, 28.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:29<01:27, 29.28s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:26, 28.68s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:26, 28.69s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:26, 28.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:25, 28.66s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:26, 28.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:53<00:53, 26.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:54, 27.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:54, 27.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:54, 27.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:54, 27.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:55, 27.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:54, 27.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:55, 27.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:12<00:22, 22.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:23, 23.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:23, 23.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:23, 23.23s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:23, 23.24s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:23, 23.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:23, 23.40s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:23, 23.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:14<00:00, 14.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:14<00:00, 18.62s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 14.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.76s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 14.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.92s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 14.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.77s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 14.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.85s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 14.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.77s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 14.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.77s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 14.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.79s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:23<12:14, 23.68s/it]  3%|▎         | 1/32 [00:27<14:04, 27.23s/it]  3%|▎         | 1/32 [00:27<14:13, 27.53s/it]  3%|▎         | 1/32 [00:28<14:35, 28.24s/it]  3%|▎         | 1/32 [00:29<15:11, 29.41s/it]  3%|▎         | 1/32 [00:32<16:46, 32.47s/it]  3%|▎         | 1/32 [00:48<25:05, 48.56s/it]  3%|▎         | 1/32 [00:51<26:39, 51.60s/it]  6%|▋         | 2/32 [00:51<12:41, 25.39s/it]  6%|▋         | 2/32 [00:53<13:01, 26.06s/it]  6%|▋         | 2/32 [00:53<13:35, 27.20s/it]  6%|▋         | 2/32 [00:53<13:18, 26.62s/it]  6%|▋         | 2/32 [00:55<13:50, 27.68s/it]  6%|▋         | 2/32 [01:04<16:04, 32.15s/it]  9%|▉         | 3/32 [01:15<11:56, 24.70s/it]  9%|▉         | 3/32 [01:16<12:09, 25.14s/it]  9%|▉         | 3/32 [01:17<12:12, 25.25s/it]  9%|▉         | 3/32 [01:21<13:11, 27.31s/it]  9%|▉         | 3/32 [01:23<13:26, 27.81s/it]  6%|▋         | 2/32 [01:29<22:09, 44.31s/it]  6%|▋         | 2/32 [01:34<23:13, 46.45s/it]  9%|▉         | 3/32 [01:38<16:04, 33.26s/it] 12%|█▎        | 4/32 [01:40<11:39, 24.97s/it] 12%|█▎        | 4/32 [01:41<11:49, 25.32s/it] 12%|█▎        | 4/32 [01:43<12:00, 25.73s/it] 12%|█▎        | 4/32 [01:47<12:21, 26.49s/it] 12%|█▎        | 4/32 [01:49<12:52, 27.58s/it] 12%|█▎        | 4/32 [02:05<14:15, 30.55s/it] 16%|█▌        | 5/32 [02:05<11:12, 24.92s/it] 16%|█▌        | 5/32 [02:08<11:21, 25.23s/it] 16%|█▌        | 5/32 [02:08<11:39, 25.89s/it] 16%|█▌        | 5/32 [02:12<11:37, 25.83s/it]  9%|▉         | 3/32 [02:16<21:52, 45.27s/it]  9%|▉         | 3/32 [02:18<21:56, 45.41s/it] 16%|█▌        | 5/32 [02:24<13:34, 30.15s/it] 19%|█▉        | 6/32 [02:27<10:23, 23.97s/it] 16%|█▌        | 5/32 [02:33<13:21, 29.70s/it] 19%|█▉        | 6/32 [02:33<11:08, 25.70s/it] 19%|█▉        | 6/32 [02:34<11:06, 25.64s/it] 19%|█▉        | 6/32 [02:39<11:22, 26.24s/it] 22%|██▏       | 7/32 [02:50<09:44, 23.38s/it] 22%|██▏       | 7/32 [02:56<10:17, 24.70s/it] 22%|██▏       | 7/32 [02:57<10:14, 24.57s/it] 19%|█▉        | 6/32 [02:57<13:30, 31.19s/it] 19%|█▉        | 6/32 [02:58<12:08, 28.02s/it] 12%|█▎        | 4/32 [03:01<21:07, 45.28s/it] 22%|██▏       | 7/32 [03:02<10:28, 25.12s/it] 12%|█▎        | 4/32 [03:10<22:18, 47.79s/it] 25%|██▌       | 8/32 [03:20<10:14, 25.60s/it] 25%|██▌       | 8/32 [03:20<09:43, 24.29s/it] 25%|██▌       | 8/32 [03:21<09:52, 24.67s/it] 22%|██▏       | 7/32 [03:23<11:19, 27.17s/it] 25%|██▌       | 8/32 [03:28<10:12, 25.51s/it] 22%|██▏       | 7/32 [03:34<13:45, 33.02s/it] 16%|█▌        | 5/32 [03:43<19:45, 43.89s/it] 28%|██▊       | 9/32 [03:45<09:19, 24.31s/it] 28%|██▊       | 9/32 [03:45<09:43, 25.35s/it] 28%|██▊       | 9/32 [03:47<09:37, 25.12s/it] 25%|██▌       | 8/32 [03:51<10:55, 27.30s/it] 28%|██▊       | 9/32 [03:52<09:33, 24.93s/it] 16%|█▌        | 5/32 [03:59<21:42, 48.23s/it] 31%|███▏      | 10/32 [04:08<09:00, 24.57s/it] 31%|███▏      | 10/32 [04:10<09:00, 24.55s/it] 25%|██▌       | 8/32 [04:11<13:42, 34.26s/it] 28%|██▊       | 9/32 [04:14<09:58, 26.03s/it] 31%|███▏      | 10/32 [04:15<09:33, 26.06s/it] 31%|███▏      | 10/32 [04:16<09:02, 24.68s/it] 19%|█▉        | 6/32 [04:28<19:12, 44.34s/it] 34%|███▍      | 11/32 [04:34<08:46, 25.07s/it] 34%|███▍      | 11/32 [04:36<08:47, 25.13s/it] 31%|███▏      | 10/32 [04:38<09:19, 25.42s/it] 34%|███▍      | 11/32 [04:39<08:58, 25.65s/it] 34%|███▍      | 11/32 [04:42<08:44, 25.00s/it] 19%|█▉        | 6/32 [04:43<20:18, 46.88s/it] 28%|██▊       | 9/32 [04:44<13:01, 33.97s/it] 38%|███▊      | 12/32 [04:56<08:05, 24.28s/it] 38%|███▊      | 12/32 [05:02<08:27, 25.37s/it] 38%|███▊      | 12/32 [05:04<08:23, 25.16s/it] 34%|███▍      | 11/32 [05:06<09:12, 26.30s/it] 38%|███▊      | 12/32 [05:11<08:45, 26.26s/it] 22%|██▏       | 7/32 [05:12<18:23, 44.15s/it] 31%|███▏      | 10/32 [05:17<12:22, 33.74s/it] 41%|████      | 13/32 [05:21<07:46, 24.54s/it] 22%|██▏       | 7/32 [05:23<18:37, 44.70s/it] 41%|████      | 13/32 [05:27<07:47, 24.59s/it] 41%|████      | 13/32 [05:28<08:03, 25.47s/it] 38%|███▊      | 12/32 [05:31<08:36, 25.80s/it] 41%|████      | 13/32 [05:35<08:07, 25.66s/it] 44%|████▍     | 14/32 [05:49<07:40, 25.58s/it] 34%|███▍      | 11/32 [05:50<11:39, 33.30s/it] 44%|████▍     | 14/32 [05:50<07:16, 24.23s/it] 44%|████▍     | 14/32 [05:54<07:43, 25.77s/it] 25%|██▌       | 8/32 [05:56<17:45, 44.39s/it] 44%|████▍     | 14/32 [05:59<07:30, 25.05s/it] 41%|████      | 13/32 [05:59<08:24, 26.56s/it] 25%|██▌       | 8/32 [06:10<18:07, 45.30s/it] 47%|████▋     | 15/32 [06:13<07:04, 25.00s/it] 47%|████▋     | 15/32 [06:18<07:11, 25.41s/it] 47%|████▋     | 15/32 [06:19<07:14, 25.55s/it] 38%|███▊      | 12/32 [06:22<10:59, 32.96s/it] 47%|████▋     | 15/32 [06:23<07:02, 24.83s/it] 44%|████▍     | 14/32 [06:26<07:55, 26.44s/it] 50%|█████     | 16/32 [06:38<06:40, 25.04s/it] 28%|██▊       | 9/32 [06:44<17:22, 45.33s/it] 50%|█████     | 16/32 [06:45<06:50, 25.64s/it] 50%|█████     | 16/32 [06:48<07:03, 26.45s/it] 50%|█████     | 16/32 [06:49<06:41, 25.11s/it] 47%|████▋     | 15/32 [06:50<07:21, 25.94s/it] 28%|██▊       | 9/32 [06:50<16:50, 43.91s/it] 41%|████      | 13/32 [06:56<10:33, 33.33s/it] 53%|█████▎    | 17/32 [07:03<06:16, 25.09s/it] 53%|█████▎    | 17/32 [07:10<06:22, 25.53s/it] 53%|█████▎    | 17/32 [07:14<06:33, 26.25s/it] 53%|█████▎    | 17/32 [07:15<06:22, 25.47s/it] 50%|█████     | 16/32 [07:17<06:59, 26.22s/it] 31%|███▏      | 10/32 [07:25<16:12, 44.19s/it] 56%|█████▋    | 18/32 [07:29<05:54, 25.34s/it] 31%|███▏      | 10/32 [07:30<15:35, 42.54s/it] 44%|████▍     | 14/32 [07:33<10:20, 34.46s/it] 56%|█████▋    | 18/32 [07:35<05:54, 25.29s/it] 56%|█████▋    | 18/32 [07:40<06:08, 26.34s/it] 53%|█████▎    | 17/32 [07:40<06:20, 25.34s/it] 56%|█████▋    | 18/32 [07:41<05:59, 25.68s/it] 59%|█████▉    | 19/32 [07:52<05:20, 24.67s/it] 59%|█████▉    | 19/32 [07:59<05:26, 25.13s/it] 59%|█████▉    | 19/32 [08:03<05:29, 25.34s/it] 47%|████▋     | 15/32 [08:06<09:39, 34.10s/it] 59%|█████▉    | 19/32 [08:08<05:38, 26.00s/it] 56%|█████▋    | 18/32 [08:08<06:03, 25.97s/it] 34%|███▍      | 11/32 [08:10<14:40, 41.92s/it] 34%|███▍      | 11/32 [08:12<15:40, 44.79s/it] 62%|██████▎   | 20/32 [08:15<04:49, 24.17s/it] 62%|██████▎   | 20/32 [08:24<04:59, 24.98s/it] 62%|██████▎   | 20/32 [08:27<04:58, 24.86s/it] 59%|█████▉    | 19/32 [08:31<05:26, 25.11s/it] 62%|██████▎   | 20/32 [08:32<05:04, 25.39s/it] 50%|█████     | 16/32 [08:36<08:43, 32.74s/it] 66%|██████▌   | 21/32 [08:39<04:24, 24.03s/it] 66%|██████▌   | 21/32 [08:51<04:40, 25.52s/it] 66%|██████▌   | 21/32 [08:51<04:32, 24.74s/it] 38%|███▊      | 12/32 [08:55<14:14, 42.70s/it] 62%|██████▎   | 20/32 [08:57<05:04, 25.39s/it] 66%|██████▌   | 21/32 [08:58<04:41, 25.60s/it] 38%|███▊      | 12/32 [09:04<15:42, 47.10s/it] 69%|██████▉   | 22/32 [09:04<04:03, 24.37s/it] 53%|█████▎    | 17/32 [09:10<08:17, 33.14s/it] 69%|██████▉   | 22/32 [09:16<04:14, 25.45s/it] 69%|██████▉   | 22/32 [09:16<04:08, 24.81s/it] 69%|██████▉   | 22/32 [09:21<04:08, 24.90s/it] 66%|██████▌   | 21/32 [09:24<04:43, 25.76s/it] 72%|███████▏  | 23/32 [09:27<03:33, 23.76s/it] 41%|████      | 13/32 [09:37<13:29, 42.63s/it] 72%|███████▏  | 23/32 [09:39<03:38, 24.26s/it] 56%|█████▋    | 18/32 [09:39<07:28, 32.03s/it] 72%|███████▏  | 23/32 [09:40<03:45, 25.02s/it] 72%|███████▏  | 23/32 [09:47<03:45, 25.03s/it] 69%|██████▉   | 22/32 [09:47<04:09, 24.97s/it] 41%|████      | 13/32 [09:50<14:47, 46.73s/it] 75%|███████▌  | 24/32 [09:53<03:15, 24.42s/it] 75%|███████▌  | 24/32 [10:02<03:11, 23.90s/it] 75%|███████▌  | 24/32 [10:10<03:31, 26.41s/it] 59%|█████▉    | 19/32 [10:10<06:50, 31.60s/it] 72%|███████▏  | 23/32 [10:13<03:49, 25.48s/it] 75%|███████▌  | 24/32 [10:16<03:31, 26.43s/it] 78%|███████▊  | 25/32 [10:17<02:51, 24.44s/it] 44%|████▍     | 14/32 [10:21<12:50, 42.82s/it] 78%|███████▊  | 25/32 [10:26<02:46, 23.85s/it] 44%|████▍     | 14/32 [10:35<13:52, 46.22s/it] 78%|███████▊  | 25/32 [10:36<03:04, 26.31s/it] 75%|███████▌  | 24/32 [10:36<03:16, 24.57s/it] 62%|██████▎   | 20/32 [10:40<06:14, 31.24s/it] 78%|███████▊  | 25/32 [10:44<03:07, 26.83s/it] 81%|████████▏ | 26/32 [10:46<02:34, 25.72s/it] 81%|████████▏ | 26/32 [10:53<02:29, 24.86s/it] 81%|████████▏ | 26/32 [11:01<02:36, 26.06s/it] 47%|████▋     | 15/32 [11:01<11:56, 42.14s/it] 78%|███████▊  | 25/32 [11:01<02:53, 24.80s/it] 81%|████████▏ | 26/32 [11:07<02:33, 25.63s/it] 84%|████████▍ | 27/32 [11:11<02:07, 25.54s/it] 66%|██████▌   | 21/32 [11:13<05:46, 31.54s/it] 84%|████████▍ | 27/32 [11:20<02:06, 25.35s/it] 84%|████████▍ | 27/32 [11:24<02:05, 25.02s/it] 81%|████████▏ | 26/32 [11:27<02:30, 25.15s/it] 47%|████▋     | 15/32 [11:27<13:37, 48.11s/it] 84%|████████▍ | 27/32 [11:35<02:11, 26.26s/it] 88%|████████▊ | 28/32 [11:35<01:40, 25.21s/it] 69%|██████▉   | 22/32 [11:43<05:12, 31.24s/it] 50%|█████     | 16/32 [11:46<11:24, 42.81s/it] 88%|████████▊ | 28/32 [11:46<01:42, 25.70s/it] 88%|████████▊ | 28/32 [11:47<01:38, 24.55s/it] 84%|████████▍ | 27/32 [11:53<02:06, 25.21s/it] 88%|████████▊ | 28/32 [11:59<01:42, 25.67s/it] 91%|█████████ | 29/32 [11:59<01:14, 24.79s/it] 91%|█████████ | 29/32 [12:11<01:12, 24.19s/it] 94%|█████████▍| 30/32 [12:11<00:38, 19.46s/it] 50%|█████     | 16/32 [12:13<12:36, 47.25s/it] 72%|███████▏  | 23/32 [12:15<04:43, 31.54s/it] 88%|████████▊ | 28/32 [12:19<01:41, 25.42s/it] 91%|█████████ | 29/32 [12:25<01:16, 25.66s/it] 94%|█████████▍| 30/32 [12:25<00:50, 25.09s/it] 53%|█████▎    | 17/32 [12:34<11:05, 44.36s/it] 94%|█████████▍| 30/32 [12:36<00:49, 24.54s/it] 97%|█████████▋| 31/32 [12:37<00:21, 21.23s/it] 91%|█████████ | 29/32 [12:44<01:16, 25.36s/it] 75%|███████▌  | 24/32 [12:46<04:09, 31.22s/it] 94%|█████████▍| 30/32 [12:50<00:50, 25.46s/it] 97%|█████████▋| 31/32 [12:52<00:25, 25.74s/it] 53%|█████▎    | 17/32 [12:55<11:28, 45.87s/it] 97%|█████████▋| 31/32 [13:00<00:24, 24.40s/it]100%|██████████| 32/32 [13:02<00:00, 22.27s/it]100%|██████████| 32/32 [13:02<00:00, 24.47s/it]
 94%|█████████▍| 30/32 [13:10<00:51, 25.55s/it]100%|██████████| 32/32 [13:15<00:00, 24.90s/it]100%|██████████| 32/32 [13:15<00:00, 24.86s/it]
 78%|███████▊  | 25/32 [13:15<03:34, 30.68s/it] 97%|█████████▋| 31/32 [13:18<00:26, 26.36s/it] 56%|█████▋    | 18/32 [13:19<10:27, 44.82s/it]100%|██████████| 32/32 [13:26<00:00, 24.89s/it]100%|██████████| 32/32 [13:26<00:00, 25.21s/it]
 56%|█████▋    | 18/32 [13:34<10:13, 43.81s/it] 97%|█████████▋| 31/32 [13:36<00:25, 25.85s/it]100%|██████████| 32/32 [13:43<00:00, 26.10s/it]100%|██████████| 32/32 [13:43<00:00, 25.75s/it]
 81%|████████▏ | 26/32 [13:50<03:11, 31.97s/it]100%|██████████| 32/32 [14:01<00:00, 25.64s/it]100%|██████████| 32/32 [14:01<00:00, 26.31s/it]
 59%|█████▉    | 19/32 [14:03<09:36, 44.33s/it] 84%|████████▍ | 27/32 [14:22<02:40, 32.01s/it] 59%|█████▉    | 19/32 [14:23<09:49, 45.31s/it] 62%|██████▎   | 20/32 [14:46<08:47, 43.93s/it] 88%|████████▊ | 28/32 [14:56<02:10, 32.59s/it] 62%|██████▎   | 20/32 [15:03<08:45, 43.77s/it] 66%|██████▌   | 21/32 [15:25<07:48, 42.55s/it] 91%|█████████ | 29/32 [15:27<01:36, 32.15s/it] 66%|██████▌   | 21/32 [15:46<07:57, 43.38s/it] 94%|█████████▍| 30/32 [16:07<01:08, 34.24s/it] 69%|██████▉   | 22/32 [16:12<07:18, 43.84s/it] 69%|██████▉   | 22/32 [16:28<07:11, 43.13s/it] 97%|█████████▋| 31/32 [16:32<00:31, 31.46s/it] 72%|███████▏  | 23/32 [16:51<06:23, 42.57s/it]100%|██████████| 32/32 [16:59<00:00, 30.16s/it]100%|██████████| 32/32 [16:59<00:00, 31.85s/it]
 72%|███████▏  | 23/32 [17:13<06:33, 43.70s/it] 78%|███████▊  | 25/32 [17:38<03:55, 33.66s/it] 75%|███████▌  | 24/32 [17:52<05:38, 42.26s/it] 81%|████████▏ | 26/32 [18:23<03:39, 36.58s/it] 78%|███████▊  | 25/32 [18:37<05:01, 43.02s/it] 84%|████████▍ | 27/32 [19:14<03:20, 40.20s/it] 81%|████████▏ | 26/32 [19:20<04:17, 42.93s/it] 88%|████████▊ | 28/32 [19:57<02:43, 40.99s/it] 84%|████████▍ | 27/32 [20:01<03:32, 42.46s/it] 91%|█████████ | 29/32 [20:42<02:06, 42.25s/it] 88%|████████▊ | 28/32 [20:50<02:57, 44.33s/it] 94%|█████████▍| 30/32 [21:26<01:25, 42.55s/it] 91%|█████████ | 29/32 [21:34<02:12, 44.23s/it] 97%|█████████▋| 31/32 [22:09<00:42, 42.67s/it] 94%|█████████▍| 30/32 [22:20<01:29, 44.84s/it]100%|██████████| 32/32 [22:49<00:00, 42.06s/it]100%|██████████| 32/32 [22:49<00:00, 42.81s/it]
 97%|█████████▋| 31/32 [23:00<00:43, 43.21s/it][rank5]:[E ProcessGroupNCCL.cpp:523] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
[rank5]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E ProcessGroupNCCL.cpp:1182] [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f1fdde42d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f1fdefea6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f1fdefedc3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f1fdefee839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f2028d01df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f202a0f0609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f2029ebb353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f1fdde42d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f1fdefea6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f1fdefedc3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f1fdefee839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f2028d01df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f202a0f0609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f2029ebb353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f1fdde42d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f1fded44b11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f2028d01df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f202a0f0609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f2029ebb353 in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E ProcessGroupNCCL.cpp:523] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600362 milliseconds before timing out.
[rank4]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E ProcessGroupNCCL.cpp:1182] [Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600362 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f9d97d1bd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f9d98ec36e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f9d98ec6c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f9d98ec7839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f9de2bdadf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f9de3fc9609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f9de3d94353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600362 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f9d97d1bd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f9d98ec36e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f9d98ec6c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f9d98ec7839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f9de2bdadf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f9de3fc9609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f9de3d94353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f9d97d1bd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f9d98c1db11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f9de2bdadf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f9de3fc9609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f9de3d94353 in /lib/x86_64-linux-gnu/libc.so.6)

100%|██████████| 32/32 [23:38<00:00, 41.85s/it]100%|██████████| 32/32 [23:38<00:00, 44.33s/it]
[rank7]:[E ProcessGroupNCCL.cpp:523] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=54, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 612013 milliseconds before timing out.
[rank7]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E ProcessGroupNCCL.cpp:1182] [Rank 7] NCCL watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=54, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 612013 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f6652827d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f66539cf6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f66539d2c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f66539d3839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f669d6e6df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f669ead5609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f669e8a0353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 7] NCCL watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=54, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 612013 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f6652827d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f66539cf6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f66539d2c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f66539d3839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f669d6e6df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f669ead5609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f669e8a0353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f6652827d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f6653729b11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f669d6e6df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f669ead5609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f669e8a0353 in /lib/x86_64-linux-gnu/libc.so.6)

[2024-07-09 15:42:51,424] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 707911 closing signal SIGTERM
[2024-07-09 15:42:51,425] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 707912 closing signal SIGTERM
[2024-07-09 15:42:51,425] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 707913 closing signal SIGTERM
[2024-07-09 15:42:51,425] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 707914 closing signal SIGTERM
[2024-07-09 15:42:51,426] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 707916 closing signal SIGTERM
[2024-07-09 15:42:51,426] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 707917 closing signal SIGTERM
[2024-07-09 15:42:51,426] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 707918 closing signal SIGTERM
[2024-07-09 15:43:02,257] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 4 (pid: 707915) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
main.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-09_15:42:51
  host      : a100-st-p4de24xlarge-277.fair-a100.hpcaas
  rank      : 4 (local_rank: 4)
  exitcode  : -6 (pid: 707915)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 707915
=======================================================
