Already on 'yangexp2two'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-24:08:43:01,293 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:43:01,294 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:43:01,294 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:43:01,294 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:43:01,294 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:43:01,294 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:43:01,294 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:43:01,294 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:43:10,744 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:43:10,744 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:43:10,744 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:43:10,744 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:43:10,744 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:43:10,744 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:43:10,744 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:43:10,744 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:43:10,768 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:43:10,768 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:43:10,768 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:43:10,768 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:43:10,768 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:43:10,768 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:43:10,768 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:43:10,768 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:43:10,768 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 4, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:43:10,768 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 4, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:43:10,768 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 4, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:43:10,768 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 4, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:43:10,768 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 4, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:43:10,768 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 4, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:43:10,768 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 4, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
2024-07-24:08:43:10,768 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'cats': True, 'widthtree': 4, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1, 'patternstrict': True}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.44s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:43, 14.56s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:43, 14.46s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:43, 14.56s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:43, 14.58s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:43, 14.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:43, 14.58s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:47, 15.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:28<00:28, 14.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:29, 14.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:29, 14.99s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:29, 14.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:30, 15.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:30, 15.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:29<00:29, 14.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:30<00:30, 15.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:43<00:14, 14.26s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:45<00:15, 15.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:15, 15.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:14, 14.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:44<00:00,  9.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:44<00:00, 11.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00,  9.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00, 11.64s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00,  9.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00, 11.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00,  9.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00, 11.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00,  9.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00,  9.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00, 11.40s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00, 11.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00,  9.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00, 11.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00,  9.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00, 11.39s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-24:08:44:38,068 INFO     [xhuggingface.py:323] Using 8 devices with data parallelism
2024-07-24:08:44:38,719 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/165 [00:00<?, ?it/s] 11%|█         | 18/165 [00:00<00:00, 175.33it/s]2024-07-24:08:44:38,926 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/165 [00:00<?, ?it/s] 22%|██▏       | 37/165 [00:00<00:00, 183.24it/s]  7%|▋         | 12/165 [00:00<00:01, 118.89it/s]2024-07-24:08:44:39,062 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
2024-07-24:08:44:39,079 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/165 [00:00<?, ?it/s] 34%|███▍      | 56/165 [00:00<00:00, 186.11it/s]  0%|          | 0/165 [00:00<?, ?it/s] 15%|█▍        | 24/165 [00:00<00:01, 118.04it/s] 12%|█▏        | 20/165 [00:00<00:00, 190.98it/s] 46%|████▌     | 76/165 [00:00<00:00, 187.80it/s]  8%|▊         | 13/165 [00:00<00:01, 125.99it/s]2024-07-24:08:44:39,212 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
2024-07-24:08:44:39,212 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/165 [00:00<?, ?it/s]  0%|          | 0/165 [00:00<?, ?it/s] 27%|██▋       | 44/165 [00:00<00:00, 151.95it/s] 24%|██▍       | 40/165 [00:00<00:00, 192.47it/s] 58%|█████▊    | 96/165 [00:00<00:00, 189.48it/s] 18%|█▊        | 30/165 [00:00<00:00, 150.12it/s] 12%|█▏        | 19/165 [00:00<00:00, 188.05it/s]2024-07-24:08:44:39,340 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
 12%|█▏        | 20/165 [00:00<00:00, 190.22it/s]  0%|          | 0/164 [00:00<?, ?it/s] 39%|███▉      | 64/165 [00:00<00:00, 167.43it/s] 36%|███▋      | 60/165 [00:00<00:00, 192.83it/s] 70%|███████   | 116/165 [00:00<00:00, 190.31it/s] 30%|██▉       | 49/165 [00:00<00:00, 167.43it/s] 23%|██▎       | 38/165 [00:00<00:00, 187.94it/s] 24%|██▍       | 40/165 [00:00<00:00, 191.00it/s] 12%|█▏        | 20/164 [00:00<00:00, 191.29it/s] 51%|█████     | 84/165 [00:00<00:00, 176.78it/s] 48%|████▊     | 80/165 [00:00<00:00, 194.27it/s] 82%|████████▏ | 136/165 [00:00<00:00, 191.03it/s] 41%|████      | 68/165 [00:00<00:00, 176.06it/s] 35%|███▌      | 58/165 [00:00<00:00, 188.95it/s] 36%|███▋      | 60/165 [00:00<00:00, 191.48it/s] 24%|██▍       | 40/164 [00:00<00:00, 192.84it/s] 63%|██████▎   | 104/165 [00:00<00:00, 182.47it/s]2024-07-24:08:44:39,586 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
 61%|██████    | 100/165 [00:00<00:00, 195.11it/s] 95%|█████████▍| 156/165 [00:00<00:00, 191.49it/s]  0%|          | 0/165 [00:00<?, ?it/s] 53%|█████▎    | 88/165 [00:00<00:00, 181.22it/s] 47%|████▋     | 78/165 [00:00<00:00, 190.14it/s] 48%|████▊     | 80/165 [00:00<00:00, 192.30it/s]100%|██████████| 165/165 [00:00<00:00, 189.35it/s]
 37%|███▋      | 60/164 [00:00<00:00, 193.15it/s] 75%|███████▌  | 124/165 [00:00<00:00, 186.07it/s] 73%|███████▎  | 120/165 [00:00<00:00, 195.76it/s] 12%|█▏        | 19/165 [00:00<00:00, 184.75it/s] 65%|██████▍   | 107/165 [00:00<00:00, 183.62it/s] 59%|█████▉    | 98/165 [00:00<00:00, 190.93it/s] 61%|██████    | 100/165 [00:00<00:00, 192.78it/s] 49%|████▉     | 80/164 [00:00<00:00, 193.92it/s] 87%|████████▋ | 144/165 [00:00<00:00, 188.40it/s] 85%|████████▍ | 140/165 [00:00<00:00, 196.08it/s] 23%|██▎       | 38/165 [00:00<00:00, 185.83it/s] 77%|███████▋  | 127/165 [00:00<00:00, 185.95it/s] 72%|███████▏  | 118/165 [00:00<00:00, 191.31it/s] 73%|███████▎  | 120/165 [00:00<00:00, 193.20it/s] 61%|██████    | 100/164 [00:00<00:00, 194.51it/s] 99%|█████████▉| 164/165 [00:00<00:00, 189.84it/s]100%|██████████| 165/165 [00:00<00:00, 176.46it/s]
 97%|█████████▋| 160/165 [00:00<00:00, 196.24it/s] 35%|███▍      | 57/165 [00:00<00:00, 187.44it/s] 89%|████████▉ | 147/165 [00:00<00:00, 187.72it/s]100%|██████████| 165/165 [00:00<00:00, 195.16it/s]
 84%|████████▎ | 138/165 [00:00<00:00, 191.60it/s] 85%|████████▍ | 140/165 [00:00<00:00, 193.40it/s] 73%|███████▎  | 120/164 [00:00<00:00, 194.88it/s] 47%|████▋     | 77/165 [00:00<00:00, 188.70it/s]100%|██████████| 165/165 [00:00<00:00, 179.96it/s]
 96%|█████████▌| 158/165 [00:00<00:00, 191.77it/s] 97%|█████████▋| 160/165 [00:00<00:00, 193.45it/s] 85%|████████▌ | 140/164 [00:00<00:00, 195.04it/s]100%|██████████| 165/165 [00:00<00:00, 192.80it/s]
100%|██████████| 165/165 [00:00<00:00, 190.87it/s]
 59%|█████▉    | 97/165 [00:00<00:00, 189.81it/s] 98%|█████████▊| 160/164 [00:00<00:00, 195.15it/s]100%|██████████| 164/164 [00:00<00:00, 194.45it/s]
 71%|███████   | 117/165 [00:00<00:00, 190.57it/s] 83%|████████▎ | 137/165 [00:00<00:00, 190.58it/s] 95%|█████████▌| 157/165 [00:00<00:00, 191.02it/s]100%|██████████| 165/165 [00:00<00:00, 189.81it/s]
2024-07-24:08:44:52,649 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:44:52,649 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:44:52,649 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:44:52,649 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:44:52,649 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:44:52,649 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:44:52,649 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:44:52,650 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/165 [00:10<28:28, 10.42s/it]Running generate_until requests:   1%|          | 2/165 [00:18<24:18,  8.95s/it]Running generate_until requests:   2%|▏         | 3/165 [00:30<28:17, 10.48s/it]Running generate_until requests:   2%|▏         | 4/165 [00:38<25:24,  9.47s/it]Running generate_until requests:   3%|▎         | 5/165 [00:42<19:36,  7.35s/it]Running generate_until requests:   3%|▎         | 5/165 [00:48<26:07,  9.79s/it]
