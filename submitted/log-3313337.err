Already on 'addinggriffin'
Your configuration specifies to merge with the ref 'refs/heads/addinggriffin'
from the remote, but no such ref was fetched.
warning: fetch updated the current branch head.
fast-forwarding your working tree from
commit 61d54b140eb3b9aa431772ccfc63f9102d65c31e.
error: Your local changes to the following files would be overwritten by merge:
	runtest.sh
Please commit your changes or stash them before you merge.
Aborting
fatal: Cannot fast-forward your working tree.
After making sure that you saved anything precious from
$ git diff 61d54b140eb3b9aa431772ccfc63f9102d65c31e
output, run
$ git reset --hard
to recover.
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:08, 22.74s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.74s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:12, 24.27s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:15, 25.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:39<00:38, 19.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:40<00:39, 19.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:41<00:40, 20.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:41<00:39, 19.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:41<00:39, 19.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:41<00:39, 19.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:40, 20.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:41<00:40, 20.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:55<00:17, 17.27s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.33s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:57<00:18, 18.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:57<00:18, 18.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 11.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 14.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 11.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 14.67s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 11.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 14.67s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 11.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 14.68s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 11.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 14.68s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 11.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 14.69s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.00s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 11.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 14.84s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:369: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:369: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:369: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:369: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:369: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:369: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:369: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:369: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:04<02:23,  4.62s/it]  3%|▎         | 1/32 [00:05<02:41,  5.22s/it]  3%|▎         | 1/32 [00:05<02:52,  5.58s/it]  3%|▎         | 1/32 [00:05<02:47,  5.40s/it]  0%|          | 0/32 [00:06<?, ?it/s]
  6%|▋         | 2/32 [00:06<01:28,  2.96s/it]  3%|▎         | 1/32 [00:07<03:58,  7.70s/it]  6%|▋         | 2/32 [00:07<01:51,  3.70s/it]  3%|▎         | 1/32 [00:09<04:58,  9.62s/it]  6%|▋         | 2/32 [00:11<02:55,  5.84s/it]  6%|▋         | 2/32 [00:16<04:17,  8.58s/it]  6%|▋         | 2/32 [00:16<04:05,  8.18s/it]  9%|▉         | 3/32 [00:18<02:42,  5.60s/it]  9%|▉         | 3/32 [00:17<02:59,  6.21s/it] 12%|█▎        | 4/32 [00:27<03:32,  7.60s/it] 12%|█▎        | 4/32 [00:28<03:31,  7.54s/it] 16%|█▌        | 5/32 [00:32<02:43,  6.05s/it] 19%|█▉        | 6/32 [00:33<01:53,  4.38s/it]  3%|▎         | 1/32 [00:34<17:46, 34.39s/it] 22%|██▏       | 7/32 [00:34<01:25,  3.41s/it]  6%|▋         | 2/32 [00:37<08:08, 16.28s/it]  9%|▉         | 3/32 [00:40<04:46,  9.89s/it]  9%|▉         | 3/32 [00:40<08:12, 16.98s/it]  6%|▋         | 2/32 [00:40<11:22, 22.75s/it]  9%|▉         | 3/32 [00:41<08:25, 17.45s/it]  9%|▉         | 3/32 [00:46<07:05, 14.68s/it] 12%|█▎        | 4/32 [00:47<04:03,  8.68s/it] 12%|█▎        | 4/32 [00:47<06:09, 13.21s/it] 16%|█▌        | 5/32 [00:49<02:48,  6.24s/it] 16%|█▌        | 5/32 [01:03<05:43, 12.72s/it]
 12%|█▎        | 4/32 [01:03<07:22, 15.81s/it]
 25%|██▌       | 8/32 [01:08<05:12, 13.00s/it] 12%|█▎        | 4/32 [01:08<07:58, 17.09s/it]
  9%|▉         | 3/32 [01:11<14:12, 29.40s/it] 28%|██▊       | 9/32 [01:14<04:13, 11.01s/it] 12%|█▎        | 4/32 [01:15<11:06, 23.81s/it] 31%|███▏      | 10/32 [01:19<03:20,  9.10s/it] 12%|█▎        | 4/32 [01:19<10:17, 22.07s/it] 34%|███▍      | 11/32 [01:20<02:20,  6.71s/it] 12%|█▎        | 4/32 [01:46<14:51, 31.83s/it] 16%|█▌        | 5/32 [01:49<12:19, 27.40s/it] 12%|█▎        | 4/32 [01:50<12:54, 27.65s/it]
 19%|█▉        | 6/32 [01:54<08:40, 20.03s/it] 22%|██▏       | 7/32 [01:58<06:08, 14.75s/it] 38%|███▊      | 12/32 [01:59<05:28, 16.40s/it] 25%|██▌       | 8/32 [02:02<04:31, 11.32s/it] 41%|████      | 13/32 [02:03<03:58, 12.56s/it] 28%|██▊       | 9/32 [02:06<03:25,  8.93s/it] 44%|████▍     | 14/32 [02:09<03:12, 10.67s/it] 31%|███▏      | 10/32 [02:10<02:43,  7.42s/it] 31%|███▏      | 10/32 [02:20<05:09, 14.05s/it]
 16%|█▌        | 5/32 [02:21<14:46, 32.83s/it] 47%|████▋     | 15/32 [02:52<05:45, 20.32s/it] 19%|█▉        | 6/32 [02:56<14:31, 33.53s/it] 22%|██▏       | 7/32 [03:01<10:06, 24.28s/it] 25%|██▌       | 8/32 [03:07<07:20, 18.35s/it] 28%|██▊       | 9/32 [03:11<05:16, 13.78s/it] 31%|███▏      | 10/32 [03:16<04:08, 11.31s/it] 34%|███▍      | 11/32 [03:18<02:58,  8.48s/it] 38%|███▊      | 12/32 [03:21<02:14,  6.72s/it] 41%|████      | 13/32 [03:25<01:48,  5.72s/it] 44%|████▍     | 14/32 [03:29<01:33,  5.20s/it] 47%|████▋     | 15/32 [03:33<04:01, 14.23s/it]
 47%|████▋     | 15/32 [03:34<01:27,  5.13s/it] 50%|█████     | 16/32 [03:39<01:24,  5.31s/it] 53%|█████▎    | 17/32 [03:43<01:12,  4.82s/it] 56%|█████▋    | 18/32 [03:45<00:56,  4.04s/it] 56%|█████▋    | 18/32 [04:17<03:20, 14.33s/it]
