Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.16s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.04s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:58, 19.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.07s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.16s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:59, 19.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.25s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:32<00:31, 15.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:33, 16.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:33, 16.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:33, 16.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:45<00:14, 14.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:47<00:14, 14.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 15.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00,  9.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00, 11.85s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00,  9.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00, 12.00s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.17s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00,  9.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00, 11.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.09s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.08s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.08s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:24<12:44, 24.66s/it]  3%|▎         | 1/32 [00:25<13:01, 25.22s/it]  3%|▎         | 1/32 [00:25<13:19, 25.80s/it]  3%|▎         | 1/32 [00:26<13:39, 26.45s/it]  3%|▎         | 1/32 [00:32<16:49, 32.58s/it]  3%|▎         | 1/32 [00:45<23:42, 45.89s/it]  3%|▎         | 1/32 [00:46<23:58, 46.40s/it]  6%|▋         | 2/32 [00:47<11:44, 23.50s/it]  6%|▋         | 2/32 [00:49<12:13, 24.44s/it]  6%|▋         | 2/32 [00:50<12:41, 25.37s/it]  6%|▋         | 2/32 [00:52<13:05, 26.18s/it]  6%|▋         | 2/32 [00:53<12:54, 25.81s/it]  3%|▎         | 1/32 [01:01<31:53, 61.74s/it]  9%|▉         | 3/32 [01:08<10:41, 22.14s/it]  9%|▉         | 3/32 [01:11<11:15, 23.31s/it]  9%|▉         | 3/32 [01:15<11:54, 24.65s/it]  9%|▉         | 3/32 [01:18<12:20, 25.53s/it]  9%|▉         | 3/32 [01:20<13:23, 27.69s/it]  6%|▋         | 2/32 [01:24<20:47, 41.59s/it] 12%|█▎        | 4/32 [01:30<10:21, 22.18s/it]  6%|▋         | 2/32 [01:33<23:17, 46.58s/it] 12%|█▎        | 4/32 [01:38<11:33, 24.77s/it] 12%|█▎        | 4/32 [01:39<11:04, 23.72s/it]  6%|▋         | 2/32 [01:41<24:18, 48.62s/it] 12%|█▎        | 4/32 [01:52<13:46, 29.51s/it] 12%|█▎        | 4/32 [01:53<14:00, 30.00s/it]  9%|▉         | 3/32 [01:57<18:12, 37.67s/it] 16%|█▌        | 5/32 [02:00<11:13, 24.95s/it] 16%|█▌        | 5/32 [02:00<10:46, 23.95s/it] 16%|█▌        | 5/32 [02:06<11:06, 24.69s/it] 16%|█▌        | 5/32 [02:14<12:02, 26.75s/it] 16%|█▌        | 5/32 [02:17<12:32, 27.86s/it]  9%|▉         | 3/32 [02:18<22:20, 46.23s/it]  9%|▉         | 3/32 [02:22<21:57, 45.44s/it] 19%|█▉        | 6/32 [02:25<10:25, 24.05s/it] 19%|█▉        | 6/32 [02:26<10:57, 25.29s/it] 12%|█▎        | 4/32 [02:27<16:07, 34.55s/it] 19%|█▉        | 6/32 [02:28<10:18, 23.79s/it] 19%|█▉        | 6/32 [02:36<10:51, 25.07s/it] 19%|█▉        | 6/32 [02:46<12:14, 28.24s/it] 22%|██▏       | 7/32 [02:47<09:58, 23.92s/it] 22%|██▏       | 7/32 [02:48<09:53, 23.75s/it] 22%|██▏       | 7/32 [02:51<09:51, 23.65s/it] 12%|█▎        | 4/32 [02:55<19:44, 42.30s/it] 22%|██▏       | 7/32 [02:56<09:46, 23.44s/it] 16%|█▌        | 5/32 [03:02<15:41, 34.87s/it] 25%|██▌       | 8/32 [03:09<09:21, 23.41s/it] 12%|█▎        | 4/32 [03:11<21:43, 46.56s/it] 22%|██▏       | 7/32 [03:11<11:17, 27.09s/it] 25%|██▌       | 8/32 [03:12<09:33, 23.90s/it] 25%|██▌       | 8/32 [03:16<09:34, 23.93s/it] 25%|██▌       | 8/32 [03:19<09:20, 23.33s/it] 19%|█▉        | 6/32 [03:31<14:14, 32.86s/it] 16%|█▌        | 5/32 [03:31<18:03, 40.11s/it] 28%|██▊       | 9/32 [03:34<09:12, 24.00s/it] 25%|██▌       | 8/32 [03:35<10:26, 26.12s/it] 28%|██▊       | 9/32 [03:37<08:53, 23.20s/it] 28%|██▊       | 9/32 [03:38<09:25, 24.58s/it] 28%|██▊       | 9/32 [03:40<08:37, 22.50s/it] 31%|███▏      | 10/32 [03:56<08:32, 23.29s/it] 31%|███▏      | 10/32 [04:01<08:31, 23.24s/it] 16%|█▌        | 5/32 [04:01<21:35, 47.96s/it] 28%|██▊       | 9/32 [04:02<10:08, 26.46s/it] 22%|██▏       | 7/32 [04:03<13:36, 32.67s/it] 31%|███▏      | 10/32 [04:04<09:07, 24.86s/it] 31%|███▏      | 10/32 [04:04<08:27, 23.05s/it] 19%|█▉        | 6/32 [04:15<17:55, 41.36s/it] 34%|███▍      | 11/32 [04:23<08:04, 23.09s/it] 34%|███▍      | 11/32 [04:23<08:34, 24.49s/it] 34%|███▍      | 11/32 [04:25<08:23, 23.96s/it] 31%|███▏      | 10/32 [04:28<09:38, 26.30s/it] 34%|███▍      | 11/32 [04:31<08:27, 24.18s/it] 19%|█▉        | 6/32 [04:40<19:24, 44.78s/it] 25%|██▌       | 8/32 [04:45<14:15, 35.67s/it] 38%|███▊      | 12/32 [04:49<07:58, 23.91s/it] 22%|██▏       | 7/32 [04:50<16:25, 39.41s/it] 34%|███▍      | 11/32 [04:52<08:58, 25.65s/it] 38%|███▊      | 12/32 [04:53<07:50, 23.53s/it] 38%|███▊      | 12/32 [04:55<08:35, 25.79s/it] 38%|███▊      | 12/32 [04:57<09:07, 27.39s/it] 41%|████      | 13/32 [05:12<07:29, 23.65s/it] 41%|████      | 13/32 [05:15<07:17, 23.03s/it] 28%|██▊       | 9/32 [05:16<13:03, 34.08s/it] 41%|████      | 13/32 [05:20<08:03, 25.42s/it] 38%|███▊      | 12/32 [05:20<08:45, 26.29s/it] 22%|██▏       | 7/32 [05:20<18:06, 43.45s/it] 41%|████      | 13/32 [05:20<08:14, 26.04s/it] 25%|██▌       | 8/32 [05:32<16:02, 40.11s/it] 44%|████▍     | 14/32 [05:38<07:15, 24.19s/it] 44%|████▍     | 14/32 [05:39<07:01, 23.42s/it] 41%|████      | 13/32 [05:42<07:56, 25.10s/it] 44%|████▍     | 14/32 [05:43<07:31, 25.11s/it] 44%|████▍     | 14/32 [05:46<07:40, 25.56s/it] 31%|███▏      | 10/32 [05:49<12:22, 33.77s/it] 25%|██▌       | 8/32 [05:57<16:27, 41.16s/it] 47%|████▋     | 15/32 [06:00<06:42, 23.69s/it] 47%|████▋     | 15/32 [06:03<06:39, 23.52s/it] 44%|████▍     | 14/32 [06:06<07:22, 24.61s/it] 47%|████▋     | 15/32 [06:09<07:03, 24.88s/it] 47%|████▋     | 15/32 [06:10<07:17, 25.73s/it] 28%|██▊       | 9/32 [06:11<15:18, 39.92s/it] 50%|█████     | 16/32 [06:20<06:01, 22.59s/it] 34%|███▍      | 11/32 [06:29<12:25, 35.48s/it] 47%|████▋     | 15/32 [06:28<06:49, 24.06s/it] 50%|█████     | 16/32 [06:35<06:45, 25.33s/it] 50%|█████     | 16/32 [06:36<06:49, 25.61s/it] 50%|█████     | 16/32 [06:38<07:12, 27.03s/it] 53%|█████▎    | 17/32 [06:42<05:35, 22.38s/it] 31%|███▏      | 10/32 [06:48<14:14, 38.83s/it] 50%|█████     | 16/32 [06:52<06:22, 23.90s/it] 38%|███▊      | 12/32 [06:57<11:06, 33.31s/it] 53%|█████▎    | 17/32 [06:59<06:11, 24.74s/it] 28%|██▊       | 9/32 [07:01<18:35, 48.50s/it] 56%|█████▋    | 18/32 [07:02<05:00, 21.49s/it] 53%|█████▎    | 17/32 [07:04<06:38, 26.58s/it] 53%|█████▎    | 17/32 [07:07<06:48, 27.21s/it] 53%|█████▎    | 17/32 [07:14<05:51, 23.40s/it] 56%|█████▋    | 18/32 [07:24<05:48, 24.88s/it] 41%|████      | 13/32 [07:26<10:08, 32.02s/it] 59%|█████▉    | 19/32 [07:27<04:55, 22.70s/it] 56%|█████▋    | 18/32 [07:28<06:03, 25.93s/it] 34%|███▍      | 11/32 [07:36<14:33, 41.62s/it] 56%|█████▋    | 18/32 [07:36<05:20, 22.90s/it] 56%|█████▋    | 18/32 [07:36<06:31, 27.97s/it] 62%|██████▎   | 20/32 [07:49<04:27, 22.30s/it] 59%|█████▉    | 19/32 [07:50<05:28, 25.25s/it] 59%|█████▉    | 19/32 [07:52<05:28, 25.30s/it] 31%|███▏      | 10/32 [07:56<18:28, 50.40s/it] 59%|█████▉    | 19/32 [07:58<05:36, 25.90s/it] 59%|█████▉    | 19/32 [07:58<04:55, 22.72s/it] 44%|████▍     | 14/32 [08:01<09:50, 32.83s/it] 38%|███▊      | 12/32 [08:10<13:07, 39.38s/it] 62%|██████▎   | 20/32 [08:13<04:55, 24.60s/it] 66%|██████▌   | 21/32 [08:16<04:23, 23.96s/it] 62%|██████▎   | 20/32 [08:17<05:02, 25.17s/it] 62%|██████▎   | 20/32 [08:17<04:48, 24.08s/it] 62%|██████▎   | 20/32 [08:20<04:30, 22.56s/it] 47%|████▋     | 15/32 [08:34<09:18, 32.86s/it] 34%|███▍      | 11/32 [08:36<16:34, 47.38s/it] 66%|██████▌   | 21/32 [08:39<04:16, 23.34s/it] 66%|██████▌   | 21/32 [08:41<04:32, 24.73s/it] 69%|██████▉   | 22/32 [08:44<04:11, 25.14s/it] 66%|██████▌   | 21/32 [08:44<04:52, 26.55s/it] 66%|██████▌   | 21/32 [08:46<04:17, 23.40s/it] 41%|████      | 13/32 [08:54<12:53, 40.72s/it] 50%|█████     | 16/32 [09:00<08:16, 31.00s/it] 69%|██████▉   | 22/32 [09:07<04:13, 25.32s/it] 69%|██████▉   | 22/32 [09:07<04:08, 24.86s/it] 72%|███████▏  | 23/32 [09:09<03:45, 25.05s/it] 69%|██████▉   | 22/32 [09:10<03:55, 23.60s/it] 69%|██████▉   | 22/32 [09:12<04:30, 27.01s/it] 38%|███▊      | 12/32 [09:27<16:05, 48.30s/it] 72%|███████▏  | 23/32 [09:30<03:21, 22.43s/it] 72%|███████▏  | 23/32 [09:30<03:40, 24.45s/it] 72%|███████▏  | 23/32 [09:33<03:44, 24.99s/it] 72%|███████▏  | 23/32 [09:33<03:47, 25.24s/it] 53%|█████▎    | 17/32 [09:34<07:57, 31.82s/it] 75%|███████▌  | 24/32 [09:35<03:22, 25.33s/it] 44%|████▍     | 14/32 [09:35<12:17, 40.99s/it] 75%|███████▌  | 24/32 [09:53<03:12, 24.05s/it] 75%|███████▌  | 24/32 [09:54<03:10, 23.86s/it] 75%|███████▌  | 24/32 [09:59<03:15, 24.39s/it] 75%|███████▌  | 24/32 [09:59<03:22, 25.31s/it] 78%|███████▊  | 25/32 [10:01<02:57, 25.40s/it] 56%|█████▋    | 18/32 [10:04<07:17, 31.24s/it] 41%|████      | 13/32 [10:11<14:52, 46.98s/it] 78%|███████▊  | 25/32 [10:16<02:46, 23.78s/it] 78%|███████▊  | 25/32 [10:17<02:45, 23.63s/it] 47%|████▋     | 15/32 [10:19<11:51, 41.84s/it] 81%|████████▏ | 26/32 [10:24<02:28, 24.72s/it] 78%|███████▊  | 25/32 [10:25<02:59, 25.69s/it] 59%|█████▉    | 19/32 [10:31<06:31, 30.08s/it] 78%|███████▊  | 25/32 [10:34<03:14, 27.79s/it] 81%|████████▏ | 26/32 [10:44<02:27, 24.58s/it] 81%|████████▏ | 26/32 [10:48<02:28, 24.69s/it] 81%|████████▏ | 26/32 [10:50<02:41, 26.84s/it] 84%|████████▍ | 27/32 [10:52<02:09, 25.86s/it] 44%|████▍     | 14/32 [10:53<13:37, 45.39s/it] 50%|█████     | 16/32 [10:59<10:58, 41.15s/it] 81%|████████▏ | 26/32 [11:00<02:43, 27.23s/it] 84%|████████▍ | 27/32 [11:08<02:01, 24.32s/it] 84%|████████▍ | 27/32 [11:09<01:57, 23.58s/it] 62%|██████▎   | 20/32 [11:09<06:28, 32.41s/it] 84%|████████▍ | 27/32 [11:15<02:11, 26.26s/it] 88%|████████▊ | 28/32 [11:16<01:40, 25.18s/it] 84%|████████▍ | 27/32 [11:27<02:14, 26.99s/it] 88%|████████▊ | 28/32 [11:30<01:34, 23.73s/it] 47%|████▋     | 15/32 [11:32<12:20, 43.58s/it] 88%|████████▊ | 28/32 [11:33<01:35, 23.89s/it] 88%|████████▊ | 28/32 [11:38<01:41, 25.47s/it] 66%|██████▌   | 21/32 [11:39<05:47, 31.60s/it] 53%|█████▎    | 17/32 [11:39<10:14, 40.97s/it] 91%|█████████ | 29/32 [11:43<01:16, 25.62s/it] 88%|████████▊ | 28/32 [11:50<01:43, 25.94s/it] 91%|█████████ | 29/32 [11:54<01:11, 23.90s/it] 91%|█████████ | 29/32 [11:56<01:10, 23.61s/it] 94%|█████████▍| 30/32 [12:05<00:39, 19.74s/it] 94%|█████████▍| 30/32 [12:08<00:51, 25.56s/it] 91%|█████████ | 29/32 [12:15<01:16, 25.56s/it] 69%|██████▉   | 22/32 [12:17<05:36, 33.68s/it] 56%|█████▋    | 18/32 [12:19<09:27, 40.53s/it] 94%|█████████▍| 30/32 [12:22<00:48, 24.11s/it] 94%|█████████▍| 30/32 [12:24<00:50, 25.49s/it] 50%|█████     | 16/32 [12:29<12:41, 47.60s/it] 97%|█████████▋| 31/32 [12:31<00:24, 24.77s/it] 97%|█████████▋| 31/32 [12:34<00:22, 22.18s/it] 94%|█████████▍| 30/32 [12:43<00:52, 26.23s/it] 97%|█████████▋| 31/32 [12:46<00:24, 24.30s/it] 97%|█████████▋| 31/32 [12:48<00:25, 25.32s/it] 72%|███████▏  | 23/32 [12:50<05:01, 33.48s/it]100%|██████████| 32/32 [12:55<00:00, 24.45s/it]100%|██████████| 32/32 [12:55<00:00, 24.22s/it]
100%|██████████| 32/32 [12:58<00:00, 22.58s/it]100%|██████████| 32/32 [12:58<00:00, 24.32s/it]
 59%|█████▉    | 19/32 [12:58<08:42, 40.21s/it] 97%|█████████▋| 31/32 [13:06<00:25, 25.39s/it] 53%|█████▎    | 17/32 [13:11<11:27, 45.83s/it]100%|██████████| 32/32 [13:13<00:00, 24.99s/it]100%|██████████| 32/32 [13:13<00:00, 24.79s/it]
100%|██████████| 32/32 [13:17<00:00, 26.24s/it]100%|██████████| 32/32 [13:17<00:00, 24.92s/it]
 75%|███████▌  | 24/32 [13:25<04:30, 33.75s/it]100%|██████████| 32/32 [13:32<00:00, 25.48s/it]100%|██████████| 32/32 [13:32<00:00, 25.38s/it]
 62%|██████▎   | 20/32 [13:38<07:59, 39.98s/it] 78%|███████▊  | 25/32 [13:51<03:41, 31.58s/it] 56%|█████▋    | 18/32 [13:51<10:19, 44.25s/it] 66%|██████▌   | 21/32 [14:17<07:18, 39.85s/it] 59%|█████▉    | 19/32 [14:26<08:59, 41.48s/it] 81%|████████▏ | 26/32 [14:33<03:28, 34.78s/it] 69%|██████▉   | 22/32 [14:56<06:34, 39.42s/it] 62%|██████▎   | 20/32 [15:07<08:14, 41.20s/it] 84%|████████▍ | 27/32 [15:17<03:06, 37.31s/it] 72%|███████▏  | 23/32 [15:35<05:54, 39.42s/it] 88%|████████▊ | 28/32 [15:47<02:20, 35.25s/it] 66%|██████▌   | 21/32 [15:51<07:42, 42.02s/it] 91%|█████████ | 29/32 [16:17<01:40, 33.54s/it] 75%|███████▌  | 24/32 [16:19<05:25, 40.73s/it] 69%|██████▉   | 22/32 [16:38<07:15, 43.53s/it] 94%|█████████▍| 30/32 [16:46<01:04, 32.13s/it] 78%|███████▊  | 25/32 [17:04<04:53, 41.96s/it] 72%|███████▏  | 23/32 [17:14<06:12, 41.34s/it] 97%|█████████▋| 31/32 [17:14<00:31, 31.14s/it] 81%|████████▏ | 26/32 [17:45<04:11, 41.86s/it]100%|██████████| 32/32 [17:46<00:00, 31.16s/it]100%|██████████| 32/32 [17:46<00:00, 33.31s/it]
 78%|███████▊  | 25/32 [17:52<03:37, 31.11s/it] 84%|████████▍ | 27/32 [18:32<03:36, 43.38s/it] 81%|████████▏ | 26/32 [18:36<03:25, 34.26s/it] 88%|████████▊ | 28/32 [19:07<02:43, 40.97s/it] 84%|████████▍ | 27/32 [19:14<02:56, 35.35s/it] 91%|█████████ | 29/32 [19:48<02:02, 40.84s/it] 88%|████████▊ | 28/32 [20:00<02:32, 38.25s/it] 94%|█████████▍| 30/32 [20:34<01:24, 42.36s/it] 91%|█████████ | 29/32 [20:47<02:01, 40.45s/it] 97%|█████████▋| 31/32 [21:15<00:41, 41.87s/it] 94%|█████████▍| 30/32 [21:37<01:26, 43.27s/it]100%|██████████| 32/32 [22:06<00:00, 44.67s/it]100%|██████████| 32/32 [22:06<00:00, 41.45s/it]
 97%|█████████▋| 31/32 [22:18<00:42, 42.57s/it]100%|██████████| 32/32 [22:48<00:00, 38.93s/it]100%|██████████| 32/32 [22:48<00:00, 42.76s/it]
