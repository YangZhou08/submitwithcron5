Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:50<00:50, 50.62s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.54s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.66s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:53<00:53, 53.17s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 28.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 28.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.49s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 29.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.49s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 28.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.43s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.66s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.81s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:19<10:14, 19.81s/it]  3%|▎         | 1/32 [00:21<11:14, 21.75s/it]  3%|▎         | 1/32 [00:21<11:10, 21.63s/it]  3%|▎         | 1/32 [00:22<11:24, 22.07s/it]  3%|▎         | 1/32 [00:22<11:28, 22.21s/it]  3%|▎         | 1/32 [00:24<12:54, 25.00s/it]  6%|▋         | 2/32 [00:38<09:26, 18.89s/it]  3%|▎         | 1/32 [00:38<19:55, 38.55s/it]  6%|▋         | 2/32 [00:40<10:01, 20.04s/it]  3%|▎         | 1/32 [00:40<21:05, 40.81s/it]  6%|▋         | 2/32 [00:41<10:16, 20.57s/it]  6%|▋         | 2/32 [00:42<10:38, 21.29s/it]  6%|▋         | 2/32 [00:43<11:00, 22.03s/it]  6%|▋         | 2/32 [00:46<11:31, 23.06s/it]  9%|▉         | 3/32 [00:59<09:21, 19.35s/it]  9%|▉         | 3/32 [01:01<09:37, 19.92s/it]  9%|▉         | 3/32 [01:01<09:44, 20.16s/it]  9%|▉         | 3/32 [01:05<10:06, 20.90s/it]  9%|▉         | 3/32 [01:06<10:40, 22.09s/it]  9%|▉         | 3/32 [01:06<11:09, 23.08s/it]  6%|▋         | 2/32 [01:11<17:38, 35.29s/it]  6%|▋         | 2/32 [01:15<18:35, 37.19s/it] 12%|█▎        | 4/32 [01:20<09:12, 19.74s/it] 12%|█▎        | 4/32 [01:20<09:11, 19.69s/it] 12%|█▎        | 4/32 [01:23<09:54, 21.24s/it] 12%|█▎        | 4/32 [01:23<09:16, 19.89s/it] 12%|█▎        | 4/32 [01:27<10:12, 21.89s/it] 12%|█▎        | 4/32 [01:29<10:53, 23.34s/it] 16%|█▌        | 5/32 [01:38<08:38, 19.20s/it] 16%|█▌        | 5/32 [01:40<08:53, 19.76s/it]  9%|▉         | 3/32 [01:43<16:21, 33.83s/it] 16%|█▌        | 5/32 [01:45<09:46, 21.72s/it] 16%|█▌        | 5/32 [01:46<09:20, 20.77s/it] 16%|█▌        | 5/32 [01:47<09:38, 21.42s/it] 16%|█▌        | 5/32 [01:54<10:41, 23.76s/it]  9%|▉         | 3/32 [01:55<18:31, 38.31s/it] 19%|█▉        | 6/32 [01:57<08:10, 18.88s/it] 19%|█▉        | 6/32 [02:00<08:43, 20.14s/it] 19%|█▉        | 6/32 [02:06<09:11, 21.21s/it] 19%|█▉        | 6/32 [02:07<09:00, 20.78s/it] 19%|█▉        | 6/32 [02:10<09:28, 21.86s/it] 22%|██▏       | 7/32 [02:15<07:46, 18.67s/it] 12%|█▎        | 4/32 [02:15<15:27, 33.12s/it] 22%|██▏       | 7/32 [02:20<08:20, 20.01s/it] 19%|█▉        | 6/32 [02:21<10:43, 24.75s/it] 22%|██▏       | 7/32 [02:24<08:27, 20.30s/it] 22%|██▏       | 7/32 [02:25<08:19, 19.97s/it] 12%|█▎        | 4/32 [02:27<16:47, 35.99s/it] 25%|██▌       | 8/32 [02:33<07:25, 18.55s/it] 22%|██▏       | 7/32 [02:36<09:42, 23.31s/it] 25%|██▌       | 8/32 [02:39<07:50, 19.58s/it] 25%|██▌       | 8/32 [02:43<07:54, 19.78s/it] 25%|██▌       | 8/32 [02:45<08:00, 20.03s/it] 16%|█▌        | 5/32 [02:47<14:41, 32.64s/it] 22%|██▏       | 7/32 [02:49<10:47, 25.91s/it] 28%|██▊       | 9/32 [02:53<07:16, 18.99s/it] 25%|██▌       | 8/32 [02:57<08:57, 22.40s/it] 28%|██▊       | 9/32 [02:57<07:23, 19.29s/it] 16%|█▌        | 5/32 [02:59<15:33, 34.57s/it] 28%|██▊       | 9/32 [03:04<07:42, 20.13s/it] 28%|██▊       | 9/32 [03:04<07:36, 19.86s/it] 31%|███▏      | 10/32 [03:13<07:04, 19.30s/it] 28%|██▊       | 9/32 [03:17<08:17, 21.62s/it] 31%|███▏      | 10/32 [03:18<07:10, 19.58s/it] 25%|██▌       | 8/32 [03:19<10:56, 27.34s/it] 19%|█▉        | 6/32 [03:23<14:38, 33.77s/it] 31%|███▏      | 10/32 [03:26<07:29, 20.45s/it] 31%|███▏      | 10/32 [03:27<07:42, 21.03s/it] 34%|███▍      | 11/32 [03:32<06:42, 19.17s/it] 34%|███▍      | 11/32 [03:37<06:48, 19.46s/it] 31%|███▏      | 10/32 [03:38<07:51, 21.45s/it] 19%|█▉        | 6/32 [03:42<16:11, 37.35s/it] 28%|██▊       | 9/32 [03:44<10:08, 26.48s/it] 34%|███▍      | 11/32 [03:47<07:16, 20.79s/it] 34%|███▍      | 11/32 [03:48<07:18, 20.88s/it] 38%|███▊      | 12/32 [03:55<06:48, 20.44s/it] 34%|███▍      | 11/32 [03:56<07:12, 20.62s/it] 22%|██▏       | 7/32 [03:59<14:23, 34.52s/it] 38%|███▊      | 12/32 [04:02<07:02, 21.14s/it] 38%|███▊      | 12/32 [04:06<06:46, 20.31s/it] 31%|███▏      | 10/32 [04:08<09:23, 25.64s/it] 38%|███▊      | 12/32 [04:12<07:14, 21.71s/it] 41%|████      | 13/32 [04:14<06:16, 19.79s/it] 22%|██▏       | 7/32 [04:16<15:08, 36.36s/it] 38%|███▊      | 12/32 [04:18<06:57, 20.85s/it] 41%|████      | 13/32 [04:24<06:46, 21.40s/it] 41%|████      | 13/32 [04:25<06:15, 19.75s/it] 41%|████      | 13/32 [04:31<06:36, 20.89s/it] 44%|████▍     | 14/32 [04:32<05:51, 19.50s/it] 34%|███▍      | 11/32 [04:36<09:14, 26.41s/it] 25%|██▌       | 8/32 [04:37<14:15, 35.65s/it] 41%|████      | 13/32 [04:38<06:32, 20.68s/it] 44%|████▍     | 14/32 [04:43<06:10, 20.58s/it] 44%|████▍     | 14/32 [04:44<05:52, 19.56s/it] 44%|████▍     | 14/32 [04:49<06:02, 20.14s/it] 25%|██▌       | 8/32 [04:50<14:12, 35.50s/it] 47%|████▋     | 15/32 [04:52<05:33, 19.65s/it] 44%|████▍     | 14/32 [04:57<06:00, 20.03s/it] 47%|████▋     | 15/32 [05:01<05:39, 19.99s/it] 47%|████▋     | 15/32 [05:03<05:33, 19.59s/it] 38%|███▊      | 12/32 [05:05<09:06, 27.32s/it] 47%|████▋     | 15/32 [05:08<05:33, 19.61s/it] 47%|████▋     | 15/32 [05:15<05:32, 19.57s/it] 50%|█████     | 16/32 [05:16<05:34, 20.94s/it] 28%|██▊       | 9/32 [05:19<14:25, 37.64s/it] 50%|█████     | 16/32 [05:20<05:12, 19.56s/it] 50%|█████     | 16/32 [05:24<05:16, 19.77s/it] 28%|██▊       | 9/32 [05:25<13:37, 35.55s/it] 41%|████      | 13/32 [05:26<08:04, 25.47s/it] 50%|█████     | 16/32 [05:28<05:18, 19.92s/it] 50%|█████     | 16/32 [05:38<05:27, 20.48s/it] 53%|█████▎    | 17/32 [05:39<05:20, 21.36s/it] 53%|█████▎    | 17/32 [05:41<05:02, 20.20s/it] 44%|████▍     | 14/32 [05:46<07:07, 23.73s/it] 53%|█████▎    | 17/32 [05:48<05:17, 21.19s/it] 53%|█████▎    | 17/32 [05:49<05:04, 20.33s/it] 31%|███▏      | 10/32 [05:52<13:18, 36.28s/it] 56%|█████▋    | 18/32 [05:58<04:48, 20.62s/it] 53%|█████▎    | 17/32 [06:01<05:19, 21.27s/it] 56%|█████▋    | 18/32 [06:01<04:40, 20.06s/it] 31%|███▏      | 10/32 [06:02<13:09, 35.87s/it] 56%|█████▋    | 18/32 [06:07<04:47, 20.55s/it] 47%|████▋     | 15/32 [06:08<06:34, 23.18s/it] 56%|█████▋    | 18/32 [06:08<04:38, 19.92s/it] 59%|█████▉    | 19/32 [06:16<04:18, 19.91s/it] 56%|█████▋    | 18/32 [06:21<04:53, 20.95s/it] 59%|█████▉    | 19/32 [06:22<04:23, 20.30s/it] 59%|█████▉    | 19/32 [06:26<04:21, 20.11s/it] 34%|███▍      | 11/32 [06:26<12:26, 35.54s/it] 50%|█████     | 16/32 [06:28<05:56, 22.26s/it] 59%|█████▉    | 19/32 [06:28<04:19, 19.97s/it] 62%|██████▎   | 20/32 [06:37<04:03, 20.29s/it] 59%|█████▉    | 19/32 [06:42<04:34, 21.10s/it] 62%|██████▎   | 20/32 [06:42<04:03, 20.33s/it] 62%|██████▎   | 20/32 [06:45<03:58, 19.84s/it] 34%|███▍      | 11/32 [06:47<13:28, 38.51s/it] 53%|█████▎    | 17/32 [06:49<05:27, 21.82s/it] 62%|██████▎   | 20/32 [06:51<04:10, 20.87s/it] 66%|██████▌   | 21/32 [06:55<03:36, 19.69s/it] 62%|██████▎   | 20/32 [07:01<04:03, 20.33s/it] 66%|██████▌   | 21/32 [07:01<03:37, 19.80s/it] 66%|██████▌   | 21/32 [07:07<03:45, 20.49s/it] 38%|███▊      | 12/32 [07:08<12:26, 37.31s/it] 56%|█████▋    | 18/32 [07:08<04:54, 21.03s/it] 66%|██████▌   | 21/32 [07:14<03:54, 21.32s/it] 69%|██████▉   | 22/32 [07:15<03:16, 19.63s/it] 38%|███▊      | 12/32 [07:19<12:13, 36.65s/it] 69%|██████▉   | 22/32 [07:21<03:19, 19.95s/it] 66%|██████▌   | 21/32 [07:24<03:52, 21.16s/it] 69%|██████▉   | 22/32 [07:28<03:25, 20.58s/it] 69%|██████▉   | 22/32 [07:32<03:24, 20.46s/it] 72%|███████▏  | 23/32 [07:33<02:53, 19.24s/it] 59%|█████▉    | 19/32 [07:33<04:48, 22.17s/it] 41%|████      | 13/32 [07:40<11:19, 35.78s/it] 72%|███████▏  | 23/32 [07:41<02:57, 19.76s/it] 69%|██████▉   | 22/32 [07:47<03:38, 21.81s/it] 72%|███████▏  | 23/32 [07:49<03:04, 20.52s/it] 72%|███████▏  | 23/32 [07:51<02:58, 19.87s/it] 78%|███████▊  | 25/32 [07:52<01:43, 14.72s/it] 41%|████      | 13/32 [07:53<11:24, 36.00s/it] 62%|██████▎   | 20/32 [07:57<04:32, 22.68s/it] 75%|███████▌  | 24/32 [07:59<02:35, 19.43s/it] 72%|███████▏  | 23/32 [08:09<03:15, 21.67s/it] 75%|███████▌  | 24/32 [08:09<02:44, 20.62s/it] 75%|███████▌  | 24/32 [08:10<02:38, 19.79s/it] 81%|████████▏ | 26/32 [08:11<01:34, 15.75s/it] 78%|███████▊  | 25/32 [08:18<02:15, 19.34s/it] 66%|██████▌   | 21/32 [08:21<04:15, 23.23s/it] 44%|████▍     | 14/32 [08:22<11:18, 37.69s/it] 75%|███████▌  | 24/32 [08:28<02:48, 21.06s/it] 78%|███████▊  | 25/32 [08:29<02:15, 19.38s/it] 78%|███████▊  | 25/32 [08:30<02:24, 20.68s/it] 44%|████▍     | 14/32 [08:31<10:55, 36.43s/it] 84%|████████▍ | 27/32 [08:33<01:27, 17.47s/it] 81%|████████▏ | 26/32 [08:43<02:06, 21.06s/it] 69%|██████▉   | 22/32 [08:45<03:54, 23.44s/it] 78%|███████▊  | 25/32 [08:47<02:22, 20.32s/it] 81%|████████▏ | 26/32 [08:47<01:54, 19.10s/it] 81%|████████▏ | 26/32 [08:49<02:01, 20.23s/it] 47%|████▋     | 15/32 [08:54<10:12, 36.01s/it] 88%|████████▊ | 28/32 [08:55<01:14, 18.63s/it] 84%|████████▍ | 27/32 [09:06<01:34, 18.89s/it] 81%|████████▏ | 26/32 [09:07<02:00, 20.13s/it] 47%|████▋     | 15/32 [09:07<10:19, 36.47s/it] 84%|████████▍ | 27/32 [09:08<01:51, 22.23s/it] 84%|████████▍ | 27/32 [09:12<01:44, 20.91s/it] 91%|█████████ | 29/32 [09:18<00:59, 19.81s/it] 72%|███████▏  | 23/32 [09:19<03:59, 26.65s/it] 84%|████████▍ | 27/32 [09:25<01:38, 19.64s/it] 88%|████████▊ | 28/32 [09:28<01:25, 21.31s/it] 88%|████████▊ | 28/32 [09:28<01:19, 19.93s/it] 88%|████████▊ | 28/32 [09:32<01:22, 20.72s/it] 94%|█████████▍| 30/32 [09:40<00:40, 20.37s/it] 50%|█████     | 16/32 [09:42<09:33, 35.87s/it] 88%|████████▊ | 28/32 [09:46<01:20, 20.03s/it] 75%|███████▌  | 24/32 [09:46<03:33, 26.70s/it] 94%|█████████▍| 30/32 [09:47<00:32, 16.05s/it] 91%|█████████ | 29/32 [09:49<01:00, 20.16s/it] 50%|█████     | 16/32 [09:49<11:07, 41.73s/it] 91%|█████████ | 29/32 [09:51<01:00, 20.25s/it] 97%|█████████▋| 31/32 [10:02<00:21, 21.08s/it] 91%|█████████ | 29/32 [10:05<00:59, 19.79s/it] 97%|█████████▋| 31/32 [10:08<00:17, 17.12s/it] 94%|█████████▍| 30/32 [10:09<00:40, 20.17s/it] 78%|███████▊  | 25/32 [10:13<03:07, 26.75s/it] 94%|█████████▍| 30/32 [10:15<00:42, 21.30s/it] 53%|█████▎    | 17/32 [10:16<08:51, 35.45s/it] 53%|█████▎    | 17/32 [10:24<09:56, 39.74s/it] 94%|█████████▍| 30/32 [10:25<00:39, 19.75s/it]100%|██████████| 32/32 [10:26<00:00, 17.50s/it]100%|██████████| 32/32 [10:26<00:00, 19.59s/it]
100%|██████████| 32/32 [10:28<00:00, 22.46s/it]100%|██████████| 32/32 [10:28<00:00, 19.65s/it]
 97%|█████████▋| 31/32 [10:28<00:19, 19.95s/it] 97%|█████████▋| 31/32 [10:34<00:20, 20.63s/it] 81%|████████▏ | 26/32 [10:39<02:39, 26.54s/it] 97%|█████████▋| 31/32 [10:45<00:19, 19.76s/it]100%|██████████| 32/32 [10:47<00:00, 19.48s/it]100%|██████████| 32/32 [10:47<00:00, 20.22s/it]
 56%|█████▋    | 18/32 [10:51<08:12, 35.16s/it]100%|██████████| 32/32 [10:54<00:00, 20.51s/it]100%|██████████| 32/32 [10:54<00:00, 20.47s/it]
 56%|█████▋    | 18/32 [10:57<08:48, 37.75s/it]100%|██████████| 32/32 [11:04<00:00, 19.53s/it]100%|██████████| 32/32 [11:04<00:00, 20.76s/it]
 84%|████████▍ | 27/32 [11:04<02:10, 26.11s/it] 59%|█████▉    | 19/32 [11:23<07:26, 34.31s/it] 88%|████████▊ | 28/32 [11:29<01:43, 25.81s/it] 59%|█████▉    | 19/32 [11:33<08:03, 37.22s/it] 91%|█████████ | 29/32 [11:54<01:16, 25.60s/it] 62%|██████▎   | 20/32 [11:57<06:48, 34.06s/it] 62%|██████▎   | 20/32 [12:09<07:22, 36.87s/it] 94%|█████████▍| 30/32 [12:21<00:51, 25.89s/it] 66%|██████▌   | 21/32 [12:40<06:46, 36.91s/it] 66%|██████▌   | 21/32 [12:41<06:29, 35.45s/it] 97%|█████████▋| 31/32 [12:48<00:26, 26.13s/it] 69%|██████▉   | 22/32 [13:13<05:55, 35.54s/it] 69%|██████▉   | 22/32 [13:16<05:53, 35.31s/it]100%|██████████| 32/32 [13:17<00:00, 27.13s/it]100%|██████████| 32/32 [13:17<00:00, 24.93s/it]
 72%|███████▏  | 23/32 [13:45<05:11, 34.61s/it] 72%|███████▏  | 23/32 [13:50<05:14, 34.94s/it] 75%|███████▌  | 24/32 [14:17<04:31, 33.93s/it] 75%|███████▌  | 24/32 [14:25<04:37, 34.66s/it] 78%|███████▊  | 25/32 [14:52<03:58, 34.06s/it] 78%|███████▊  | 25/32 [14:58<03:59, 34.18s/it] 81%|████████▏ | 26/32 [15:30<03:21, 33.55s/it] 81%|████████▏ | 26/32 [15:30<03:32, 35.36s/it] 84%|████████▍ | 27/32 [16:05<02:50, 34.03s/it] 84%|████████▍ | 27/32 [16:30<03:33, 42.64s/it] 88%|████████▊ | 28/32 [16:41<02:18, 34.66s/it]