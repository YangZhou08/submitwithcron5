Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:36<01:12, 36.01s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:37<01:14, 37.19s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:37<01:15, 37.83s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:37<01:14, 37.06s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:37<01:15, 37.78s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:37<01:15, 37.58s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:37<01:15, 37.98s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:37<01:15, 37.75s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:09<00:34, 34.45s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:35, 35.04s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:34, 34.92s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:34, 34.72s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:35, 35.02s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:34, 34.78s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:18<00:39, 39.53s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:19<00:40, 40.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:30<00:00, 28.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:30<00:00, 30.26s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:32<00:00, 28.74s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:32<00:00, 30.72s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 28.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.46s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 28.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.58s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:32<00:00, 28.73s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:32<00:00, 30.71s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 28.70s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.64s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:44<00:00, 33.11s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:44<00:00, 34.69s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:46<00:00, 34.11s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:46<00:00, 35.53s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:38<19:48, 38.33s/it]  3%|▎         | 1/32 [00:39<20:12, 39.10s/it]  3%|▎         | 1/32 [00:40<20:58, 40.59s/it]  3%|▎         | 1/32 [00:40<21:07, 40.89s/it]  3%|▎         | 1/32 [00:45<23:38, 45.77s/it]  3%|▎         | 1/32 [00:49<25:23, 49.15s/it]  3%|▎         | 1/32 [01:01<31:40, 61.29s/it]  3%|▎         | 1/32 [01:10<36:13, 70.11s/it]  6%|▋         | 2/32 [01:17<19:32, 39.10s/it]  6%|▋         | 2/32 [01:18<19:34, 39.16s/it]  6%|▋         | 2/32 [01:19<19:53, 39.78s/it]  6%|▋         | 2/32 [01:19<19:53, 39.79s/it]  6%|▋         | 2/32 [01:24<20:46, 41.54s/it]  6%|▋         | 2/32 [01:37<24:12, 48.41s/it]  9%|▉         | 3/32 [01:54<18:24, 38.08s/it]  6%|▋         | 2/32 [01:57<29:02, 58.07s/it]  9%|▉         | 3/32 [01:56<18:42, 38.70s/it]  9%|▉         | 3/32 [01:58<18:54, 39.13s/it]  9%|▉         | 3/32 [01:58<19:06, 39.52s/it]  9%|▉         | 3/32 [02:02<19:15, 39.84s/it]  6%|▋         | 2/32 [02:06<30:58, 61.96s/it]  9%|▉         | 3/32 [02:22<22:39, 46.87s/it] 12%|█▎        | 4/32 [02:34<17:59, 38.57s/it] 12%|█▎        | 4/32 [02:35<18:00, 38.60s/it] 12%|█▎        | 4/32 [02:36<18:06, 38.81s/it] 12%|█▎        | 4/32 [02:37<18:14, 39.09s/it] 12%|█▎        | 4/32 [02:42<18:40, 40.01s/it]  9%|▉         | 3/32 [02:56<28:25, 58.81s/it]  9%|▉         | 3/32 [03:04<29:05, 60.18s/it] 12%|█▎        | 4/32 [03:07<21:33, 46.19s/it] 16%|█▌        | 5/32 [03:10<17:04, 37.95s/it] 16%|█▌        | 5/32 [03:12<17:06, 38.00s/it] 16%|█▌        | 5/32 [03:16<17:49, 39.63s/it] 16%|█▌        | 5/32 [03:21<18:23, 40.88s/it] 16%|█▌        | 5/32 [03:24<18:17, 40.65s/it] 19%|█▉        | 6/32 [03:47<16:17, 37.60s/it] 19%|█▉        | 6/32 [03:50<16:28, 38.03s/it] 16%|█▌        | 5/32 [03:53<20:49, 46.27s/it] 12%|█▎        | 4/32 [03:54<27:16, 58.43s/it] 19%|█▉        | 6/32 [03:55<16:58, 39.17s/it] 19%|█▉        | 6/32 [03:59<17:20, 40.04s/it] 19%|█▉        | 6/32 [04:00<17:00, 39.25s/it] 12%|█▎        | 4/32 [04:02<27:39, 59.25s/it] 22%|██▏       | 7/32 [04:28<16:04, 38.57s/it] 22%|██▏       | 7/32 [04:31<16:11, 38.84s/it] 22%|██▏       | 7/32 [04:32<16:01, 38.45s/it] 22%|██▏       | 7/32 [04:37<16:23, 39.33s/it] 22%|██▏       | 7/32 [04:39<16:18, 39.14s/it] 19%|█▉        | 6/32 [04:40<20:05, 46.36s/it] 16%|█▌        | 5/32 [04:54<26:30, 58.89s/it] 16%|█▌        | 5/32 [05:00<26:28, 58.83s/it] 25%|██▌       | 8/32 [05:09<15:42, 39.25s/it] 25%|██▌       | 8/32 [05:09<15:10, 37.95s/it] 25%|██▌       | 8/32 [05:09<15:25, 38.56s/it] 25%|██▌       | 8/32 [05:15<15:36, 39.03s/it] 25%|██▌       | 8/32 [05:20<15:54, 39.75s/it] 22%|██▏       | 7/32 [05:26<19:20, 46.43s/it] 28%|██▊       | 9/32 [05:46<14:25, 37.65s/it] 28%|██▊       | 9/32 [05:47<14:54, 38.90s/it] 28%|██▊       | 9/32 [05:49<14:58, 39.05s/it] 19%|█▉        | 6/32 [05:54<25:39, 59.20s/it] 28%|██▊       | 9/32 [05:54<14:52, 38.82s/it] 19%|█▉        | 6/32 [05:56<25:06, 57.93s/it] 28%|██▊       | 9/32 [05:57<14:53, 38.85s/it] 25%|██▌       | 8/32 [06:15<18:55, 47.32s/it] 31%|███▏      | 10/32 [06:23<13:50, 37.73s/it] 31%|███▏      | 10/32 [06:26<14:17, 38.97s/it] 31%|███▏      | 10/32 [06:27<14:11, 38.72s/it] 31%|███▏      | 10/32 [06:34<14:24, 39.28s/it] 31%|███▏      | 10/32 [06:35<14:09, 38.61s/it] 22%|██▏       | 7/32 [06:51<24:24, 58.60s/it] 22%|██▏       | 7/32 [06:52<23:49, 57.19s/it] 28%|██▊       | 9/32 [07:01<17:53, 46.67s/it] 34%|███▍      | 11/32 [07:02<13:19, 38.08s/it] 34%|███▍      | 11/32 [07:05<13:39, 39.01s/it] 34%|███▍      | 11/32 [07:08<13:45, 39.29s/it] 34%|███▍      | 11/32 [07:11<13:29, 38.53s/it] 34%|███▍      | 11/32 [07:18<13:56, 39.84s/it] 38%|███▊      | 12/32 [07:41<12:46, 38.34s/it] 38%|███▊      | 12/32 [07:44<13:00, 39.04s/it] 38%|███▊      | 12/32 [07:48<13:11, 39.58s/it] 25%|██▌       | 8/32 [07:49<23:21, 58.40s/it] 38%|███▊      | 12/32 [07:49<12:48, 38.40s/it] 31%|███▏      | 10/32 [07:50<17:23, 47.45s/it] 25%|██▌       | 8/32 [07:51<23:10, 57.95s/it] 38%|███▊      | 12/32 [07:57<13:14, 39.72s/it] 41%|████      | 13/32 [08:22<12:15, 38.71s/it] 41%|████      | 13/32 [08:26<12:44, 40.25s/it] 41%|████      | 13/32 [08:28<12:37, 39.89s/it] 41%|████      | 13/32 [08:31<12:29, 39.45s/it] 41%|████      | 13/32 [08:34<12:18, 38.88s/it] 34%|███▍      | 11/32 [08:35<16:22, 46.79s/it] 28%|██▊       | 9/32 [08:47<22:19, 58.24s/it] 28%|██▊       | 9/32 [08:53<22:38, 59.06s/it] 44%|████▍     | 14/32 [09:01<11:39, 38.86s/it] 44%|████▍     | 14/32 [09:06<12:04, 40.23s/it] 44%|████▍     | 14/32 [09:07<11:52, 39.61s/it] 44%|████▍     | 14/32 [09:09<11:41, 38.98s/it] 44%|████▍     | 14/32 [09:14<11:42, 39.04s/it] 38%|███▊      | 12/32 [09:23<15:42, 47.10s/it] 47%|████▋     | 15/32 [09:40<11:00, 38.84s/it] 31%|███▏      | 10/32 [09:43<21:07, 57.59s/it] 47%|████▋     | 15/32 [09:43<11:06, 39.22s/it] 47%|████▋     | 15/32 [09:45<11:02, 38.99s/it] 47%|████▋     | 15/32 [09:47<10:58, 38.72s/it] 47%|████▋     | 15/32 [09:52<10:58, 38.72s/it] 31%|███▏      | 10/32 [09:54<21:56, 59.82s/it] 41%|████      | 13/32 [10:10<14:54, 47.06s/it] 50%|█████     | 16/32 [10:18<10:17, 38.59s/it] 50%|█████     | 16/32 [10:21<10:21, 38.86s/it] 50%|█████     | 16/32 [10:24<10:25, 39.11s/it] 50%|█████     | 16/32 [10:26<10:22, 38.92s/it] 50%|█████     | 16/32 [10:30<10:18, 38.63s/it] 34%|███▍      | 11/32 [10:41<20:11, 57.69s/it] 34%|███▍      | 11/32 [10:54<20:55, 59.78s/it] 44%|████▍     | 14/32 [10:55<13:57, 46.53s/it] 53%|█████▎    | 17/32 [10:56<09:37, 38.51s/it] 53%|█████▎    | 17/32 [10:59<09:38, 38.55s/it] 53%|█████▎    | 17/32 [11:05<09:52, 39.47s/it] 53%|█████▎    | 17/32 [11:08<09:36, 38.43s/it] 53%|█████▎    | 17/32 [11:08<09:57, 39.85s/it] 56%|█████▋    | 18/32 [11:33<08:51, 37.99s/it] 38%|███▊      | 12/32 [11:39<19:15, 57.80s/it] 56%|█████▋    | 18/32 [11:41<09:01, 38.66s/it] 47%|████▋     | 15/32 [11:42<13:10, 46.47s/it] 56%|█████▋    | 18/32 [11:41<09:17, 39.80s/it] 56%|█████▋    | 18/32 [11:46<09:10, 39.32s/it] 56%|█████▋    | 18/32 [11:48<09:05, 38.95s/it] 38%|███▊      | 12/32 [11:52<19:43, 59.16s/it] 59%|█████▉    | 19/32 [12:14<08:23, 38.74s/it] 59%|█████▉    | 19/32 [12:19<08:17, 38.28s/it] 59%|█████▉    | 19/32 [12:21<08:35, 39.68s/it] 59%|█████▉    | 19/32 [12:24<08:23, 38.70s/it] 59%|█████▉    | 19/32 [12:25<08:19, 38.41s/it] 50%|█████     | 16/32 [12:28<12:25, 46.57s/it] 41%|████      | 13/32 [12:35<18:08, 57.31s/it] 41%|████      | 13/32 [12:48<18:27, 58.30s/it] 62%|██████▎   | 20/32 [12:56<07:58, 39.85s/it] 62%|██████▎   | 20/32 [12:57<07:39, 38.30s/it] 62%|██████▎   | 20/32 [12:58<07:46, 38.84s/it] 62%|██████▎   | 20/32 [13:02<07:43, 38.59s/it] 62%|██████▎   | 20/32 [13:03<07:39, 38.28s/it] 53%|█████▎    | 17/32 [13:18<11:50, 47.36s/it] 44%|████▍     | 14/32 [13:35<17:25, 58.09s/it] 66%|██████▌   | 21/32 [13:35<07:00, 38.25s/it] 66%|██████▌   | 21/32 [13:35<07:16, 39.67s/it] 66%|██████▌   | 21/32 [13:37<07:08, 38.92s/it] 66%|██████▌   | 21/32 [13:41<07:06, 38.75s/it] 66%|██████▌   | 21/32 [13:45<07:12, 39.28s/it] 44%|████▍     | 14/32 [13:51<17:54, 59.71s/it] 56%|█████▋    | 18/32 [14:06<11:05, 47.55s/it] 69%|██████▉   | 22/32 [14:13<06:31, 39.17s/it] 69%|██████▉   | 22/32 [14:16<06:29, 38.90s/it] 69%|██████▉   | 22/32 [14:17<06:32, 39.23s/it] 69%|██████▉   | 22/32 [14:22<06:33, 39.32s/it] 69%|██████▉   | 22/32 [14:24<06:33, 39.31s/it] 47%|████▋     | 15/32 [14:36<16:43, 59.04s/it] 72%|███████▏  | 23/32 [14:50<05:46, 38.45s/it] 47%|████▋     | 15/32 [14:51<16:55, 59.72s/it] 59%|█████▉    | 19/32 [14:51<10:09, 46.89s/it] 72%|███████▏  | 23/32 [14:55<05:48, 38.78s/it] 72%|███████▏  | 23/32 [14:56<05:53, 39.32s/it] 72%|███████▏  | 23/32 [14:59<05:48, 38.72s/it] 72%|███████▏  | 23/32 [15:03<05:50, 38.99s/it] 75%|███████▌  | 24/32 [15:30<05:11, 38.97s/it] 78%|███████▊  | 25/32 [15:34<03:29, 29.93s/it] 50%|█████     | 16/32 [15:34<15:38, 58.68s/it] 75%|███████▌  | 24/32 [15:36<05:05, 38.23s/it] 75%|███████▌  | 24/32 [15:36<05:17, 39.63s/it] 62%|██████▎   | 20/32 [15:37<09:21, 46.77s/it] 75%|███████▌  | 24/32 [15:41<05:10, 38.85s/it] 50%|█████     | 16/32 [15:49<15:46, 59.17s/it] 78%|███████▊  | 25/32 [16:09<04:32, 38.98s/it] 81%|████████▏ | 26/32 [16:15<03:15, 32.56s/it] 78%|███████▊  | 25/32 [16:15<04:36, 39.47s/it] 78%|███████▊  | 25/32 [16:17<04:32, 38.92s/it] 78%|███████▊  | 25/32 [16:18<04:28, 38.29s/it] 66%|██████▌   | 21/32 [16:30<08:52, 48.38s/it] 53%|█████▎    | 17/32 [16:32<14:36, 58.40s/it] 53%|█████▎    | 17/32 [16:48<14:49, 59.31s/it] 81%|████████▏ | 26/32 [16:50<03:56, 39.44s/it] 84%|████████▍ | 27/32 [16:52<02:49, 33.93s/it] 81%|████████▏ | 26/32 [16:53<03:53, 38.92s/it] 81%|████████▏ | 26/32 [16:56<03:54, 39.08s/it] 81%|████████▏ | 26/32 [16:59<03:53, 38.97s/it] 69%|██████▉   | 22/32 [17:15<07:53, 47.39s/it] 84%|████████▍ | 27/32 [17:30<03:18, 39.70s/it] 84%|████████▍ | 27/32 [17:31<03:13, 38.61s/it] 88%|████████▊ | 28/32 [17:33<02:22, 35.72s/it] 56%|█████▋    | 18/32 [17:33<13:50, 59.33s/it] 84%|████████▍ | 27/32 [17:37<03:17, 39.58s/it] 84%|████████▍ | 27/32 [17:40<03:19, 39.85s/it] 56%|█████▋    | 18/32 [17:46<13:45, 58.94s/it] 72%|███████▏  | 23/32 [18:04<07:11, 47.97s/it] 88%|████████▊ | 28/32 [18:07<02:31, 37.95s/it] 88%|████████▊ | 28/32 [18:09<02:37, 39.44s/it] 91%|█████████ | 29/32 [18:13<01:51, 37.10s/it] 88%|████████▊ | 28/32 [18:15<02:37, 39.29s/it] 88%|████████▊ | 28/32 [18:19<02:37, 39.30s/it] 59%|█████▉    | 19/32 [18:30<12:39, 58.40s/it] 91%|█████████ | 29/32 [18:44<01:52, 37.54s/it] 59%|█████▉    | 19/32 [18:46<12:48, 59.13s/it] 94%|█████████▍| 30/32 [18:50<01:01, 30.74s/it] 75%|███████▌  | 24/32 [18:52<06:23, 47.90s/it] 91%|█████████ | 29/32 [18:55<01:58, 39.44s/it] 91%|█████████ | 29/32 [18:58<01:57, 39.27s/it] 94%|█████████▍| 30/32 [18:58<01:18, 39.21s/it] 94%|█████████▍| 30/32 [19:21<01:14, 37.40s/it] 62%|██████▎   | 20/32 [19:26<11:32, 57.75s/it] 97%|█████████▋| 31/32 [19:29<00:32, 32.84s/it] 78%|███████▊  | 25/32 [19:31<05:16, 45.18s/it] 97%|█████████▋| 31/32 [19:35<00:38, 38.48s/it] 94%|█████████▍| 30/32 [19:36<01:19, 39.89s/it] 94%|█████████▍| 30/32 [19:37<01:18, 39.20s/it] 62%|██████▎   | 20/32 [19:46<11:51, 59.28s/it] 97%|█████████▋| 31/32 [19:58<00:37, 37.17s/it]100%|██████████| 32/32 [20:08<00:00, 34.42s/it]100%|██████████| 32/32 [20:08<00:00, 37.77s/it]
 81%|████████▏ | 26/32 [20:09<04:18, 43.05s/it]100%|██████████| 32/32 [20:14<00:00, 38.71s/it]100%|██████████| 32/32 [20:14<00:00, 37.95s/it]
 97%|█████████▋| 31/32 [20:15<00:38, 38.80s/it] 97%|█████████▋| 31/32 [20:16<00:39, 39.80s/it] 66%|██████▌   | 21/32 [20:26<10:41, 58.35s/it]100%|██████████| 32/32 [20:37<00:00, 37.83s/it]100%|██████████| 32/32 [20:37<00:00, 38.67s/it]
 66%|██████▌   | 21/32 [20:47<10:57, 59.81s/it] 84%|████████▍ | 27/32 [20:51<03:33, 42.76s/it]100%|██████████| 32/32 [20:54<00:00, 38.89s/it]100%|██████████| 32/32 [20:54<00:00, 39.19s/it]
100%|██████████| 32/32 [20:54<00:00, 39.41s/it]100%|██████████| 32/32 [20:54<00:00, 39.21s/it]
 69%|██████▉   | 22/32 [21:24<09:42, 58.23s/it] 88%|████████▊ | 28/32 [21:30<02:46, 41.62s/it] 69%|██████▉   | 22/32 [21:46<09:57, 59.77s/it] 91%|█████████ | 29/32 [22:06<02:00, 40.13s/it] 72%|███████▏  | 23/32 [22:21<08:43, 58.12s/it] 72%|███████▏  | 23/32 [22:44<08:52, 59.20s/it] 94%|█████████▍| 30/32 [22:45<01:19, 39.72s/it] 75%|███████▌  | 24/32 [23:19<07:44, 58.06s/it] 97%|█████████▋| 31/32 [23:25<00:39, 39.73s/it] 75%|███████▌  | 24/32 [23:42<07:50, 58.76s/it]100%|██████████| 32/32 [24:02<00:00, 39.07s/it]100%|██████████| 32/32 [24:02<00:00, 45.09s/it]
 78%|███████▊  | 25/32 [24:21<06:53, 59.07s/it] 78%|███████▊  | 25/32 [24:42<06:53, 59.01s/it] 81%|████████▏ | 26/32 [25:22<05:58, 59.79s/it] 81%|████████▏ | 26/32 [25:41<05:55, 59.18s/it] 84%|████████▍ | 27/32 [26:20<04:56, 59.22s/it] 84%|████████▍ | 27/32 [26:42<04:59, 59.83s/it] 88%|████████▊ | 28/32 [27:18<03:55, 58.82s/it] 88%|████████▊ | 28/32 [27:42<03:59, 59.78s/it] 91%|█████████ | 29/32 [28:17<02:57, 59.01s/it] 91%|█████████ | 29/32 [28:42<02:59, 59.74s/it] 94%|█████████▍| 30/32 [29:15<01:57, 58.70s/it] 94%|█████████▍| 30/32 [29:41<01:59, 59.72s/it][rank5]:[E ProcessGroupNCCL.cpp:523] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=86, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600983 milliseconds before timing out.
[rank5]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E ProcessGroupNCCL.cpp:1182] [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=86, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600983 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fc332364d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7fc33350c6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7fc33350fc3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fc333510839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7fc37d223df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7fc37e612609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7fc37e3dd353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=86, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600983 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fc332364d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7fc33350c6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7fc33350fc3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fc333510839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7fc37d223df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7fc37e612609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7fc37e3dd353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fc332364d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7fc333266b11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7fc37d223df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7fc37e612609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7fc37e3dd353 in /lib/x86_64-linux-gnu/libc.so.6)

[2024-07-09 20:19:14,915] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 4017891 closing signal SIGTERM
[2024-07-09 20:19:14,917] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 4017892 closing signal SIGTERM
[2024-07-09 20:19:14,917] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 4017893 closing signal SIGTERM
[2024-07-09 20:19:14,917] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 4017894 closing signal SIGTERM
[2024-07-09 20:19:14,917] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 4017895 closing signal SIGTERM
[2024-07-09 20:19:14,918] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 4017897 closing signal SIGTERM
[2024-07-09 20:19:14,918] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 4017898 closing signal SIGTERM
[2024-07-09 20:19:16,674] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 5 (pid: 4017896) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
main.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-09_20:19:14
  host      : a100-st-p4de24xlarge-119.fair-a100.hpcaas
  rank      : 5 (local_rank: 5)
  exitcode  : -6 (pid: 4017896)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 4017896
========================================================
