Already on 'yangexp2threee'
From github.com:Infini-AI-Lab/GRIFFIN2
   81dc3c0..d0357fc  yangexp2threee -> origin/yangexp2threee
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
2024-07-27:01:31:09,714 INFO     [main.py:288] Verbosity set to INFO
2024-07-27:01:31:20,046 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-07-27:01:31:20,047 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-27:01:31:20,074 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-27:01:31:20,075 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': False, 'check': False, 'contextlength': 1500, 'kernel_size': 10, 'thr': 0.05}
2024-07-27:01:31:20,084 INFO     [xhuggingface.py:168] Using device 'cuda'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.25s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.03s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-27:01:31:43,078 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-27:01:31:43,078 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-27:01:31:43,182 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/396 [00:00<?, ?it/s] 43%|████▎     | 169/396 [00:00<00:00, 1686.96it/s] 86%|████████▌ | 339/396 [00:00<00:00, 1690.62it/s]100%|██████████| 396/396 [00:00<00:00, 1688.75it/s]
2024-07-27:01:31:43,424 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/396 [00:00<?, ?it/s]Running generate_until requests:   0%|          | 1/396 [00:44<4:52:13, 44.39s/it]Running generate_until requests:   1%|          | 2/396 [01:26<4:42:50, 43.07s/it]Running generate_until requests:   1%|          | 3/396 [02:08<4:39:19, 42.64s/it]Running generate_until requests:   1%|          | 4/396 [02:50<4:37:27, 42.47s/it]Running generate_until requests:   1%|▏         | 5/396 [03:33<4:36:01, 42.36s/it]Running generate_until requests:   2%|▏         | 6/396 [04:15<4:34:51, 42.29s/it]Running generate_until requests:   2%|▏         | 7/396 [04:57<4:33:45, 42.23s/it]Running generate_until requests:   2%|▏         | 8/396 [05:39<4:32:43, 42.18s/it]Running generate_until requests:   2%|▏         | 9/396 [06:21<4:31:56, 42.16s/it]Running generate_until requests:   3%|▎         | 10/396 [07:03<4:31:05, 42.14s/it]Running generate_until requests:   3%|▎         | 11/396 [07:45<4:30:23, 42.14s/it]Running generate_until requests:   3%|▎         | 12/396 [08:27<4:29:38, 42.13s/it]Running generate_until requests:   3%|▎         | 13/396 [09:09<4:28:54, 42.13s/it]Running generate_until requests:   4%|▎         | 14/396 [09:52<4:28:58, 42.25s/it]Running generate_until requests:   4%|▍         | 15/396 [10:34<4:28:03, 42.21s/it]Running generate_until requests:   4%|▍         | 16/396 [11:17<4:27:43, 42.27s/it]Running generate_until requests:   4%|▍         | 17/396 [11:59<4:26:41, 42.22s/it]Running generate_until requests:   5%|▍         | 18/396 [12:41<4:25:42, 42.18s/it]