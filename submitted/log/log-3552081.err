Already on 'yangexp2threee'
warning: fetch updated the current branch head.
fast-forwarding your working tree from
commit 3828bef74bce9e650289220713cddec1f26f65a0.
error: Your local changes to the following files would be overwritten by merge:
	llama12_static_cache_sdpa_with_check.py
	llama12_static_cache_sdpa_with_treee.py
Please commit your changes or stash them before you merge.
Aborting
fatal: Cannot fast-forward your working tree.
After making sure that you saved anything precious from
$ git diff 3828bef74bce9e650289220713cddec1f26f65a0
output, run
$ git reset --hard
to recover.
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
2024-07-28:03:16:08,340 INFO     [main.py:288] Verbosity set to INFO
2024-07-28:03:16:17,964 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-07-28:03:16:17,965 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-28:03:16:17,988 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-28:03:16:17,988 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'check': False, 'contextlength': 1500, 'kernel_size': 10, 'thr': 0.05, 'attentionimplementation': 'general'}
2024-07-28:03:16:17,997 INFO     [xhuggingface.py:168] Using device 'cuda'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:15<00:15,  7.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  4.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.73s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-28:03:17:22,277 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-28:03:17:22,277 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-28:03:17:22,370 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/396 [00:00<?, ?it/s] 43%|████▎     | 169/396 [00:00<00:00, 1684.66it/s] 86%|████████▌ | 339/396 [00:00<00:00, 1688.67it/s]100%|██████████| 396/396 [00:00<00:00, 1686.89it/s]
2024-07-28:03:17:22,614 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/396 [00:00<?, ?it/s]Running generate_until requests:   0%|          | 1/396 [00:10<1:09:31, 10.56s/it]Running generate_until requests:   1%|          | 2/396 [00:19<1:01:25,  9.35s/it]Running generate_until requests:   1%|          | 3/396 [00:27<58:55,  9.00s/it]  Running generate_until requests:   1%|          | 4/396 [00:36<58:12,  8.91s/it]Running generate_until requests:   1%|▏         | 5/396 [00:44<57:12,  8.78s/it]Running generate_until requests:   2%|▏         | 6/396 [00:53<57:34,  8.86s/it]Running generate_until requests:   2%|▏         | 7/396 [01:02<56:54,  8.78s/it]Running generate_until requests:   2%|▏         | 8/396 [01:11<56:20,  8.71s/it]Running generate_until requests:   2%|▏         | 9/396 [01:19<56:11,  8.71s/it]Running generate_until requests:   3%|▎         | 10/396 [01:28<55:52,  8.69s/it]Running generate_until requests:   3%|▎         | 11/396 [01:37<55:31,  8.65s/it]Running generate_until requests:   3%|▎         | 12/396 [01:45<55:07,  8.61s/it]Running generate_until requests:   3%|▎         | 13/396 [01:54<55:23,  8.68s/it]Running generate_until requests:   4%|▎         | 14/396 [02:03<55:40,  8.75s/it]Running generate_until requests:   4%|▍         | 15/396 [02:11<55:20,  8.71s/it]Running generate_until requests:   4%|▍         | 16/396 [02:20<54:48,  8.65s/it]Running generate_until requests:   4%|▍         | 17/396 [02:29<54:48,  8.68s/it]Running generate_until requests:   5%|▍         | 18/396 [02:37<54:32,  8.66s/it]