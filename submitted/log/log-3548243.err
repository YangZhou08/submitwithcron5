fatal: Unable to create '/fsx-storygen/beidic/yang/GRIFFIN2/.git/index.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
error: cannot lock ref 'refs/remotes/origin/yangexp2threee': is at e12829f1d745757a95fa731ce90a16dcd92ef5bb but expected d0357fc3277f1e8a458184541a3280b1bc45da7f
From github.com:Infini-AI-Lab/GRIFFIN2
 ! d0357fc..e12829f  yangexp2threee -> origin/yangexp2threee  (unable to update local ref)
warning: fetch updated the current branch head.
fast-forwarding your working tree from
commit d0357fc3277f1e8a458184541a3280b1bc45da7f.
error: Your local changes to the following files would be overwritten by merge:
	debugging1.sh
	llama12_static_cache_sdpa_with_check.py
	xhuggingface.py
Please commit your changes or stash them before you merge.
Aborting
fatal: Cannot fast-forward your working tree.
After making sure that you saved anything precious from
$ git diff d0357fc3277f1e8a458184541a3280b1bc45da7f
output, run
$ git reset --hard
to recover.
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
2024-07-27:02:28:08,650 INFO     [main.py:288] Verbosity set to INFO
2024-07-27:02:28:18,737 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-07-27:02:28:18,738 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-07-27:02:28:18,763 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-27:02:28:18,763 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'check': False, 'contextlength': 1500, 'kernel_size': 16, 'thr': 0.05}
2024-07-27:02:28:18,772 INFO     [xhuggingface.py:168] Using device 'cuda'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.25s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-27:02:29:22,383 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-27:02:29:22,383 WARNING  [task.py:322] [Task: gsm8k_cot] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-07-27:02:29:22,470 INFO     [task.py:395] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/396 [00:00<?, ?it/s] 43%|████▎     | 169/396 [00:00<00:00, 1687.82it/s] 86%|████████▌ | 339/396 [00:00<00:00, 1693.70it/s]100%|██████████| 396/396 [00:00<00:00, 1690.11it/s]
2024-07-27:02:29:22,712 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/396 [00:00<?, ?it/s]Running generate_until requests:   0%|          | 1/396 [00:21<2:23:16, 21.76s/it]Running generate_until requests:   1%|          | 2/396 [00:41<2:14:10, 20.43s/it]Running generate_until requests:   1%|          | 3/396 [01:00<2:10:53, 19.98s/it]Running generate_until requests:   1%|          | 4/396 [01:20<2:09:43, 19.86s/it]Running generate_until requests:   1%|▏         | 5/396 [01:39<2:08:28, 19.72s/it]Running generate_until requests:   2%|▏         | 6/396 [01:59<2:07:33, 19.62s/it]Running generate_until requests:   2%|▏         | 7/396 [02:18<2:07:00, 19.59s/it]Running generate_until requests:   2%|▏         | 8/396 [02:38<2:06:38, 19.58s/it]Running generate_until requests:   2%|▏         | 9/396 [02:57<2:06:15, 19.57s/it]Running generate_until requests:   3%|▎         | 10/396 [03:17<2:06:03, 19.59s/it]Running generate_until requests:   3%|▎         | 11/396 [03:36<2:05:22, 19.54s/it]Running generate_until requests:   3%|▎         | 12/396 [03:56<2:04:53, 19.52s/it]Running generate_until requests:   3%|▎         | 13/396 [04:15<2:04:31, 19.51s/it]Running generate_until requests:   4%|▎         | 14/396 [04:35<2:04:04, 19.49s/it]Running generate_until requests:   4%|▍         | 15/396 [04:54<2:03:42, 19.48s/it]Running generate_until requests:   4%|▍         | 16/396 [05:15<2:06:23, 19.96s/it]Running generate_until requests:   4%|▍         | 17/396 [05:35<2:05:05, 19.80s/it]Running generate_until requests:   5%|▍         | 18/396 [05:54<2:04:09, 19.71s/it]Running generate_until requests:   5%|▍         | 19/396 [06:14<2:03:27, 19.65s/it]Running generate_until requests:   5%|▌         | 20/396 [06:33<2:02:50, 19.60s/it]Running generate_until requests:   5%|▌         | 21/396 [06:53<2:02:14, 19.56s/it]Running generate_until requests:   6%|▌         | 22/396 [07:12<2:01:51, 19.55s/it]Running generate_until requests:   6%|▌         | 23/396 [07:32<2:01:22, 19.52s/it]Running generate_until requests:   6%|▌         | 24/396 [07:51<2:00:53, 19.50s/it]Running generate_until requests:   6%|▋         | 25/396 [08:11<2:00:28, 19.48s/it]Running generate_until requests:   7%|▋         | 26/396 [08:30<2:00:05, 19.48s/it]Running generate_until requests:   7%|▋         | 27/396 [08:50<1:59:44, 19.47s/it]Running generate_until requests:   7%|▋         | 28/396 [09:09<1:59:51, 19.54s/it]Running generate_until requests:   7%|▋         | 29/396 [09:29<1:59:18, 19.51s/it]Running generate_until requests:   8%|▊         | 30/396 [09:48<1:58:54, 19.49s/it]Running generate_until requests:   8%|▊         | 31/396 [10:08<1:58:33, 19.49s/it]Running generate_until requests:   8%|▊         | 32/396 [10:27<1:58:24, 19.52s/it]