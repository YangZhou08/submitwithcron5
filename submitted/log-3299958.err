Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:59, 19.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:04, 21.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:04, 21.36s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:04, 21.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:04, 21.36s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.07s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:02, 20.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:05, 21.89s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:32<00:30, 15.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:32, 16.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:32, 16.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:32, 16.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:32, 16.20s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:32, 16.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:32, 16.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:32, 16.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:44<00:13, 13.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:46<00:14, 14.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:47<00:14, 14.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00,  9.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00, 11.63s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.04s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00,  9.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.23s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:22<11:52, 23.00s/it]  3%|▎         | 1/32 [00:24<12:36, 24.40s/it]  3%|▎         | 1/32 [00:28<14:35, 28.23s/it]  3%|▎         | 1/32 [00:30<15:50, 30.68s/it]  3%|▎         | 1/32 [00:35<18:25, 35.67s/it]  3%|▎         | 1/32 [00:36<18:52, 36.54s/it]  3%|▎         | 1/32 [00:39<20:18, 39.32s/it]  3%|▎         | 1/32 [00:41<21:13, 41.07s/it]  6%|▋         | 2/32 [00:43<10:53, 21.78s/it]  6%|▋         | 2/32 [00:45<11:14, 22.49s/it]  6%|▋         | 2/32 [01:04<16:22, 32.76s/it]  6%|▋         | 2/32 [01:05<16:31, 33.06s/it]  6%|▋         | 2/32 [01:07<16:31, 33.07s/it]  9%|▉         | 3/32 [01:10<11:30, 23.80s/it]  6%|▋         | 2/32 [01:14<18:47, 37.58s/it]  9%|▉         | 3/32 [01:18<13:17, 27.50s/it]  6%|▋         | 2/32 [01:23<21:00, 42.00s/it]  6%|▋         | 2/32 [01:23<21:00, 42.00s/it]  9%|▉         | 3/32 [01:31<14:27, 29.91s/it]  9%|▉         | 3/32 [01:32<14:20, 29.66s/it]  9%|▉         | 3/32 [01:33<15:03, 31.15s/it]  9%|▉         | 3/32 [01:40<15:41, 32.46s/it] 12%|█▎        | 4/32 [01:48<13:42, 29.37s/it] 12%|█▎        | 4/32 [01:52<14:06, 30.25s/it] 12%|█▎        | 4/32 [01:55<12:48, 27.43s/it] 12%|█▎        | 4/32 [01:55<12:29, 26.78s/it] 12%|█▎        | 4/32 [01:58<13:29, 28.91s/it]  9%|▉         | 3/32 [02:08<20:57, 43.36s/it] 12%|█▎        | 4/32 [02:08<14:10, 30.36s/it]  9%|▉         | 3/32 [02:09<21:07, 43.71s/it] 16%|█▌        | 5/32 [02:15<12:51, 28.57s/it] 16%|█▌        | 5/32 [02:17<12:41, 28.20s/it] 16%|█▌        | 5/32 [02:21<11:57, 26.57s/it] 16%|█▌        | 5/32 [02:23<12:25, 27.61s/it] 16%|█▌        | 5/32 [02:30<13:22, 29.72s/it] 19%|█▉        | 6/32 [02:35<11:02, 25.47s/it] 16%|█▌        | 5/32 [02:41<14:10, 31.51s/it] 19%|█▉        | 6/32 [02:45<12:14, 28.25s/it] 19%|█▉        | 6/32 [02:45<11:11, 25.84s/it] 12%|█▎        | 4/32 [02:56<21:08, 45.30s/it] 22%|██▏       | 7/32 [02:57<10:11, 24.45s/it] 19%|█▉        | 6/32 [02:59<12:47, 29.53s/it] 12%|█▎        | 4/32 [03:01<21:56, 47.00s/it] 19%|█▉        | 6/32 [03:06<14:17, 32.96s/it] 22%|██▏       | 7/32 [03:07<10:56, 26.25s/it] 22%|██▏       | 7/32 [03:13<11:07, 26.71s/it] 19%|█▉        | 6/32 [03:21<14:53, 34.36s/it] 25%|██▌       | 8/32 [03:27<10:26, 26.08s/it] 22%|██▏       | 7/32 [03:28<12:11, 29.27s/it] 25%|██▌       | 8/32 [03:33<10:22, 25.96s/it] 16%|█▌        | 5/32 [03:37<19:38, 43.66s/it] 25%|██▌       | 8/32 [03:40<10:36, 26.51s/it] 22%|██▏       | 7/32 [03:46<14:38, 35.14s/it] 16%|█▌        | 5/32 [03:49<21:17, 47.32s/it] 25%|██▌       | 8/32 [03:51<11:00, 27.51s/it] 28%|██▊       | 9/32 [03:54<10:07, 26.43s/it] 22%|██▏       | 7/32 [03:55<14:12, 34.11s/it] 28%|██▊       | 9/32 [04:00<10:10, 26.55s/it] 28%|██▊       | 9/32 [04:09<10:29, 27.39s/it] 28%|██▊       | 9/32 [04:14<09:56, 25.93s/it] 25%|██▌       | 8/32 [04:18<13:44, 34.34s/it] 31%|███▏      | 10/32 [04:19<09:34, 26.09s/it] 31%|███▏      | 10/32 [04:28<09:48, 26.77s/it] 19%|█▉        | 6/32 [04:30<19:32, 45.10s/it] 19%|█▉        | 6/32 [04:33<20:42, 47.80s/it] 31%|███▏      | 10/32 [04:33<09:39, 26.32s/it] 31%|███▏      | 10/32 [04:36<09:04, 24.73s/it] 25%|██▌       | 8/32 [04:38<14:49, 37.05s/it] 34%|███▍      | 11/32 [04:46<09:12, 26.31s/it] 28%|██▊       | 9/32 [04:53<13:11, 34.42s/it] 34%|███▍      | 11/32 [04:58<09:06, 26.02s/it] 34%|███▍      | 11/32 [05:03<08:52, 25.36s/it] 34%|███▍      | 11/32 [05:05<10:27, 29.90s/it] 38%|███▊      | 12/32 [05:08<08:18, 24.93s/it] 22%|██▏       | 7/32 [05:09<18:23, 44.13s/it] 28%|██▊       | 9/32 [05:12<13:52, 36.19s/it] 31%|███▏      | 10/32 [05:14<11:07, 30.32s/it] 22%|██▏       | 7/32 [05:17<19:02, 45.70s/it] 38%|███▊      | 12/32 [05:23<08:32, 25.63s/it] 38%|███▊      | 12/32 [05:28<08:28, 25.42s/it] 38%|███▊      | 12/32 [05:31<09:37, 28.87s/it] 41%|████      | 13/32 [05:35<08:03, 25.45s/it] 34%|███▍      | 11/32 [05:42<10:19, 29.48s/it] 41%|████      | 13/32 [05:47<07:58, 25.16s/it] 31%|███▏      | 10/32 [05:48<13:14, 36.11s/it] 41%|████      | 13/32 [05:51<08:19, 26.27s/it] 25%|██▌       | 8/32 [05:55<17:55, 44.79s/it] 41%|████      | 13/32 [05:59<08:31, 26.94s/it] 44%|████▍     | 14/32 [05:59<07:30, 25.05s/it] 38%|███▊      | 12/32 [06:05<09:14, 27.72s/it] 25%|██▌       | 8/32 [06:07<18:50, 47.09s/it] 44%|████▍     | 14/32 [06:13<07:37, 25.39s/it] 44%|████▍     | 14/32 [06:15<07:37, 25.39s/it] 44%|████▍     | 14/32 [06:20<07:33, 25.20s/it] 47%|████▋     | 15/32 [06:22<06:55, 24.43s/it] 34%|███▍      | 11/32 [06:27<12:54, 36.88s/it] 41%|████      | 13/32 [06:32<08:42, 27.49s/it] 47%|████▋     | 15/32 [06:36<07:02, 24.85s/it] 47%|████▋     | 15/32 [06:40<06:41, 23.65s/it] 50%|█████     | 16/32 [06:45<06:24, 24.00s/it] 28%|██▊       | 9/32 [06:44<16:55, 44.13s/it] 47%|████▋     | 15/32 [06:45<07:36, 26.84s/it] 38%|███▊      | 12/32 [06:49<10:47, 32.40s/it] 28%|██▊       | 9/32 [06:53<18:44, 48.89s/it] 44%|████▍     | 14/32 [06:56<07:54, 26.35s/it] 50%|█████     | 16/32 [07:09<07:15, 27.22s/it] 50%|█████     | 16/32 [07:10<06:48, 25.50s/it] 50%|█████     | 16/32 [07:14<07:19, 27.45s/it] 41%|████      | 13/32 [07:17<09:48, 30.97s/it] 53%|█████▎    | 17/32 [07:17<06:35, 26.39s/it] 47%|████▋     | 15/32 [07:22<07:23, 26.12s/it] 31%|███▏      | 10/32 [07:29<16:16, 44.41s/it] 31%|███▏      | 10/32 [07:36<17:16, 47.11s/it] 53%|█████▎    | 17/32 [07:37<06:51, 27.44s/it] 44%|████▍     | 14/32 [07:38<08:27, 28.19s/it] 53%|█████▎    | 17/32 [07:41<06:48, 27.20s/it] 56%|█████▋    | 18/32 [07:45<06:18, 27.03s/it] 53%|█████▎    | 17/32 [07:57<08:01, 32.07s/it] 47%|████▋     | 15/32 [08:01<07:31, 26.56s/it] 56%|█████▋    | 18/32 [08:03<06:17, 26.96s/it] 34%|███▍      | 11/32 [08:07<14:48, 42.31s/it] 50%|█████     | 16/32 [08:08<08:33, 32.11s/it] 59%|█████▉    | 19/32 [08:08<05:35, 25.79s/it] 56%|█████▋    | 18/32 [08:20<07:13, 30.96s/it] 34%|███▍      | 11/32 [08:22<16:22, 46.80s/it] 56%|█████▋    | 18/32 [08:25<07:12, 30.91s/it] 59%|█████▉    | 19/32 [08:37<06:17, 29.05s/it] 62%|██████▎   | 20/32 [08:41<05:36, 28.01s/it] 50%|█████     | 16/32 [08:42<08:14, 30.88s/it] 53%|█████▎    | 17/32 [08:44<08:20, 33.34s/it] 59%|█████▉    | 19/32 [08:47<06:25, 29.69s/it] 59%|█████▉    | 19/32 [08:57<06:46, 31.24s/it] 62%|██████▎   | 20/32 [09:02<05:35, 27.94s/it] 38%|███▊      | 12/32 [09:03<15:31, 46.56s/it] 56%|█████▋    | 18/32 [09:12<07:23, 31.65s/it] 66%|██████▌   | 21/32 [09:13<05:20, 29.18s/it] 53%|█████▎    | 17/32 [09:15<07:51, 31.42s/it] 62%|██████▎   | 20/32 [09:17<05:56, 29.70s/it] 38%|███▊      | 12/32 [09:23<16:56, 50.84s/it] 66%|██████▌   | 21/32 [09:31<05:09, 28.10s/it] 62%|██████▎   | 20/32 [09:34<06:35, 32.95s/it] 59%|█████▉    | 19/32 [09:34<06:14, 28.78s/it] 69%|██████▉   | 22/32 [09:39<04:42, 28.23s/it] 66%|██████▌   | 21/32 [09:47<05:29, 29.91s/it] 41%|████      | 13/32 [09:51<14:52, 46.96s/it] 66%|██████▌   | 21/32 [09:54<05:18, 28.98s/it] 69%|██████▉   | 22/32 [09:59<04:40, 28.02s/it] 72%|███████▏  | 23/32 [10:06<04:11, 27.91s/it] 62%|██████▎   | 20/32 [10:10<06:11, 31.00s/it] 56%|█████▋    | 18/32 [10:11<09:02, 38.78s/it] 41%|████      | 13/32 [10:16<16:22, 51.72s/it] 69%|██████▉   | 22/32 [10:22<05:12, 31.22s/it] 72%|███████▏  | 23/32 [10:27<04:13, 28.21s/it] 75%|███████▌  | 24/32 [10:31<03:35, 26.88s/it] 69%|██████▉   | 22/32 [10:32<05:18, 31.81s/it] 44%|████▍     | 14/32 [10:36<13:54, 46.38s/it] 66%|██████▌   | 21/32 [10:41<05:41, 31.02s/it] 59%|█████▉    | 19/32 [10:42<07:54, 36.52s/it] 72%|███████▏  | 23/32 [10:43<04:13, 28.22s/it] 78%|███████▊  | 25/32 [10:51<02:53, 24.79s/it] 44%|████▍     | 14/32 [10:58<14:38, 48.79s/it] 75%|███████▌  | 24/32 [11:02<04:00, 30.12s/it] 72%|███████▏  | 23/32 [11:03<04:43, 31.54s/it] 75%|███████▌  | 24/32 [11:04<03:28, 26.09s/it] 69%|██████▉   | 22/32 [11:04<04:44, 28.50s/it] 44%|████▍     | 14/32 [11:06<14:16, 47.59s/it]
 62%|██████▎   | 20/32 [11:08<06:41, 33.46s/it] 47%|████▋     | 15/32 [11:23<13:13, 46.69s/it] 72%|███████▏  | 23/32 [11:28<04:04, 27.15s/it] 78%|███████▊  | 25/32 [11:28<03:22, 28.90s/it] 78%|███████▊  | 25/32 [11:28<02:58, 25.54s/it] 81%|████████▏ | 26/32 [11:29<02:52, 28.75s/it] 66%|██████▌   | 21/32 [11:35<05:45, 31.40s/it] 75%|███████▌  | 24/32 [11:40<04:24, 33.07s/it] 81%|████████▏ | 26/32 [11:52<02:30, 25.13s/it] 50%|█████     | 16/32 [11:52<10:59, 41.24s/it] 84%|████████▍ | 27/32 [11:54<02:18, 27.70s/it] 69%|██████▉   | 22/32 [11:56<04:44, 28.44s/it] 75%|███████▌  | 24/32 [11:58<03:43, 27.99s/it] 81%|████████▏ | 26/32 [12:00<02:59, 29.86s/it] 78%|███████▊  | 25/32 [12:06<03:37, 31.10s/it] 84%|████████▍ | 27/32 [12:13<01:58, 23.75s/it] 88%|████████▊ | 28/32 [12:21<01:49, 27.36s/it] 53%|█████▎    | 17/32 [12:21<09:21, 37.43s/it] 72%|███████▏  | 23/32 [12:23<04:11, 27.99s/it] 84%|████████▍ | 27/32 [12:25<02:21, 28.32s/it] 78%|███████▊  | 25/32 [12:27<03:18, 28.37s/it] 81%|████████▏ | 26/32 [12:33<02:59, 29.92s/it] 88%|████████▊ | 28/32 [12:41<01:40, 25.20s/it] 56%|█████▋    | 18/32 [12:43<07:40, 32.91s/it] 78%|███████▊  | 25/32 [12:48<02:25, 20.77s/it] 91%|█████████ | 29/32 [12:51<01:25, 28.38s/it] 88%|████████▊ | 28/32 [12:52<01:52, 28.17s/it] 81%|████████▏ | 26/32 [13:00<02:58, 29.83s/it] 59%|█████▉    | 19/32 [13:06<06:29, 29.94s/it] 84%|████████▍ | 27/32 [13:09<02:38, 31.63s/it] 91%|█████████ | 29/32 [13:12<01:20, 26.77s/it] 94%|█████████▍| 30/32 [13:14<00:53, 26.75s/it] 81%|████████▏ | 26/32 [13:16<02:14, 22.49s/it] 91%|█████████ | 29/32 [13:22<01:25, 28.42s/it] 62%|██████▎   | 20/32 [13:29<05:32, 27.73s/it] 88%|████████▊ | 28/32 [13:31<01:54, 28.65s/it] 84%|████████▍ | 27/32 [13:35<02:36, 31.37s/it] 94%|█████████▍| 30/32 [13:36<00:52, 26.02s/it] 84%|████████▍ | 27/32 [13:41<01:55, 23.07s/it] 97%|█████████▋| 31/32 [13:43<00:27, 27.25s/it] 94%|█████████▍| 30/32 [13:49<00:56, 28.24s/it] 94%|█████████▍| 30/32 [13:56<00:42, 21.40s/it] 97%|█████████▋| 31/32 [13:59<00:25, 25.12s/it] 88%|████████▊ | 28/32 [14:06<02:04, 31.21s/it] 66%|██████▌   | 21/32 [14:06<05:36, 30.55s/it] 88%|████████▊ | 28/32 [14:06<01:35, 23.83s/it]100%|██████████| 32/32 [14:13<00:00, 28.12s/it]100%|██████████| 32/32 [14:13<00:00, 26.67s/it]
 97%|█████████▋| 31/32 [14:25<00:30, 30.36s/it]100%|██████████| 32/32 [14:30<00:00, 26.72s/it]100%|██████████| 32/32 [14:30<00:00, 27.19s/it]
 97%|█████████▋| 31/32 [14:32<00:24, 24.95s/it] 91%|█████████ | 29/32 [14:38<01:34, 31.34s/it] 91%|█████████ | 29/32 [14:41<01:20, 26.80s/it] 69%|██████▉   | 22/32 [14:43<05:25, 32.58s/it]100%|██████████| 32/32 [14:53<00:00, 29.78s/it]100%|██████████| 32/32 [14:53<00:00, 27.92s/it]
100%|██████████| 32/32 [14:57<00:00, 24.99s/it]100%|██████████| 32/32 [14:57<00:00, 28.05s/it]
 72%|███████▏  | 23/32 [15:06<04:27, 29.71s/it] 94%|█████████▍| 30/32 [15:08<00:53, 26.84s/it] 94%|█████████▍| 30/32 [15:11<01:04, 32.11s/it] 75%|███████▌  | 24/32 [15:29<03:40, 27.55s/it] 97%|█████████▋| 31/32 [15:31<00:25, 25.66s/it] 97%|█████████▋| 31/32 [15:49<00:33, 33.67s/it] 78%|███████▊  | 25/32 [15:49<02:58, 25.44s/it]100%|██████████| 32/32 [15:51<00:00, 24.24s/it]100%|██████████| 32/32 [15:51<00:00, 29.74s/it]
100%|██████████| 32/32 [16:19<00:00, 32.74s/it]100%|██████████| 32/32 [16:19<00:00, 30.62s/it]
 81%|████████▏ | 26/32 [16:23<02:48, 28.07s/it] 84%|████████▍ | 27/32 [16:48<02:14, 26.93s/it] 88%|████████▊ | 28/32 [17:18<01:51, 27.91s/it] 91%|█████████ | 29/32 [17:42<01:20, 26.89s/it] 94%|█████████▍| 30/32 [18:13<00:56, 28.15s/it] 97%|█████████▋| 31/32 [18:40<00:27, 27.75s/it]100%|██████████| 32/32 [19:04<00:00, 26.52s/it]100%|██████████| 32/32 [19:04<00:00, 35.76s/it]
[rank4]:[E ProcessGroupNCCL.cpp:523] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600906 milliseconds before timing out.
[rank4]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E ProcessGroupNCCL.cpp:1182] [Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600906 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7eff7b61dd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7eff7c7c56e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7eff7c7c8c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7eff7c7c9839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7effc64dcdf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7effc78cb609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7effc7696353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600906 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7eff7b61dd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7eff7c7c56e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7eff7c7c8c3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7eff7c7c9839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7effc64dcdf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7effc78cb609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7effc7696353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7eff7b61dd87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7eff7c51fb11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7effc64dcdf4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7effc78cb609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7effc7696353 in /lib/x86_64-linux-gnu/libc.so.6)

[rank3]:[E ProcessGroupNCCL.cpp:523] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600250 milliseconds before timing out.
[rank3]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E ProcessGroupNCCL.cpp:1182] [Rank 3] NCCL watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600250 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f96383f1d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f96395996e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f963959cc3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f963959d839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f96832b0df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f968469f609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f968446a353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 3] NCCL watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=53, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600250 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f96383f1d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f96395996e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f963959cc3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f963959d839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7f96832b0df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f968469f609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f968446a353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f96383f1d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f96392f3b11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7f96832b0df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f968469f609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f968446a353 in /lib/x86_64-linux-gnu/libc.so.6)

[2024-07-09 16:10:58,626] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 882751 closing signal SIGTERM
[2024-07-09 16:10:58,627] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 882752 closing signal SIGTERM
[2024-07-09 16:10:58,627] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 882755 closing signal SIGTERM
[2024-07-09 16:10:58,628] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 882756 closing signal SIGTERM
[2024-07-09 16:10:58,628] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 882757 closing signal SIGTERM
[2024-07-09 16:10:58,628] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 882758 closing signal SIGTERM
[2024-07-09 16:11:14,602] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 3 (pid: 882754) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
main.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-09_16:10:58
  host      : a100-st-p4de24xlarge-322.fair-a100.hpcaas
  rank      : 3 (local_rank: 3)
  exitcode  : -6 (pid: 882754)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 882754
=======================================================
