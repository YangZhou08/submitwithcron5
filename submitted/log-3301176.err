Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:10, 23.56s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:12, 24.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:12, 24.28s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.65s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.36s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:12, 24.25s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:12, 24.15s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:12, 24.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:41, 20.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:42, 21.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:42, 21.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:43, 21.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:42, 21.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:42, 21.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:42, 21.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:42, 21.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:57<00:17, 17.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:18, 18.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:18, 18.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:18, 18.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:18, 18.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:18, 18.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:18, 18.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:18, 18.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 11.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 14.81s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.08s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.15s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.15s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.10s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 11.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:00<00:00, 15.13s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:23<12:09, 23.52s/it]  3%|▎         | 1/32 [00:25<13:02, 25.24s/it]  3%|▎         | 1/32 [00:28<14:33, 28.18s/it]  3%|▎         | 1/32 [00:30<15:57, 30.87s/it]  3%|▎         | 1/32 [00:35<18:14, 35.32s/it]  3%|▎         | 1/32 [00:36<18:46, 36.34s/it]  3%|▎         | 1/32 [00:39<20:27, 39.60s/it]  3%|▎         | 1/32 [00:43<22:18, 43.17s/it]  6%|▋         | 2/32 [00:48<12:19, 24.66s/it]  6%|▋         | 2/32 [00:50<12:42, 25.42s/it]  6%|▋         | 2/32 [00:58<14:40, 29.36s/it]  6%|▋         | 2/32 [01:05<16:01, 32.05s/it]  6%|▋         | 2/32 [01:05<16:40, 33.36s/it]  9%|▉         | 3/32 [01:15<12:17, 25.42s/it]  6%|▋         | 2/32 [01:16<18:58, 37.96s/it]  9%|▉         | 3/32 [01:18<12:45, 26.40s/it]  9%|▉         | 3/32 [01:18<12:13, 25.30s/it]  6%|▋         | 2/32 [01:21<20:09, 40.31s/it]  6%|▋         | 2/32 [01:23<21:12, 42.43s/it]  9%|▉         | 3/32 [01:29<13:50, 28.63s/it]  9%|▉         | 3/32 [01:32<14:30, 30.03s/it] 12%|█▎        | 4/32 [01:39<11:00, 23.60s/it] 12%|█▎        | 4/32 [01:43<12:19, 26.41s/it] 12%|█▎        | 4/32 [01:49<13:12, 28.30s/it] 12%|█▎        | 4/32 [01:51<12:02, 25.81s/it] 12%|█▎        | 4/32 [01:59<13:30, 28.96s/it]  9%|▉         | 3/32 [02:00<19:28, 40.31s/it] 16%|█▌        | 5/32 [02:05<11:00, 24.45s/it] 16%|█▌        | 5/32 [02:06<11:24, 25.34s/it]  9%|▉         | 3/32 [02:07<20:41, 42.83s/it] 16%|█▌        | 5/32 [02:16<11:32, 25.63s/it]  9%|▉         | 3/32 [02:16<23:19, 48.25s/it] 16%|█▌        | 5/32 [02:18<12:53, 28.65s/it] 12%|█▎        | 4/32 [02:28<16:33, 35.49s/it] 16%|█▌        | 5/32 [02:30<13:22, 29.73s/it] 19%|█▉        | 6/32 [02:36<11:31, 26.59s/it] 19%|█▉        | 6/32 [02:38<11:57, 27.59s/it] 19%|█▉        | 6/32 [02:43<11:50, 27.34s/it] 19%|█▉        | 6/32 [02:59<12:47, 29.53s/it] 22%|██▏       | 7/32 [02:59<10:36, 25.48s/it] 19%|█▉        | 6/32 [03:00<13:45, 31.74s/it] 22%|██▏       | 7/32 [03:02<10:59, 26.36s/it] 16%|█▌        | 5/32 [03:05<16:04, 35.73s/it] 22%|██▏       | 7/32 [03:12<11:33, 27.76s/it] 12%|█▎        | 4/32 [03:15<24:44, 53.03s/it] 12%|█▎        | 4/32 [03:17<24:45, 53.07s/it] 25%|██▌       | 8/32 [03:26<10:18, 25.78s/it] 22%|██▏       | 7/32 [03:28<12:10, 29.22s/it] 19%|█▉        | 6/32 [03:34<14:29, 33.45s/it] 25%|██▌       | 8/32 [03:35<11:25, 28.55s/it] 22%|██▏       | 7/32 [03:38<14:04, 33.79s/it] 25%|██▌       | 8/32 [03:42<11:26, 28.61s/it] 25%|██▌       | 8/32 [03:51<10:58, 27.44s/it] 28%|██▊       | 9/32 [03:59<10:46, 28.12s/it] 16%|█▌        | 5/32 [04:00<22:19, 49.63s/it] 28%|██▊       | 9/32 [04:02<10:41, 27.91s/it] 28%|██▊       | 9/32 [04:05<10:17, 26.86s/it] 16%|█▌        | 5/32 [04:05<23:17, 51.77s/it] 22%|██▏       | 7/32 [04:08<14:03, 33.73s/it] 25%|██▌       | 8/32 [04:09<13:12, 33.03s/it] 28%|██▊       | 9/32 [04:14<09:55, 25.90s/it] 31%|███▏      | 10/32 [04:24<09:54, 27.04s/it] 31%|███▏      | 10/32 [04:29<10:13, 27.87s/it] 31%|███▏      | 10/32 [04:33<09:54, 27.01s/it] 25%|██▌       | 8/32 [04:34<12:30, 31.27s/it] 31%|███▏      | 10/32 [04:36<09:02, 24.64s/it] 19%|█▉        | 6/32 [04:40<20:00, 46.19s/it] 28%|██▊       | 9/32 [04:42<12:39, 33.04s/it] 34%|███▍      | 11/32 [04:50<09:24, 26.87s/it] 19%|█▉        | 6/32 [04:50<21:31, 49.66s/it] 34%|███▍      | 11/32 [04:55<09:00, 25.75s/it] 34%|███▍      | 11/32 [05:01<10:08, 28.95s/it] 31%|███▏      | 10/32 [05:02<10:40, 29.12s/it] 34%|███▍      | 11/32 [05:03<08:52, 25.34s/it] 28%|██▊       | 9/32 [05:03<11:42, 30.55s/it] 22%|██▏       | 7/32 [05:21<18:29, 44.37s/it] 38%|███▊      | 12/32 [05:23<08:57, 26.88s/it] 38%|███▊      | 12/32 [05:25<09:43, 29.19s/it] 31%|███▏      | 10/32 [05:26<10:23, 28.32s/it] 38%|███▊      | 12/32 [05:28<08:28, 25.40s/it] 34%|███▍      | 11/32 [05:29<09:54, 28.29s/it] 22%|██▏       | 7/32 [05:29<19:17, 46.28s/it] 38%|███▊      | 12/32 [05:30<09:28, 28.40s/it] 41%|████      | 13/32 [05:51<08:35, 27.12s/it] 34%|███▍      | 11/32 [05:52<09:35, 27.42s/it] 38%|███▊      | 12/32 [05:51<08:51, 26.59s/it] 41%|████      | 13/32 [05:55<09:23, 29.67s/it] 41%|████      | 13/32 [05:59<08:31, 26.94s/it] 41%|████      | 13/32 [05:59<09:04, 28.64s/it] 44%|████▍     | 14/32 [06:12<07:38, 25.48s/it] 25%|██▌       | 8/32 [06:14<18:16, 45.69s/it] 25%|██▌       | 8/32 [06:15<18:56, 47.34s/it] 38%|███▊      | 12/32 [06:16<08:52, 26.63s/it] 41%|████      | 13/32 [06:17<08:21, 26.38s/it] 44%|████▍     | 14/32 [06:19<08:24, 28.00s/it] 44%|████▍     | 14/32 [06:20<07:34, 25.23s/it] 44%|████▍     | 14/32 [06:24<08:14, 27.45s/it]
 44%|████▍     | 14/32 [06:26<08:28, 28.25s/it] 47%|████▋     | 15/32 [06:35<06:59, 24.67s/it] 47%|████▋     | 15/32 [06:40<06:42, 23.69s/it] 44%|████▍     | 14/32 [06:40<07:35, 25.28s/it] 41%|████      | 13/32 [06:41<08:11, 25.86s/it] 47%|████▋     | 15/32 [07:02<08:35, 30.31s/it] 28%|██▊       | 9/32 [07:02<17:51, 46.59s/it] 28%|██▊       | 9/32 [07:04<18:22, 47.95s/it] 47%|████▋     | 15/32 [07:05<07:06, 25.09s/it] 44%|████▍     | 14/32 [07:07<07:46, 25.92s/it] 50%|█████     | 16/32 [07:10<06:50, 25.63s/it] 50%|█████     | 16/32 [07:11<07:27, 27.97s/it] 47%|████▋     | 15/32 [07:30<07:08, 25.20s/it] 50%|█████     | 16/32 [07:31<08:00, 30.00s/it] 53%|█████▎    | 17/32 [07:36<06:47, 27.16s/it] 53%|█████▎    | 17/32 [07:41<06:50, 27.34s/it] 50%|█████     | 16/32 [07:48<08:09, 30.61s/it] 31%|███▏      | 10/32 [07:50<17:11, 46.89s/it] 31%|███▏      | 10/32 [07:50<17:22, 47.38s/it] 53%|█████▎    | 17/32 [08:00<07:26, 29.75s/it] 50%|█████     | 16/32 [08:03<07:18, 27.43s/it] 56%|█████▋    | 18/32 [08:19<07:26, 31.89s/it] 56%|█████▋    | 18/32 [08:22<07:16, 31.19s/it] 53%|█████▎    | 17/32 [08:23<07:56, 31.75s/it] 56%|█████▋    | 18/32 [08:23<06:27, 27.71s/it] 53%|█████▎    | 17/32 [08:30<06:52, 27.53s/it] 34%|███▍      | 11/32 [08:38<16:41, 47.69s/it] 59%|█████▉    | 19/32 [08:43<06:23, 29.52s/it] 59%|█████▉    | 19/32 [08:46<05:43, 26.44s/it] 59%|█████▉    | 19/32 [08:49<06:28, 29.90s/it] 56%|█████▋    | 18/32 [08:49<07:02, 30.16s/it] 34%|███▍      | 11/32 [08:55<18:18, 52.32s/it] 56%|█████▋    | 18/32 [08:56<06:18, 27.01s/it] 62%|██████▎   | 20/32 [09:03<05:21, 26.77s/it] 62%|██████▎   | 20/32 [09:09<05:04, 25.40s/it] 59%|█████▉    | 19/32 [09:10<05:56, 27.41s/it] 38%|███▊      | 12/32 [09:18<15:05, 45.26s/it] 62%|██████▎   | 20/32 [09:19<05:59, 29.95s/it] 66%|██████▌   | 21/32 [09:27<04:43, 25.74s/it] 59%|█████▉    | 19/32 [09:30<06:18, 29.10s/it] 38%|███▊      | 12/32 [09:41<16:50, 50.51s/it] 62%|██████▎   | 20/32 [09:44<05:53, 29.46s/it] 66%|██████▌   | 21/32 [09:47<05:20, 29.17s/it] 69%|██████▉   | 22/32 [09:49<04:05, 24.59s/it] 66%|██████▌   | 21/32 [09:49<05:31, 30.17s/it] 62%|██████▎   | 20/32 [09:55<05:34, 27.85s/it] 41%|████      | 13/32 [10:07<14:38, 46.22s/it] 66%|██████▌   | 21/32 [10:14<05:24, 29.47s/it] 41%|████      | 13/32 [10:17<14:33, 45.97s/it] 66%|██████▌   | 21/32 [10:23<05:07, 27.96s/it] 72%|███████▏  | 23/32 [10:24<04:10, 27.80s/it] 69%|██████▉   | 22/32 [10:24<05:14, 31.49s/it] 69%|██████▉   | 22/32 [10:25<05:18, 31.85s/it] 69%|██████▉   | 22/32 [10:35<04:30, 27.08s/it] 72%|███████▏  | 23/32 [10:45<04:15, 28.43s/it] 72%|███████▏  | 23/32 [10:49<04:24, 29.33s/it] 44%|████▍     | 14/32 [10:51<13:39, 45.52s/it] 69%|██████▉   | 22/32 [10:51<04:38, 27.81s/it] 78%|███████▊  | 25/32 [10:56<02:36, 22.34s/it] 72%|███████▏  | 23/32 [10:57<03:48, 25.37s/it] 44%|████▍     | 14/32 [10:58<13:21, 44.51s/it] 75%|███████▌  | 24/32 [11:06<03:30, 26.31s/it] 75%|███████▌  | 24/32 [11:12<03:39, 27.40s/it] 75%|███████▌  | 24/32 [11:19<03:15, 24.40s/it] 72%|███████▏  | 23/32 [11:19<04:11, 27.92s/it] 78%|███████▊  | 25/32 [11:31<03:00, 25.74s/it] 81%|████████▏ | 26/32 [11:32<02:34, 25.73s/it] 47%|████▋     | 15/32 [11:32<12:34, 44.40s/it] 78%|███████▊  | 25/32 [11:33<02:58, 25.47s/it] 78%|███████▊  | 25/32 [11:40<02:44, 23.54s/it] 47%|████▋     | 15/32 [11:51<13:19, 47.05s/it] 75%|███████▌  | 24/32 [11:53<03:58, 29.77s/it] 81%|████████▏ | 26/32 [11:55<02:31, 25.32s/it] 84%|████████▍ | 27/32 [12:04<02:16, 27.40s/it] 81%|████████▏ | 26/32 [12:05<02:23, 23.86s/it] 81%|████████▏ | 26/32 [12:08<02:49, 28.31s/it] 50%|█████     | 16/32 [12:14<11:37, 43.61s/it] 84%|████████▍ | 27/32 [12:16<01:59, 23.98s/it] 78%|███████▊  | 25/32 [12:19<03:19, 28.56s/it] 84%|████████▍ | 27/32 [12:31<02:02, 24.42s/it] 84%|████████▍ | 27/32 [12:32<02:15, 27.15s/it] 88%|████████▊ | 28/32 [12:37<01:56, 29.07s/it] 50%|█████     | 16/32 [12:41<12:50, 48.18s/it] 88%|████████▊ | 28/32 [12:45<01:41, 25.40s/it] 81%|████████▏ | 26/32 [12:51<02:57, 29.51s/it] 88%|████████▊ | 28/32 [12:53<01:35, 23.89s/it] 88%|████████▊ | 28/32 [13:02<01:52, 28.02s/it] 53%|█████▎    | 17/32 [13:12<12:00, 48.02s/it] 84%|████████▍ | 27/32 [13:15<02:20, 28.00s/it] 91%|█████████ | 29/32 [13:15<01:20, 26.98s/it] 91%|█████████ | 29/32 [13:17<01:11, 23.75s/it] 91%|█████████ | 29/32 [13:22<01:40, 33.45s/it] 91%|█████████ | 29/32 [13:27<01:20, 26.93s/it] 94%|█████████▍| 30/32 [13:40<00:52, 26.21s/it] 94%|█████████▍| 30/32 [13:42<00:48, 24.17s/it] 88%|████████▊ | 28/32 [13:43<01:51, 27.86s/it] 53%|█████▎    | 17/32 [13:44<13:07, 52.51s/it] 94%|█████████▍| 30/32 [13:57<01:07, 33.91s/it] 94%|█████████▍| 30/32 [13:58<00:56, 28.23s/it] 97%|█████████▋| 31/32 [14:03<00:25, 25.30s/it] 56%|█████▋    | 18/32 [14:04<11:28, 49.20s/it] 97%|█████████▋| 31/32 [14:09<00:25, 25.20s/it] 91%|█████████ | 29/32 [14:11<01:24, 28.14s/it] 56%|█████▋    | 18/32 [14:24<11:23, 48.80s/it] 97%|█████████▋| 31/32 [14:25<00:27, 27.84s/it] 97%|█████████▋| 31/32 [14:27<00:32, 32.68s/it]100%|██████████| 32/32 [14:32<00:00, 24.45s/it]100%|██████████| 32/32 [14:32<00:00, 27.27s/it]
100%|██████████| 32/32 [14:34<00:00, 26.89s/it]100%|██████████| 32/32 [14:34<00:00, 27.32s/it]
 94%|█████████▍| 30/32 [14:39<00:55, 27.91s/it] 59%|█████▉    | 19/32 [14:46<10:10, 46.99s/it]100%|██████████| 32/32 [14:49<00:00, 26.62s/it]100%|██████████| 32/32 [14:49<00:00, 27.78s/it]
100%|██████████| 32/32 [14:54<00:00, 31.06s/it]100%|██████████| 32/32 [14:54<00:00, 27.96s/it]
 97%|█████████▋| 31/32 [15:13<00:29, 29.94s/it] 59%|█████▉    | 19/32 [15:20<11:02, 50.97s/it]100%|██████████| 32/32 [15:41<00:00, 29.35s/it]100%|██████████| 32/32 [15:41<00:00, 29.43s/it]
 62%|██████▎   | 20/32 [15:47<10:12, 51.08s/it] 62%|██████▎   | 20/32 [16:25<11:00, 55.05s/it] 66%|██████▌   | 21/32 [16:45<09:44, 53.15s/it] 66%|██████▌   | 21/32 [16:59<08:57, 48.89s/it] 69%|██████▉   | 22/32 [17:32<08:33, 51.33s/it] 69%|██████▉   | 22/32 [18:06<09:01, 54.20s/it] 72%|███████▏  | 23/32 [18:21<07:36, 50.75s/it] 72%|███████▏  | 23/32 [19:01<08:10, 54.52s/it] 75%|███████▌  | 24/32 [19:05<06:29, 48.69s/it] 78%|███████▊  | 25/32 [19:41<05:14, 44.99s/it] 75%|███████▌  | 24/32 [20:06<07:40, 57.60s/it] 81%|████████▏ | 26/32 [20:51<05:13, 52.20s/it] 78%|███████▊  | 25/32 [20:52<06:18, 54.13s/it] 84%|████████▍ | 27/32 [21:37<04:11, 50.40s/it] 81%|████████▏ | 26/32 [21:39<05:12, 52.16s/it] 88%|████████▊ | 28/32 [22:25<03:18, 49.70s/it] 84%|████████▍ | 27/32 [22:42<04:35, 55.19s/it]