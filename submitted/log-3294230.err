Already on 'addinggriffin'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.47s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.82s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.04s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 49.00s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 29.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.38s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 29.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.74s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.66s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.69s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/CommonSenseReasoning/main.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  promptids = torch.tensor(promptids, dtype = torch.long).to(args.device)
  0%|          | 0/32 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  3%|▎         | 1/32 [00:40<21:02, 40.73s/it]  3%|▎         | 1/32 [00:43<22:36, 43.76s/it]  3%|▎         | 1/32 [00:46<24:12, 46.85s/it]  3%|▎         | 1/32 [00:48<25:10, 48.73s/it]  3%|▎         | 1/32 [00:50<25:53, 50.12s/it]  3%|▎         | 1/32 [00:56<29:04, 56.27s/it]  3%|▎         | 1/32 [01:14<38:28, 74.47s/it]  3%|▎         | 1/32 [01:23<43:16, 83.76s/it]  6%|▋         | 2/32 [01:27<22:09, 44.31s/it]  6%|▋         | 2/32 [01:28<22:17, 44.59s/it]  6%|▋         | 2/32 [01:34<23:16, 46.55s/it]  6%|▋         | 2/32 [01:34<23:40, 47.35s/it]  6%|▋         | 2/32 [01:39<24:53, 49.78s/it]  6%|▋         | 2/32 [01:55<28:58, 57.94s/it]  9%|▉         | 3/32 [02:18<22:37, 46.82s/it]  9%|▉         | 3/32 [02:18<22:04, 45.67s/it]  9%|▉         | 3/32 [02:20<23:18, 48.24s/it]  6%|▋         | 2/32 [02:22<35:25, 70.85s/it]  9%|▉         | 3/32 [02:23<23:20, 48.28s/it]  9%|▉         | 3/32 [02:26<23:28, 48.57s/it]  6%|▋         | 2/32 [02:29<36:35, 73.18s/it]  9%|▉         | 3/32 [02:52<27:47, 57.49s/it] 12%|█▎        | 4/32 [03:05<21:27, 45.97s/it] 12%|█▎        | 4/32 [03:04<21:45, 46.64s/it] 12%|█▎        | 4/32 [03:06<21:29, 46.05s/it] 12%|█▎        | 4/32 [03:07<22:18, 47.81s/it] 12%|█▎        | 4/32 [03:19<23:34, 50.51s/it]  9%|▉         | 3/32 [03:22<31:48, 65.81s/it]  9%|▉         | 3/32 [03:24<31:27, 65.08s/it] 16%|█▌        | 5/32 [03:47<20:22, 45.29s/it] 16%|█▌        | 5/32 [03:49<20:22, 45.28s/it] 12%|█▎        | 4/32 [03:52<27:23, 58.71s/it] 16%|█▌        | 5/32 [03:55<21:09, 47.02s/it] 16%|█▌        | 5/32 [03:57<21:53, 48.64s/it] 16%|█▌        | 5/32 [04:08<22:25, 49.85s/it] 12%|█▎        | 4/32 [04:25<30:15, 64.86s/it] 12%|█▎        | 4/32 [04:33<31:04, 66.60s/it] 19%|█▉        | 6/32 [04:34<19:35, 45.20s/it] 19%|█▉        | 6/32 [04:35<19:57, 46.06s/it] 16%|█▌        | 5/32 [04:35<23:52, 53.07s/it] 19%|█▉        | 6/32 [04:48<21:26, 49.48s/it] 19%|█▉        | 6/32 [04:51<21:40, 50.03s/it] 19%|█▉        | 6/32 [04:56<21:16, 49.11s/it] 22%|██▏       | 7/32 [05:21<19:10, 46.04s/it] 19%|█▉        | 6/32 [05:25<22:27, 51.84s/it] 22%|██▏       | 7/32 [05:29<20:14, 48.58s/it] 16%|█▌        | 5/32 [05:33<29:40, 65.94s/it] 22%|██▏       | 7/32 [05:37<20:32, 49.29s/it] 22%|██▏       | 7/32 [05:40<20:45, 49.82s/it] 22%|██▏       | 7/32 [05:40<19:49, 47.59s/it] 16%|█▌        | 5/32 [05:53<32:00, 71.13s/it] 25%|██▌       | 8/32 [06:08<18:32, 46.37s/it] 22%|██▏       | 7/32 [06:12<21:00, 50.40s/it] 25%|██▌       | 8/32 [06:18<17:49, 44.56s/it] 25%|██▌       | 8/32 [06:19<19:35, 48.97s/it] 25%|██▌       | 8/32 [06:25<19:27, 48.63s/it] 25%|██▌       | 8/32 [06:27<19:31, 48.82s/it] 19%|█▉        | 6/32 [06:42<28:56, 66.78s/it] 28%|██▊       | 9/32 [06:53<17:35, 45.91s/it] 19%|█▉        | 6/32 [06:58<29:59, 69.22s/it] 25%|██▌       | 8/32 [07:06<20:32, 51.35s/it] 28%|██▊       | 9/32 [07:09<18:54, 49.32s/it] 28%|██▊       | 9/32 [07:10<18:03, 47.10s/it] 28%|██▊       | 9/32 [07:10<17:59, 46.91s/it] 28%|██▊       | 9/32 [07:13<18:35, 48.50s/it] 31%|███▏      | 10/32 [07:47<16:44, 45.65s/it] 31%|███▏      | 10/32 [07:46<17:44, 48.38s/it] 22%|██▏       | 7/32 [07:49<27:50, 66.81s/it] 28%|██▊       | 9/32 [07:52<19:06, 49.86s/it] 31%|███▏      | 10/32 [07:56<17:10, 46.84s/it] 31%|███▏      | 10/32 [08:02<17:52, 48.73s/it] 22%|██▏       | 7/32 [08:07<28:51, 69.24s/it] 31%|███▏      | 10/32 [08:09<18:33, 50.62s/it] 34%|███▍      | 11/32 [08:33<16:42, 47.73s/it] 34%|███▍      | 11/32 [08:37<16:27, 47.04s/it] 31%|███▏      | 10/32 [08:38<17:45, 48.43s/it] 34%|███▍      | 11/32 [08:40<16:03, 45.90s/it] 34%|███▍      | 11/32 [08:44<16:23, 46.82s/it] 34%|███▍      | 11/32 [08:53<16:56, 48.43s/it] 25%|██▌       | 8/32 [08:58<27:04, 67.69s/it] 25%|██▌       | 8/32 [09:13<27:15, 68.13s/it] 38%|███▊      | 12/32 [09:20<15:15, 45.77s/it] 38%|███▊      | 12/32 [09:23<16:11, 48.59s/it] 38%|███▊      | 12/32 [09:27<15:10, 45.54s/it] 34%|███▍      | 11/32 [09:29<17:16, 49.33s/it] 38%|███▊      | 12/32 [09:30<15:39, 47.00s/it] 38%|███▊      | 12/32 [09:43<16:21, 49.10s/it] 28%|██▊       | 9/32 [10:05<25:47, 67.27s/it] 41%|████      | 13/32 [10:09<14:49, 46.80s/it] 41%|████      | 13/32 [10:11<14:15, 45.02s/it] 41%|████      | 13/32 [10:11<15:17, 48.28s/it] 41%|████      | 13/32 [10:18<15:02, 47.48s/it] 38%|███▊      | 12/32 [10:18<16:27, 49.38s/it] 28%|██▊       | 9/32 [10:23<26:22, 68.82s/it] 41%|████      | 13/32 [10:35<15:50, 50.01s/it] 44%|████▍     | 14/32 [10:53<13:48, 46.04s/it] 44%|████▍     | 14/32 [10:54<13:17, 44.31s/it] 44%|████▍     | 14/32 [10:54<14:02, 46.78s/it] 44%|████▍     | 14/32 [11:05<14:09, 47.20s/it] 31%|███▏      | 10/32 [11:06<23:59, 65.43s/it] 41%|████      | 13/32 [11:10<15:49, 49.97s/it] 44%|████▍     | 14/32 [11:20<14:32, 48.47s/it] 47%|████▋     | 15/32 [11:37<12:49, 45.29s/it] 31%|███▏      | 10/32 [11:37<25:48, 70.40s/it] 47%|████▋     | 15/32 [11:37<12:54, 45.55s/it] 47%|████▋     | 15/32 [11:42<12:52, 45.47s/it] 47%|████▋     | 15/32 [11:47<12:56, 45.67s/it] 44%|████▍     | 14/32 [11:58<14:49, 49.44s/it] 34%|███▍      | 11/32 [12:09<22:41, 64.82s/it] 47%|████▋     | 15/32 [12:21<14:46, 52.13s/it] 50%|█████     | 16/32 [12:24<12:14, 45.91s/it] 50%|█████     | 16/32 [12:28<12:10, 45.66s/it] 50%|█████     | 16/32 [12:34<13:03, 48.97s/it] 50%|█████     | 16/32 [12:38<12:36, 47.29s/it] 47%|████▋     | 15/32 [12:41<13:28, 47.56s/it] 34%|███▍      | 11/32 [12:51<25:01, 71.49s/it] 53%|█████▎    | 17/32 [13:08<11:19, 45.30s/it] 53%|█████▎    | 17/32 [13:14<11:29, 45.94s/it] 53%|█████▎    | 17/32 [13:19<11:53, 47.57s/it] 50%|█████     | 16/32 [13:19<14:20, 53.80s/it] 38%|███▊      | 12/32 [13:22<22:25, 67.29s/it] 53%|█████▎    | 17/32 [13:23<11:39, 46.63s/it] 50%|█████     | 16/32 [13:26<12:26, 46.67s/it] 56%|█████▋    | 18/32 [13:51<10:25, 44.65s/it] 38%|███▊      | 12/32 [14:01<23:41, 71.05s/it] 56%|█████▋    | 18/32 [14:04<10:57, 46.97s/it] 56%|█████▋    | 18/32 [14:04<10:58, 47.03s/it] 56%|█████▋    | 18/32 [14:10<10:52, 46.64s/it] 53%|█████▎    | 17/32 [14:10<11:28, 45.89s/it] 53%|█████▎    | 17/32 [14:15<13:39, 54.63s/it] 41%|████      | 13/32 [14:28<21:09, 66.83s/it] 59%|█████▉    | 19/32 [14:36<09:43, 44.89s/it] 59%|█████▉    | 19/32 [14:51<10:09, 46.85s/it] 59%|█████▉    | 19/32 [14:52<10:15, 47.31s/it] 56%|█████▋    | 18/32 [14:55<10:39, 45.64s/it] 59%|█████▉    | 19/32 [15:01<10:24, 48.02s/it] 56%|█████▋    | 18/32 [15:07<12:34, 53.88s/it] 41%|████      | 13/32 [15:11<22:23, 70.72s/it] 62%|██████▎   | 20/32 [15:19<08:50, 44.23s/it] 62%|██████▎   | 20/32 [15:36<09:15, 46.30s/it] 44%|████▍     | 14/32 [15:39<20:22, 67.94s/it] 59%|█████▉    | 19/32 [15:43<10:01, 46.29s/it] 62%|██████▎   | 20/32 [15:43<09:40, 48.35s/it] 62%|██████▎   | 20/32 [15:48<09:34, 47.89s/it] 66%|██████▌   | 21/32 [16:01<07:58, 43.50s/it] 59%|█████▉    | 19/32 [16:01<11:39, 53.84s/it] 44%|████▍     | 14/32 [16:15<20:34, 68.56s/it] 62%|██████▎   | 20/32 [16:27<09:08, 45.67s/it] 66%|██████▌   | 21/32 [16:29<08:51, 48.30s/it] 66%|██████▌   | 21/32 [16:36<09:07, 49.81s/it] 66%|██████▌   | 21/32 [16:39<08:56, 48.79s/it] 47%|████▋     | 15/32 [16:45<19:07, 67.52s/it] 69%|██████▉   | 22/32 [16:47<07:24, 44.41s/it] 62%|██████▎   | 20/32 [17:01<11:09, 55.77s/it] 47%|████▋     | 15/32 [17:15<18:39, 65.86s/it] 69%|██████▉   | 22/32 [17:16<07:59, 47.97s/it] 66%|██████▌   | 21/32 [17:18<08:41, 47.40s/it] 69%|██████▉   | 22/32 [17:22<08:06, 48.70s/it] 69%|██████▉   | 22/32 [17:23<07:51, 47.15s/it] 72%|███████▏  | 23/32 [17:31<06:38, 44.33s/it] 66%|██████▌   | 21/32 [17:55<10:05, 55.02s/it] 50%|█████     | 16/32 [17:55<18:12, 68.29s/it] 72%|███████▏  | 23/32 [18:01<07:04, 47.14s/it] 69%|██████▉   | 22/32 [18:06<07:55, 47.56s/it] 72%|███████▏  | 23/32 [18:10<07:05, 47.24s/it] 72%|███████▏  | 23/32 [18:10<07:17, 48.59s/it] 75%|███████▌  | 24/32 [18:14<05:52, 44.00s/it] 50%|█████     | 16/32 [18:27<18:03, 67.73s/it] 75%|███████▌  | 24/32 [18:43<06:04, 45.54s/it] 69%|██████▉   | 22/32 [18:49<09:09, 54.97s/it] 75%|███████▌  | 24/32 [18:50<06:07, 45.99s/it] 75%|███████▌  | 24/32 [18:52<06:05, 45.69s/it] 72%|███████▏  | 23/32 [18:54<07:08, 47.65s/it] 78%|███████▊  | 25/32 [19:06<05:23, 46.18s/it] 53%|█████▎    | 17/32 [19:15<17:54, 71.65s/it] 78%|███████▊  | 25/32 [19:28<05:18, 45.53s/it] 75%|███████▌  | 24/32 [19:35<06:03, 45.48s/it] 53%|█████▎    | 17/32 [19:35<17:01, 68.07s/it] 78%|███████▊  | 25/32 [19:36<05:16, 45.22s/it] 78%|███████▊  | 25/32 [19:38<05:25, 46.51s/it] 72%|███████▏  | 23/32 [19:42<08:07, 54.19s/it] 81%|████████▏ | 26/32 [19:56<04:45, 47.50s/it] 81%|████████▏ | 26/32 [20:20<04:44, 47.46s/it] 78%|███████▊  | 25/32 [20:20<05:19, 45.58s/it] 81%|████████▏ | 26/32 [20:24<04:36, 46.10s/it] 75%|███████▌  | 24/32 [20:26<06:48, 51.09s/it] 81%|████████▏ | 26/32 [20:27<04:44, 47.37s/it] 56%|█████▋    | 18/32 [20:30<16:59, 72.80s/it] 84%|████████▍ | 27/32 [20:43<03:56, 47.28s/it] 56%|█████▋    | 18/32 [20:44<15:56, 68.30s/it] 84%|████████▍ | 27/32 [21:04<03:51, 46.27s/it] 81%|████████▏ | 26/32 [21:11<04:42, 47.14s/it] 84%|████████▍ | 27/32 [21:17<03:59, 47.91s/it] 84%|████████▍ | 27/32 [21:19<04:03, 48.62s/it] 59%|█████▉    | 19/32 [21:25<14:36, 67.46s/it] 78%|███████▊  | 25/32 [21:29<06:22, 54.67s/it] 88%|████████▊ | 28/32 [21:38<03:18, 49.57s/it] 88%|████████▊ | 28/32 [21:54<03:10, 47.52s/it] 59%|█████▉    | 19/32 [21:55<14:55, 68.90s/it] 84%|████████▍ | 27/32 [21:58<03:54, 46.89s/it] 88%|████████▊ | 28/32 [22:09<03:16, 49.04s/it] 88%|████████▊ | 28/32 [22:10<03:18, 49.50s/it] 94%|█████████▍| 30/32 [22:24<01:14, 37.37s/it] 81%|████████▏ | 26/32 [22:26<05:33, 55.56s/it] 62%|██████▎   | 20/32 [22:28<13:13, 66.12s/it] 91%|█████████ | 29/32 [22:45<02:25, 48.60s/it] 88%|████████▊ | 28/32 [22:46<03:09, 47.36s/it] 91%|█████████ | 29/32 [23:01<02:29, 49.93s/it] 91%|█████████ | 29/32 [23:02<02:31, 50.46s/it] 97%|█████████▋| 31/32 [23:11<00:39, 39.64s/it] 62%|██████▎   | 20/32 [23:13<14:22, 71.86s/it] 84%|████████▍ | 27/32 [23:23<04:38, 55.80s/it] 91%|█████████ | 29/32 [23:31<02:20, 46.69s/it] 66%|██████▌   | 21/32 [23:33<12:01, 65.63s/it] 94%|█████████▍| 30/32 [23:33<01:36, 48.37s/it] 94%|█████████▍| 30/32 [23:48<01:37, 48.89s/it] 94%|█████████▍| 30/32 [23:48<01:37, 48.85s/it]100%|██████████| 32/32 [23:54<00:00, 40.51s/it]100%|██████████| 32/32 [23:54<00:00, 44.82s/it]
 88%|████████▊ | 28/32 [24:12<03:35, 53.94s/it] 97%|█████████▋| 31/32 [24:20<00:47, 47.81s/it] 66%|██████▌   | 21/32 [24:20<12:54, 70.37s/it] 94%|█████████▍| 30/32 [24:24<01:37, 48.53s/it] 97%|█████████▋| 31/32 [24:31<00:47, 47.38s/it] 97%|█████████▋| 31/32 [24:34<00:48, 48.27s/it] 69%|██████▉   | 22/32 [24:41<11:03, 66.36s/it] 91%|█████████ | 29/32 [24:56<02:32, 50.87s/it] 97%|█████████▋| 31/32 [25:10<00:47, 47.82s/it]100%|██████████| 32/32 [25:15<00:00, 50.10s/it]100%|██████████| 32/32 [25:15<00:00, 47.37s/it]
100%|██████████| 32/32 [25:24<00:00, 48.66s/it]100%|██████████| 32/32 [25:24<00:00, 47.64s/it]
 69%|██████▉   | 22/32 [25:30<11:42, 70.30s/it]100%|██████████| 32/32 [25:33<00:00, 51.52s/it]100%|██████████| 32/32 [25:33<00:00, 47.91s/it]
 94%|█████████▍| 30/32 [25:42<01:38, 49.46s/it] 72%|███████▏  | 23/32 [25:49<10:01, 66.88s/it]100%|██████████| 32/32 [25:51<00:00, 45.76s/it]100%|██████████| 32/32 [25:51<00:00, 48.48s/it]
 97%|█████████▋| 31/32 [26:26<00:47, 47.61s/it] 72%|███████▏  | 23/32 [26:46<10:46, 71.78s/it] 75%|███████▌  | 24/32 [26:58<09:01, 67.63s/it]100%|██████████| 32/32 [27:13<00:00, 47.52s/it]100%|██████████| 32/32 [27:13<00:00, 51.04s/it]
 78%|███████▊  | 25/32 [27:57<06:26, 55.18s/it] 78%|███████▊  | 25/32 [28:01<07:43, 66.25s/it] 81%|████████▏ | 26/32 [29:04<05:48, 58.01s/it] 81%|████████▏ | 26/32 [29:04<06:31, 65.29s/it] 84%|████████▍ | 27/32 [30:04<05:17, 63.56s/it] 84%|████████▍ | 27/32 [30:12<05:03, 60.77s/it] 88%|████████▊ | 28/32 [31:12<04:19, 64.91s/it] 88%|████████▊ | 28/32 [31:18<04:08, 62.07s/it] 91%|█████████ | 29/32 [32:20<03:17, 65.79s/it] 91%|█████████ | 29/32 [32:36<03:19, 66.45s/it] 94%|█████████▍| 30/32 [33:30<02:14, 67.22s/it] 94%|█████████▍| 30/32 [33:44<02:13, 66.89s/it][rank5]:[E ProcessGroupNCCL.cpp:523] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600561 milliseconds before timing out.
[rank5]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E ProcessGroupNCCL.cpp:1182] [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600561 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fc485d42d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7fc486eea6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7fc486eedc3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fc486eee839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7fc4d0c01df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7fc4d1ff0609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7fc4d1dbb353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=48, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600561 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fc485d42d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7fc486eea6e6 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7fc486eedc3d in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fc486eee839 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd6df4 (0x7fc4d0c01df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7fc4d1ff0609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7fc4d1dbb353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fc485d42d87 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7fc486c44b11 in /fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd6df4 (0x7fc4d0c01df4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7fc4d1ff0609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7fc4d1dbb353 in /lib/x86_64-linux-gnu/libc.so.6)

 97%|█████████▋| 31/32 [34:43<01:08, 68.78s/it] 97%|█████████▋| 31/32 [34:49<01:06, 66.32s/it][2024-07-08 22:22:06,161] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2471169 closing signal SIGTERM
[2024-07-08 22:22:06,163] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2471170 closing signal SIGTERM
[2024-07-08 22:22:06,163] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2471171 closing signal SIGTERM
[2024-07-08 22:22:06,164] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2471172 closing signal SIGTERM
[2024-07-08 22:22:06,164] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2471173 closing signal SIGTERM
[2024-07-08 22:22:06,164] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2471175 closing signal SIGTERM
[2024-07-08 22:22:06,164] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2471176 closing signal SIGTERM
[2024-07-08 22:22:08,044] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 5 (pid: 2471174) of binary: /fsx-storygen/beidic/anaconda3/envs/griffin/bin/python3.9
Traceback (most recent call last):
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fsx-storygen/beidic/anaconda3/envs/griffin/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
main.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-08_22:22:06
  host      : a100-st-p4de24xlarge-603.fair-a100.hpcaas
  rank      : 5 (local_rank: 5)
  exitcode  : -6 (pid: 2471174)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2471174
========================================================
