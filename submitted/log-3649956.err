Already on 'yangex3'
From github.com:Infini-AI-Lab/GRIFFIN2
   ebd7af5..00bb0d4  yangex3        -> origin/yangex3
   27e7195..d8228f6  yangexp2threee -> origin/yangexp2threee
 * [new branch]      yangexppp      -> origin/yangexppp
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
2024-08-03:02:39:36,458 INFO     [main.py:288] Verbosity set to INFO
2024-08-03:02:39:46,972 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-08-03:02:39:46,973 INFO     [main.py:378] Selected Tasks: ['gsm8k_cot']
2024-08-03:02:39:47,009 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-08-03:02:39:47,009 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-70B-Instruct', 'griffin': False, 'check': False, 'contextlength': 1500, 'kernel_size': 16, 'thr': 0.05}
2024-08-03:02:39:47,020 INFO     [xhuggingface.py:173] Using device 'cuda'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:05<02:47,  5.77s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:11<02:36,  5.58s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:16<02:31,  5.63s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:22<02:26,  5.64s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:28<02:19,  5.58s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:33<02:13,  5.57s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:38<02:05,  5.48s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:44<02:01,  5.52s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:50<01:56,  5.56s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:55<01:49,  5.48s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [01:02<01:55,  6.06s/it]Loading checkpoint shards:  40%|████      | 12/30 [01:08<01:45,  5.86s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [01:13<01:38,  5.78s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [01:19<01:31,  5.72s/it]Loading checkpoint shards:  50%|█████     | 15/30 [01:24<01:23,  5.59s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [01:30<01:17,  5.54s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [01:35<01:11,  5.48s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [01:38<01:15,  5.82s/it]
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/GRIFFIN2/main.py", line 465, in <module>
    cli_evaluate()
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/GRIFFIN2/main.py", line 384, in cli_evaluate
    results = xevaluator.simple_evaluate(
  File "/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/GRIFFIN2/xevaluator.py", line 188, in simple_evaluate
    lm = XHFLM.create_from_arg_string(
  File "/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/lm_eval/api/model.py", line 133, in create_from_arg_string
    return cls(**args, **args2)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/GRIFFIN2/xhuggingface.py", line 212, in __init__
    self._create_model(
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/GRIFFIN2/xhuggingface.py", line 535, in _create_model
    self._model = LlamaForCausalLM.from_pretrained(
  File "/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3502, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3926, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/transformers/modeling_utils.py", line 805, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 
