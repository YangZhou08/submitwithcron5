Already on 'yangexp2two'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-07-24:08:56:08,376 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:56:08,388 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:56:08,407 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:56:08,497 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:56:08,501 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:56:08,507 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:56:08,512 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:56:08,517 INFO     [main.py:288] Verbosity set to INFO
2024-07-24:08:56:15,066 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:56:15,067 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:56:15,073 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:56:15,074 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:56:15,074 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:56:15,074 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:56:15,378 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:56:15,384 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:56:15,384 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:56:15,405 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:56:15,410 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:56:15,411 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:56:15,535 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:56:15,541 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:56:15,541 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:56:15,672 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:56:15,679 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:56:15,679 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-07-24:08:56:15,813 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:56:15,820 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:56:15,820 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-07-24:08:56:16,120 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-07-24:08:56:16,126 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-24:08:56:16,126 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B', 'griffin': True, 'widthtree': 8, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.81s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.76s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.68s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.81s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:07,  3.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:07,  3.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:07,  3.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:07,  3.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.14s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.04s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.97s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.15s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.09s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.20s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-24:08:57:09,043 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 20/165 [00:00<00:00, 191.24it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-24:08:57:09,178 INFO     [xhuggingface.py:323] Using 8 devices with data parallelism
2024-07-24:08:57:09,237 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/165 [00:00<?, ?it/s] 24%|██▍       | 40/165 [00:00<00:00, 192.16it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-07-24:08:57:09,330 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 20/165 [00:00<00:00, 191.74it/s] 36%|███▋      | 60/165 [00:00<00:00, 192.36it/s] 12%|█▏        | 19/165 [00:00<00:00, 189.16it/s] 24%|██▍       | 40/165 [00:00<00:00, 193.02it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 48%|████▊     | 80/165 [00:00<00:00, 192.74it/s] 24%|██▎       | 39/165 [00:00<00:00, 191.30it/s] 36%|███▋      | 60/165 [00:00<00:00, 193.63it/s] 61%|██████    | 100/165 [00:00<00:00, 193.66it/s] 36%|███▌      | 59/165 [00:00<00:00, 191.63it/s] 48%|████▊     | 80/165 [00:00<00:00, 193.97it/s] 73%|███████▎  | 120/165 [00:00<00:00, 194.30it/s] 48%|████▊     | 79/165 [00:00<00:00, 192.60it/s]2024-07-24:08:57:09,773 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
 61%|██████    | 100/165 [00:00<00:00, 194.45it/s] 85%|████████▍ | 140/165 [00:00<00:00, 194.67it/s]  0%|          | 0/165 [00:00<?, ?it/s] 60%|██████    | 99/165 [00:00<00:00, 193.41it/s] 73%|███████▎  | 120/165 [00:00<00:00, 194.65it/s] 97%|█████████▋| 160/165 [00:00<00:00, 194.92it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 12%|█▏        | 19/165 [00:00<00:00, 188.11it/s]100%|██████████| 165/165 [00:00<00:00, 193.97it/s]
 72%|███████▏  | 119/165 [00:00<00:00, 193.85it/s] 85%|████████▍ | 140/165 [00:00<00:00, 195.08it/s] 24%|██▎       | 39/165 [00:00<00:00, 189.44it/s] 84%|████████▍ | 139/165 [00:00<00:00, 194.10it/s] 97%|█████████▋| 160/165 [00:00<00:00, 195.38it/s] 36%|███▌      | 59/165 [00:00<00:00, 189.73it/s]100%|██████████| 165/165 [00:00<00:00, 194.61it/s]
 96%|█████████▋| 159/165 [00:00<00:00, 194.03it/s]100%|██████████| 165/165 [00:00<00:00, 193.28it/s]
 48%|████▊     | 79/165 [00:00<00:00, 190.49it/s]2024-07-24:08:57:10,278 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
 60%|██████    | 99/165 [00:00<00:00, 190.98it/s]  0%|          | 0/165 [00:00<?, ?it/s] 72%|███████▏  | 119/165 [00:00<00:00, 191.24it/s]  8%|▊         | 13/165 [00:00<00:01, 121.47it/s] 84%|████████▍ | 139/165 [00:00<00:00, 191.43it/s] 19%|█▉        | 31/165 [00:00<00:00, 151.19it/s]2024-07-24:08:57:10,568 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/165 [00:00<?, ?it/s]2024-07-24:08:57:10,610 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
 96%|█████████▋| 159/165 [00:00<00:00, 191.48it/s]  0%|          | 0/165 [00:00<?, ?it/s] 31%|███       | 51/165 [00:00<00:00, 169.87it/s]100%|██████████| 165/165 [00:00<00:00, 190.88it/s]
 12%|█▏        | 19/165 [00:00<00:00, 188.24it/s] 11%|█         | 18/165 [00:00<00:00, 177.12it/s] 43%|████▎     | 71/165 [00:00<00:00, 178.85it/s] 24%|██▎       | 39/165 [00:00<00:00, 189.59it/s] 55%|█████▌    | 91/165 [00:00<00:00, 184.37it/s] 22%|██▏       | 36/165 [00:00<00:00, 140.45it/s] 36%|███▌      | 59/165 [00:00<00:00, 189.91it/s] 67%|██████▋   | 111/165 [00:00<00:00, 187.23it/s] 31%|███       | 51/165 [00:00<00:00, 129.55it/s] 79%|███████▉  | 131/165 [00:00<00:00, 189.35it/s] 47%|████▋     | 78/165 [00:00<00:00, 155.08it/s]2024-07-24:08:57:11,116 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
 39%|███▉      | 65/165 [00:00<00:00, 123.09it/s] 92%|█████████▏| 151/165 [00:00<00:00, 190.88it/s]  0%|          | 0/164 [00:00<?, ?it/s] 58%|█████▊    | 95/165 [00:00<00:00, 147.65it/s]100%|██████████| 165/165 [00:00<00:00, 182.42it/s]
 50%|████▉     | 82/165 [00:00<00:00, 137.32it/s] 10%|█         | 17/164 [00:00<00:00, 169.07it/s] 69%|██████▉   | 114/165 [00:00<00:00, 158.67it/s] 61%|██████    | 101/165 [00:00<00:00, 153.34it/s] 23%|██▎       | 37/164 [00:00<00:00, 181.84it/s] 80%|████████  | 132/165 [00:00<00:00, 163.67it/s] 73%|███████▎  | 120/165 [00:00<00:00, 163.59it/s] 35%|███▍      | 57/164 [00:00<00:00, 185.79it/s] 90%|█████████ | 149/165 [00:00<00:00, 163.26it/s] 84%|████████▍ | 139/165 [00:00<00:00, 170.88it/s] 47%|████▋     | 77/164 [00:00<00:00, 187.94it/s]100%|██████████| 165/165 [00:01<00:00, 164.57it/s]
 96%|█████████▋| 159/165 [00:01<00:00, 177.01it/s]100%|██████████| 165/165 [00:01<00:00, 158.38it/s]
 59%|█████▉    | 97/164 [00:00<00:00, 189.41it/s] 71%|███████▏  | 117/164 [00:00<00:00, 189.89it/s] 84%|████████▎ | 137/164 [00:00<00:00, 190.51it/s] 96%|█████████▌| 157/164 [00:00<00:00, 190.69it/s]100%|██████████| 164/164 [00:00<00:00, 188.55it/s]
2024-07-24:08:57:25,261 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:57:25,261 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:57:25,261 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:57:25,261 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:57:25,261 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:57:25,261 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:57:25,261 INFO     [xevaluator.py:395] Running generate_until requests
2024-07-24:08:57:25,262 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/165 [00:18<49:26, 18.09s/it]Running generate_until requests:   1%|          | 2/165 [00:27<34:45, 12.79s/it]Running generate_until requests:   2%|▏         | 3/165 [00:48<45:03, 16.69s/it]Running generate_until requests:   2%|▏         | 4/165 [01:01<40:42, 15.17s/it]Running generate_until requests:   3%|▎         | 5/165 [01:08<33:11, 12.45s/it]Running generate_until requests:   4%|▎         | 6/165 [01:21<33:16, 12.56s/it]Running generate_until requests:   4%|▍         | 7/165 [01:29<28:47, 10.93s/it]Running generate_until requests:   4%|▍         | 7/165 [01:36<36:25, 13.83s/it]
