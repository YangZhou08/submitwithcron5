fatal: Unable to create '/fsx-storygen/beidic/yang/GRIFFIN2/.git/index.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
fatal: Unable to create '/fsx-storygen/beidic/yang/GRIFFIN2/.git/index.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:05<02:37,  5.42s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:10<02:32,  5.44s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:16<02:31,  5.61s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:22<02:28,  5.73s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:28<02:22,  5.68s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:33<02:15,  5.66s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:39<02:09,  5.63s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:45<02:03,  5.63s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:50<01:58,  5.63s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:55<01:50,  5.52s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [01:01<01:43,  5.42s/it]Loading checkpoint shards:  40%|████      | 12/30 [01:06<01:37,  5.42s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [01:12<01:33,  5.50s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [01:17<01:28,  5.53s/it]Loading checkpoint shards:  50%|█████     | 15/30 [01:23<01:21,  5.44s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [01:28<01:15,  5.37s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [01:34<01:13,  5.68s/it]Loading checkpoint shards:  60%|██████    | 18/30 [01:40<01:08,  5.67s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [01:45<01:02,  5.65s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [01:51<00:55,  5.52s/it]Loading checkpoint shards:  70%|███████   | 21/30 [01:56<00:49,  5.46s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [02:01<00:43,  5.40s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [02:08<00:40,  5.74s/it]Loading checkpoint shards:  80%|████████  | 24/30 [02:13<00:34,  5.69s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [02:19<00:27,  5.56s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [02:24<00:21,  5.47s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [02:29<00:16,  5.39s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [02:35<00:11,  5.52s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [02:40<00:05,  5.54s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:43<00:00,  4.62s/it]Loading checkpoint shards: 100%|██████████| 30/30 [02:43<00:00,  5.45s/it]
  0%|          | 0/1000 [00:00<?, ?it/s]/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/fsx-storygen/beidic/anaconda3/envs/griffinn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 1/1000 [02:26<40:44:57, 146.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 2/1000 [04:54<40:46:21, 147.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 3/1000 [07:24<41:08:25, 148.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 4/1000 [09:57<41:38:47, 150.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 5/1000 [12:30<41:49:47, 151.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 6/1000 [15:04<42:01:28, 152.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 7/1000 [17:41<42:22:38, 153.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 8/1000 [20:15<42:21:32, 153.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 9/1000 [22:47<42:13:22, 153.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 10/1000 [25:19<42:00:21, 152.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 11/1000 [27:51<41:55:05, 152.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 12/1000 [30:25<41:59:05, 152.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|▏         | 13/1000 [32:58<41:59:38, 153.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|▏         | 14/1000 [35:35<42:16:41, 154.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 15/1000 [38:08<42:07:48, 153.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 16/1000 [40:46<42:23:52, 155.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 17/1000 [43:22<42:25:25, 155.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 18/1000 [46:01<42:40:37, 156.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 19/1000 [48:38<42:39:12, 156.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 20/1000 [51:11<42:22:10, 155.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 21/1000 [53:48<42:21:54, 155.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 22/1000 [56:23<42:18:08, 155.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 23/1000 [59:00<42:23:01, 156.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 24/1000 [1:01:34<42:07:46, 155.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▎         | 25/1000 [1:04:08<41:57:49, 154.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 26/1000 [1:06:45<42:07:55, 155.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 27/1000 [1:09:18<41:49:16, 154.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 28/1000 [1:11:52<41:42:00, 154.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 29/1000 [1:14:26<41:40:30, 154.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 30/1000 [1:17:01<41:40:43, 154.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 31/1000 [1:19:35<41:31:16, 154.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 32/1000 [1:22:15<41:58:36, 156.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 33/1000 [1:24:49<41:45:09, 155.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
